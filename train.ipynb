{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import plot_confusion_matrix, plot_confusion_matrix, get_gzipped_model_size, rescale_data\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from models.ConvLSTM import ConvLSTM\n",
    "from models.ConvLSTM_VGG import ConvLSTM_VGG\n",
    "from models.TinyFallNet import TinyFallNet\n",
    "from models.ResNet24 import ResNet24\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# mac\n",
    "# data_path = config['data_path_mac']\n",
    "# sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "# label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# windows\n",
    "data_path = config['data_path_win']\n",
    "sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# linux\n",
    "# data_path = config['data_path_linux']\n",
    "# sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "# label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# data mode. Combination of sensor data.\n",
    "data_mode = 'ACC+GYRO' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "window_size = config['window_size'] # window size\n",
    "fall_threshold = config['fall_threshold'] # threshold for windows labeled as fall\n",
    "num_window_fall_data = config['num_window_fall_data']   # number of windows labeled as fall\n",
    "num_window_not_fall_data = config['num_window_not_fall_data']    # number of windows labeled as not fall\n",
    "acc_max = config['acc_max'] \n",
    "gyro_max = config['gyro_max'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  5e-4\n",
      "batch_size:  64\n",
      "epochs:  50\n",
      "lr_factor:  0.5\n",
      "patience:  5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyFallNet_6axis_8bitInput\" # \"ConvLSTM\" or \"ConvLSTM_VGG\" or \"TinyFallNet\" or \"ResNet24\" or \"TinyFallNet_6axis\"\n",
    "load_from_checkpoint = config['load_from_checkpoint']\n",
    "\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = models.load_model('./saved_models/'+model_name+'.keras')\n",
    "else:\n",
    "    if model_name == \"ConvLSTM\":\n",
    "        model = ConvLSTM()\n",
    "        data_mode = 'ACC+GYRO+MAG' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ConvLSTM_VGG\":\n",
    "        model = ConvLSTM_VGG()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet\":\n",
    "        model = TinyFallNet()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ResNet24\":\n",
    "        model = ResNet24()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet_6axis\":\n",
    "        model = TinyFallNet(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "        model = TinyFallNet(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    else:\n",
    "        print(\"Please select a valid model name\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = config['learning_rate']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "lr_factor = config['lr_factor']\n",
    "patience = config['patience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n",
      "Data shape:  (75888, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            fall_threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data,\n",
    "                            data_mode)\n",
    "\n",
    "print(\"Data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the rescaling is done only once\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\" and data.dtype!=np.int8:\n",
    "    dtype_out = np.int8 # rescaled input data type\n",
    "    data = rescale_data(data, dtype_out, acc_max=acc_max, gyro_max=acc_max)\n",
    "else:\n",
    "    data = data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (75888, 50, 6)\n",
      "Data dtype:  int8\n",
      "in_channels:  6\n",
      "not_fall_size:  75060\n",
      "fall_size:  828\n"
     ]
    }
   ],
   "source": [
    "in_channels = data.shape[2]\n",
    "print(\"Data shape: \", data.shape)\n",
    "print(\"Data dtype: \", data.dtype)\n",
    "print('in_channels: ', in_channels)\n",
    "\n",
    "label = label.astype(np.int64)\n",
    "data_copy = data.reshape(data.shape[0], 50, in_channels)\n",
    "\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('not_fall_size: ', B_size)\t\n",
    "print('fall_size: ', A_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_copy, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the saved_data folder if it does not exist\n",
    "if not os.path.exists('./saved_data'):\n",
    "    os.makedirs('./saved_data')\n",
    "# save the test data, train data and validation data\n",
    "np.save('./saved_data/X_test.npy', X_test)\n",
    "np.save('./saved_data/y_test.npy', y_test)\n",
    "np.save('./saved_data/X_train.npy', X_train)\n",
    "np.save('./saved_data/y_train.npy', y_train)\n",
    "np.save('./saved_data/X_val.npy', X_val)\n",
    "np.save('./saved_data/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 50, 6)             0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 1, 48, 64)            1216      ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 1, 24, 64)            0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 1, 24, 16)            1040      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1, 24, 16)            64        ['conv2d_3[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 1, 24, 16)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 1, 24, 16)            784       ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1, 24, 16)            64        ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 1, 24, 64)            1088      ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1, 24, 64)            256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 1, 24, 64)            4160      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1, 24, 64)            0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'conv2d_2[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 1, 24, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 1, 24, 16)            1040      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1, 24, 16)            64        ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 1, 24, 16)            784       ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1, 24, 16)            64        ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 1, 24, 64)            1088      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1, 24, 64)            256       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 1, 24, 64)            4160      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_5[0][0]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    , 'conv2d_6[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 1, 24, 64)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1, 24, 16)            64        ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1, 24, 16)            64        ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1, 24, 64)            256       ['conv2d_13[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    , 'conv2d_10[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 1, 24, 64)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1, 24, 16)            64        ['conv2d_15[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 1, 24, 16)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1, 24, 16)            64        ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1, 24, 64)            256       ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 1, 24, 64)            0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 1, 12, 64)            0         ['re_lu_11[0][0]']            \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 768)                  0         ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    1538      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32578 (127.26 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "            loss='categorical_crossentropy',\n",
    "            #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 50, 9))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48633, 2)\n",
      "y_val.shape:  (12159, 2)\n",
      "X_train.shape:  (48633, 50, 6)\n",
      "y_train.shape:  (48633, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From g:\\python\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From g:\\python\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "760/760 [==============================] - 12s 11ms/step - loss: 0.5276 - accuracy: 0.9043 - val_loss: 0.7718 - val_accuracy: 0.6341 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2974 - accuracy: 0.9394 - val_loss: 0.1219 - val_accuracy: 0.9618 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2693 - accuracy: 0.9430 - val_loss: 0.2612 - val_accuracy: 0.9295 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2569 - accuracy: 0.9464 - val_loss: 0.1568 - val_accuracy: 0.9363 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2419 - accuracy: 0.9485 - val_loss: 0.1008 - val_accuracy: 0.9705 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2398 - accuracy: 0.9509 - val_loss: 0.1341 - val_accuracy: 0.9510 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2115 - accuracy: 0.9545 - val_loss: 0.1241 - val_accuracy: 0.9581 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "760/760 [==============================] - 9s 11ms/step - loss: 0.2169 - accuracy: 0.9553 - val_loss: 0.2451 - val_accuracy: 0.9255 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.1622 - accuracy: 0.9647 - val_loss: 0.1286 - val_accuracy: 0.9555 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2112 - accuracy: 0.9579 - val_loss: 0.0939 - val_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.1692 - accuracy: 0.9623 - val_loss: 0.1051 - val_accuracy: 0.9703 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.1597 - accuracy: 0.9657 - val_loss: 0.1020 - val_accuracy: 0.9705 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2256 - accuracy: 0.9585 - val_loss: 0.1942 - val_accuracy: 0.9346 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "760/760 [==============================] - 8s 11ms/step - loss: 0.2049 - accuracy: 0.9604 - val_loss: 0.1039 - val_accuracy: 0.9679 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9677\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "760/760 [==============================] - 9s 11ms/step - loss: 0.1552 - accuracy: 0.9677 - val_loss: 0.1569 - val_accuracy: 0.9544 - lr: 5.0000e-04\n",
      "Epoch 15: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (366, 50, 6)\n",
      "12/12 - 0s - loss: 0.1189 - accuracy: 0.9617 - 98ms/epoch - 8ms/step\n",
      "Test loss: [0.11891750991344452, 0.9617486596107483]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step\n",
      "[[168  11]\n",
      " [  3 184]]\n",
      "Confusion matrix, without normalization\n",
      "[[168  11]\n",
      " [  3 184]]\n",
      "accuracy:  0.9617486338797814\n",
      "f1_score:  0.963350785340314\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "# f1 score\n",
    "precision_fp = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall_fp = cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "f1_score_fp = 2 * precision_fp * recall_fp / (precision_fp + recall_fp)\n",
    "print('f1_score: ', f1_score_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpd44s2jfw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpd44s2jfw\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./saved_models/'+model_name+'.keras')\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if model_name==\"ConvLSTM\" or model_name==\"ConvLSTM_VGG\":\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "model_tflite = converter.convert()\n",
    "# save the model\n",
    "open('./saved_models/'+model_name+'.tflite', \"wb\").write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ConvLSTM model\n",
    "if model_name==\"ConvLSTM\" or model_name==\"ConvLSTM_VGG\":\n",
    "    def representative_data_gen():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_q_model = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    print('input: ', input_type)\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "    print('output: ', output_type)\n",
    "    # Save the quantized model to disk\n",
    "    open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLa  (None, 50, 6)                3         ['input_2[0][0]']             \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " quant_reshape_1 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 48, 64)            1347      ['quant_reshape_1[0][0]']     \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 24, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 24, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 24, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 24, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 24, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 24, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 24, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 24, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 24, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 24, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 24, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 24, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_12[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_13[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_10[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_16[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_17[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_14[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 12, 64)            3         ['quant_re_lu_11[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 768)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1543      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34087 (133.15 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 2277 (8.89 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model = tfmot.quantization.keras.quantize_model(model)\n",
    "q_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis_8bitInput\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpe2_5q8ya\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpe2_5q8ya\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.int8'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_q.keras')\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "  print('model name: ', model_name)\n",
    "  converter.inference_input_type = tf.int8  # Keep input as int8 for TinyFallNet_6axis_8bitInput\n",
    "else:\n",
    "  converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_q_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48633, 2)\n",
      "y_val.shape:  (12159, 2)\n",
      "Epoch 1/50\n",
      "760/760 [==============================] - 20s 18ms/step - loss: 0.2142 - accuracy: 0.9574 - val_loss: 0.0598 - val_accuracy: 0.9783 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "760/760 [==============================] - 13s 18ms/step - loss: 0.1515 - accuracy: 0.9693 - val_loss: 0.0680 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.1653 - accuracy: 0.9727 - val_loss: 0.2976 - val_accuracy: 0.9196 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.1939 - accuracy: 0.9638 - val_loss: 0.1145 - val_accuracy: 0.9636 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.1584 - accuracy: 0.9691 - val_loss: 0.0821 - val_accuracy: 0.9738 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.9731\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.1375 - accuracy: 0.9731 - val_loss: 0.0601 - val_accuracy: 0.9827 - lr: 5.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "    assert X_train.dtype == np.int8\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "q_history = q_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpagncmga2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpagncmga2\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.int8'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68976"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_qat.keras')  # The file needs to end with the .keras extension\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "  converter.inference_input_type = tf.int8  # Keep input as int8 for TinyFallNet_6axis_8bitInput\n",
    "else:\n",
    "  converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "\n",
    "# Convert the model\n",
    "tflite_q_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open('./saved_models/'+model_name+'_qat.tflite', \"wb\").write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis_8bitInput\n",
      "input:  {'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([ 1, 50,  6]), 'shape_signature': array([-1, 50,  6]), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 73, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[175   4]\n",
      " [  7 180]]\n",
      "Confusion matrix, without normalization\n",
      "[[175   4]\n",
      " [  7 180]]\n",
      "f1_score:  0.9703504043126685\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "    print('model name: ', model_name)\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= tflite_q_model)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "#accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "#print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 6)             1         ['input_2[0][0]']             \n",
      " e_1 (PruneLowMagnitude)                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            2370      ['prune_low_magnitude_reshape_\n",
      " _1 (PruneLowMagnitude)                                             1[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 24, 64)            1         ['prune_low_magnitude_conv2d_1\n",
      " oling2d (PruneLowMagnitude                                         [0][0]']                      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _3 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_3\n",
      " normalization (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu   (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu[0]\n",
      " _4 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_4\n",
      " normalization_1 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 1 (PruneLowMagnitude)                                              rmalization_1[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_1[\n",
      " _5 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_5\n",
      " normalization_2 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _2 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add (P  (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " runeLowMagnitude)                                                  rmalization_2[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add[0][0\n",
      " 2 (PruneLowMagnitude)                                              ]']                           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_2[\n",
      " _7 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_7\n",
      " normalization_3 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 3 (PruneLowMagnitude)                                              rmalization_3[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_3[\n",
      " _8 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_4 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 4 (PruneLowMagnitude)                                              rmalization_4[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_4[\n",
      " _9 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_5 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_2[\n",
      " _6 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_1   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_5[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_6\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_1[0]\n",
      " 5 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_5[\n",
      " _11 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_6 (PruneLowM                                         1[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 6 (PruneLowMagnitude)                                              rmalization_6[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_6[\n",
      " _12 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_7 (PruneLowM                                         2[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 7 (PruneLowMagnitude)                                              rmalization_7[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_7[\n",
      " _13 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_8 (PruneLowM                                         3[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_5[\n",
      " _10 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_2   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_8[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    0[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_2[0]\n",
      " 8 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_8[\n",
      " _15 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_9 (PruneLowM                                         5[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 9 (PruneLowMagnitude)                                              rmalization_9[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_9[\n",
      " _16 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_10 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 10 (PruneLowMagnitude)                                             rmalization_10[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_10\n",
      " _17 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_11 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_8[\n",
      " _14 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_3   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_11[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    4[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_3[0]\n",
      " 11 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 12, 64)            1         ['prune_low_magnitude_re_lu_11\n",
      " e_pooling2d (PruneLowMagni                                         [0][0]']                      \n",
      " tude)                                                                                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 768)                  1         ['prune_low_magnitude_average_\n",
      " n (PruneLowMagnitude)                                              pooling2d[0][0]']             \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense   (None, 2)                    3076      ['prune_low_magnitude_flatten[\n",
      " (PruneLowMagnitude)                                                0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62982 (246.22 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 31172 (121.96 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "ups = pruning_callbacks.UpdatePruningStep()\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "760/760 [==============================] - 21s 14ms/step - loss: 0.1616 - accuracy: 0.9679 - val_loss: 0.1284 - val_accuracy: 0.9590 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1730 - accuracy: 0.9664 - val_loss: 0.0830 - val_accuracy: 0.9734 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.2086 - accuracy: 0.9598 - val_loss: 0.1412 - val_accuracy: 0.9527 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1438 - accuracy: 0.9665 - val_loss: 0.1242 - val_accuracy: 0.9621 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1258 - accuracy: 0.9724 - val_loss: 0.1341 - val_accuracy: 0.9561 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1061 - accuracy: 0.9752 - val_loss: 0.0593 - val_accuracy: 0.9814 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0849 - accuracy: 0.9810 - val_loss: 0.0483 - val_accuracy: 0.9851 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1091 - accuracy: 0.9767 - val_loss: 0.0736 - val_accuracy: 0.9764 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0852 - accuracy: 0.9816 - val_loss: 0.0777 - val_accuracy: 0.9785 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0838 - accuracy: 0.9824 - val_loss: 0.0480 - val_accuracy: 0.9860 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0917 - accuracy: 0.9817 - val_loss: 0.1638 - val_accuracy: 0.9473 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0788 - accuracy: 0.9826 - val_loss: 0.0590 - val_accuracy: 0.9831 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0821 - accuracy: 0.9837 - val_loss: 0.0409 - val_accuracy: 0.9870 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0836 - accuracy: 0.9860 - val_loss: 0.2265 - val_accuracy: 0.9447 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.1103 - accuracy: 0.9789 - val_loss: 0.0516 - val_accuracy: 0.9859 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0524 - accuracy: 0.9886 - val_loss: 0.0482 - val_accuracy: 0.9854 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0542 - accuracy: 0.9881 - val_loss: 0.0535 - val_accuracy: 0.9858 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9905\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.0422 - accuracy: 0.9905 - val_loss: 0.0428 - val_accuracy: 0.9878 - lr: 5.0000e-04\n",
      "Epoch 18: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fa02210bb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.11663836240768433\n",
      "Pruned model accuracy:  0.9699453711509705\n",
      "Full-precision model accuracy:  0.9617486338797814\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpapbx7prm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpapbx7prm\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "pruned_model_unstructured.save('./saved_models/'+model_name+'_pruned_unstructured.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))\n",
    "\n",
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = './saved_models/'+model_name+'_pruned_unstructured.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  76904\n",
      "Size of the full-precision model:  120275\n",
      "The achieved compression ratio is 1.56x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size('./saved_models/'+model_name+'_pruned_unstructured.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('./saved_models/'+model_name+'.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('./saved_models/'+model_name+'.tflite') / get_gzipped_model_size('./saved_models/'+model_name+'_pruned_unstructured.tflite')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_1 (Quantize  (None, 50, 6)                3         ['input_2[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_1 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_1[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 48, 64)            1347      ['quant_reshape_1[0][0]']     \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 24, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 24, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 24, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 24, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 24, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 24, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 24, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 24, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 24, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 24, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 24, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 24, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_12[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_13[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_10[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 24, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 24, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_16[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_17[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_14[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 12, 64)            3         ['quant_re_lu_11[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 768)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1543      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34087 (133.15 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 2277 (8.89 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (48633, 50, 6)\n",
      "y_train.shape:  (48633, 2)\n",
      "64\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 19s 17ms/step - loss: 0.4595 - accuracy: 0.9695 - val_loss: 0.0641 - val_accuracy: 0.9824 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0498 - accuracy: 0.9898 - val_loss: 0.0405 - val_accuracy: 0.9888 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "760/760 [==============================] - 13s 16ms/step - loss: 0.0466 - accuracy: 0.9899 - val_loss: 0.0462 - val_accuracy: 0.9877 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0304 - accuracy: 0.9937 - val_loss: 0.0316 - val_accuracy: 0.9911 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0322 - accuracy: 0.9937 - val_loss: 0.0700 - val_accuracy: 0.9863 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.0527 - accuracy: 0.9904 - val_loss: 0.0864 - val_accuracy: 0.9802 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.0759 - accuracy: 0.9884 - val_loss: 0.0630 - val_accuracy: 0.9841 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.0559 - accuracy: 0.9900 - val_loss: 0.0529 - val_accuracy: 0.9882 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9887\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.0710 - accuracy: 0.9887 - val_loss: 0.1338 - val_accuracy: 0.9748 - lr: 5.0000e-04\n",
      "Epoch 9: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fa4524b250>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "    assert X_train.dtype == np.int8\n",
    "print(batch_size)\n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpafuqayu5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpafuqayu5\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69232"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_qat_model.save('./saved_models/'+model_name+'_pqat.keras')  # The file needs to end with the .keras extension\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "  converter.inference_input_type = tf.int8  # Keep input as int8 for TinyFallNet_6axis_8bitInput\n",
    "else:\n",
    "  converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "open('./saved_models/'+model_name+'_pqat.tflite', \"wb\").write(pruned_qat_tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis_8bitInput\n",
      "input:  {'name': 'TinyFallNet/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars;TinyFallNet/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars/ReadVariableOp;TinyFallNet/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1', 'index': 24, 'shape': array([ 1, 50,  6]), 'shape_signature': array([-1, 50,  6]), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'TinyFallNet/quant_dense/Softmax', 'index': 73, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[173   6]\n",
      " [  3 184]]\n",
      "Confusion matrix, without normalization\n",
      "[[173   6]\n",
      " [  3 184]]\n",
      "f1_score:  0.9761273209549072\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if model_name == \"TinyFallNet_6axis_8bitInput\":\n",
    "    print('model name: ', model_name)\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= pruned_qat_tflite_model)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned QAT model:  33197\n",
      "Size of th QAT model:  42333\n",
      "Size of the full-precision model:  120275\n",
      "The achieved compression ratio is 3.62x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size('./saved_models/'+model_name+'_pqat.tflite'))\n",
    "print('Size of th QAT model: ', get_gzipped_model_size('./saved_models/'+model_name+'_qat.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('./saved_models/'+model_name+'.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('saved_models/'+ model_name+ '.tflite') / get_gzipped_model_size('./saved_models/'+model_name+'_pqat.tflite')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import plot_confusion_matrix, plot_confusion_matrix, get_gzipped_model_size, rescale_data\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from models.ConvLSTM import ConvLSTM\n",
    "from models.ConvLSTM_VGG import ConvLSTM_VGG\n",
    "from models.TinyFallNet import TinyFallNet\n",
    "from models.ResNet24 import ResNet24\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# mac\n",
    "# data_path = config['data_path_mac']\n",
    "# sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "# label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# windows\n",
    "# data_path = config['data_path_win']\n",
    "# data_path = config['data_path_linux']\n",
    "data_path = config['data_path_mac']\n",
    "sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# linux\n",
    "# data_path = config['data_path_linux']\n",
    "# sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "# label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# data mode. Combination of sensor data.\n",
    "# data_mode = 'ACC+GYRO' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "window_size = config['window_size'] # window size\n",
    "fall_threshold = config['fall_threshold'] # threshold for windows labeled as fall\n",
    "num_window_fall_data = config['num_window_fall_data']   # number of windows labeled as fall\n",
    "num_window_not_fall_data = config['num_window_not_fall_data']    # number of windows labeled as not fall\n",
    "acc_max = config['acc_max'] \n",
    "gyro_max = config['gyro_max'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyFallNet_6axis\" # \"ConvLSTM\" or \"ConvLSTM_VGG\" or \"TinyFallNet\" or \"ResNet24\" or \"TinyFallNet_6axis\"\n",
    "# when train_with_int is True, scaled data will be used for training, generate full integer quantized model(int8 input, int8 output)\n",
    "# when train_with_int is False, original data will be used for training, generate three models: dynamic range quantized model(float32 input, float32 output)\n",
    "#                                                                                               full integer quantized model(int8 input, int8 output)\n",
    "#                                                                                               full integer quantized model(float32 input, int8 output)\n",
    "train_with_int = True\n",
    "# use_float_input = True\n",
    "load_from_checkpoint = config['load_from_checkpoint']\n",
    "\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = models.load_model('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "else:\n",
    "    if model_name == \"ConvLSTM\":\n",
    "        model = ConvLSTM()\n",
    "        data_mode = 'ACC+GYRO+MAG' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ConvLSTM_6axis\":\n",
    "        model = ConvLSTM(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"ConvLSTM_VGG\":\n",
    "        model = ConvLSTM_VGG()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet\":\n",
    "        model = TinyFallNet()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ResNet24\":\n",
    "        model = ResNet24()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet_6axis\":\n",
    "        model = TinyFallNet(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    else:\n",
    "        print(\"Please select a valid model name\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = config['learning_rate']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "lr_factor = config['lr_factor']\n",
    "patience = config['patience']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n",
      "Data shape:  (75936, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            fall_threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data,\n",
    "                            data_mode)\n",
    "\n",
    "print(\"Data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the rescaling is done only once\n",
    "if train_with_int==True and data.dtype!=np.int8:\n",
    "    dtype_out = np.int8 # rescaled input data type\n",
    "    data = rescale_data(data, dtype_out, acc_max=acc_max, gyro_max=gyro_max)\n",
    "else:\n",
    "    data = data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (75936, 50, 6)\n",
      "Data dtype:  int8\n",
      "in_channels:  6\n",
      "not_fall_size:  75060\n",
      "fall_size:  876\n"
     ]
    }
   ],
   "source": [
    "in_channels = data.shape[2]\n",
    "print(\"Data shape: \", data.shape)\n",
    "print(\"Data dtype: \", data.dtype)\n",
    "print('in_channels: ', in_channels)\n",
    "\n",
    "label = label.astype(np.int64)\n",
    "data_copy = data.reshape(data.shape[0], 50, in_channels)\n",
    "\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('not_fall_size: ', B_size)\t\n",
    "print('fall_size: ', A_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_copy, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the saved_data folder if it does not exist\n",
    "if not os.path.exists('./saved_data'):\n",
    "    os.makedirs('./saved_data')\n",
    "# save the test data, train data and validation data\n",
    "np.save('./saved_data/X_test.npy', X_test)\n",
    "np.save('./saved_data/y_test.npy', y_test)\n",
    "np.save('./saved_data/X_train.npy', X_train)\n",
    "np.save('./saved_data/y_train.npy', y_train)\n",
    "np.save('./saved_data/X_val.npy', X_val)\n",
    "np.save('./saved_data/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 1, 50, 6)             0         ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 1, 48, 64)            1216      ['reshape_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 24, 64)            0         ['conv2d_51[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 1, 24, 16)            1040      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 1, 24, 16)            64        ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_36[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 1, 24, 16)            64        ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_37[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 1, 24, 64)            256       ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 1, 24, 64)            4160      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_52[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)             (None, 1, 24, 64)            0         ['add_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 1, 24, 16)            64        ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_39[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 1, 24, 16)            64        ['conv2d_58[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_40[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 1, 24, 64)            256       ['conv2d_59[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_41[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_56[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)             (None, 1, 24, 64)            0         ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_41[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 1, 24, 16)            64        ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_42[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 1, 24, 16)            64        ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_43[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 1, 24, 64)            256       ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_41[0][0]']            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_44[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_60[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)             (None, 1, 24, 64)            0         ['add_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_44[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 1, 24, 16)            64        ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_45[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 1, 24, 16)            64        ['conv2d_66[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_46[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 1, 24, 64)            256       ['conv2d_67[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_44[0][0]']            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_47[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_64[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)             (None, 1, 24, 64)            0         ['add_15[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (Avera  (None, 1, 12, 64)            0         ['re_lu_47[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 768)                  0         ['average_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 2)                    1538      ['flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32578 (127.26 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "            loss='categorical_crossentropy',\n",
    "            #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 50, 9))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48598, 2)\n",
      "y_val.shape:  (12150, 2)\n",
      "X_train.shape:  (48598, 50, 6)\n",
      "y_train.shape:  (48598, 2)\n",
      "Epoch 1/50\n",
      "760/760 [==============================] - 12s 14ms/step - loss: 0.6117 - accuracy: 0.8882 - val_loss: 0.3834 - val_accuracy: 0.8625 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.3422 - accuracy: 0.9285 - val_loss: 0.1954 - val_accuracy: 0.9324 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.2732 - accuracy: 0.9412 - val_loss: 0.2429 - val_accuracy: 0.9210 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.2452 - accuracy: 0.9480 - val_loss: 0.1071 - val_accuracy: 0.9620 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.3016 - accuracy: 0.9323 - val_loss: 0.0983 - val_accuracy: 0.9595 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.2117 - accuracy: 0.9512 - val_loss: 0.0885 - val_accuracy: 0.9671 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.2442 - accuracy: 0.9461 - val_loss: 0.1512 - val_accuracy: 0.9485 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.1910 - accuracy: 0.9581 - val_loss: 0.1235 - val_accuracy: 0.9561 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.1764 - accuracy: 0.9593 - val_loss: 0.0855 - val_accuracy: 0.9714 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "684/760 [==========================>...] - ETA: 1s - loss: 0.1763 - accuracy: 0.9593"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (23291, 50, 9)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# (23291,)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (360, 50, 6)\n",
      "12/12 - 0s - loss: 1.0735 - accuracy: 0.8000 - 38ms/epoch - 3ms/step\n",
      "Test loss: [1.0734577178955078, 0.800000011920929]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "[[175   1]\n",
      " [ 71 113]]\n",
      "Confusion matrix, without normalization\n",
      "[[175   1]\n",
      " [ 71 113]]\n",
      "accuracy:  0.8\n",
      "f1_score:  0.7583892617449665\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHpCAYAAAChumdzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOoklEQVR4nO3deVhU9f4H8PcMyIDIDIvKSCKguOCGa4S4JonmrqUWJbjmdct9qVDAhdwVc8ny53Y1l1RKLdM0xQVJUNxF3E0ETAQEYp3z+8PL5AgWwwzMHOf98jnP43zPmXM+h8utd9/lHIkgCAKIiIiIjJjU0AUQERER/RsGFiIiIjJ6DCxERERk9BhYiIiIyOgxsBAREZHRY2AhIiIio8fAQkREREaPgYWIiIiMHgMLERERGT0GFiKRS0hIQJcuXaBQKCCRSBAREaHX89+9excSiQQbN27U63nFrGPHjujYsaOhyyAyKQwsRHpw69YtfPLJJ6hduzYsLS0hl8vh4+ODFStW4K+//irXawcEBODSpUuYN28etmzZglatWpXr9SpSYGAgJBIJ5HJ5iT/HhIQESCQSSCQSLF68WOvzJyYmIjg4GHFxcXqolojKk7mhCyASuwMHDuD999+HTCbD4MGD0bhxY+Tl5eHkyZOYOnUqrly5gnXr1pXLtf/66y9ERUXh888/x9ixY8vlGi4uLvjrr79QqVKlcjn/vzE3N0d2djb27duHAQMGaOzbunUrLC0tkZOTU6ZzJyYmIiQkBK6urmjWrFmpv3fo0KEyXY+Iyo6BhUgHd+7cwaBBg+Di4oKjR4+iRo0a6n1jxozBzZs3ceDAgXK7/uPHjwEAtra25XYNiUQCS0vLcjv/v5HJZPDx8cF3331XLLBs27YN3bt3x+7duyukluzsbFSuXBkWFhYVcj0i+huHhIh0sHDhQmRmZmL9+vUaYaWIu7s7Pv30U/XngoICzJkzB3Xq1IFMJoOrqys+++wz5ObmanzP1dUVPXr0wMmTJ/Hmm2/C0tIStWvXxubNm9XHBAcHw8XFBQAwdepUSCQSuLq6Ang+lFL09xcFBwdDIpFotB0+fBht27aFra0tqlSpgvr16+Ozzz5T73/VHJajR4+iXbt2sLa2hq2tLXr37o1r166VeL2bN28iMDAQtra2UCgUGDJkCLKzs1/9g33Jhx9+iJ9//hlpaWnqtrNnzyIhIQEffvhhseNTU1MxZcoUNGnSBFWqVIFcLke3bt1w4cIF9THHjh1D69atAQBDhgxRDy0V3WfHjh3RuHFjxMbGon379qhcubL65/LyHJaAgABYWloWu38/Pz/Y2dkhMTGx1PdKRCVjYCHSwb59+1C7dm20adOmVMcPHz4cs2bNQosWLbBs2TJ06NABYWFhGDRoULFjb968iffeew/vvPMOlixZAjs7OwQGBuLKlSsAgH79+mHZsmUAgA8++ABbtmzB8uXLtar/ypUr6NGjB3JzcxEaGoolS5agV69eOHXq1D9+79dff4Wfnx9SUlIQHByMSZMm4fTp0/Dx8cHdu3eLHT9gwAA8e/YMYWFhGDBgADZu3IiQkJBS19mvXz9IJBLs2bNH3bZt2zY0aNAALVq0KHb87du3ERERgR49emDp0qWYOnUqLl26hA4dOqjDg4eHB0JDQwEAI0eOxJYtW7Blyxa0b99efZ4nT56gW7duaNasGZYvX45OnTqVWN+KFStQrVo1BAQEoLCwEADw9ddf49ChQ1i5ciWcnJxKfa9E9AoCEZVJenq6AEDo3bt3qY6Pi4sTAAjDhw/XaJ8yZYoAQDh69Ki6zcXFRQAgREZGqttSUlIEmUwmTJ48Wd12584dAYCwaNEijXMGBAQILi4uxWqYPXu28OL/7ZctWyYAEB4/fvzKuouusWHDBnVbs2bNhOrVqwtPnjxRt124cEGQSqXC4MGDi11v6NChGufs27ev4ODg8Mprvngf1tbWgiAIwnvvvSd07txZEARBKCwsFJRKpRASElLizyAnJ0coLCwsdh8ymUwIDQ1Vt509e7bYvRXp0KGDAEBYu3Ztifs6dOig0fbLL78IAIS5c+cKt2/fFqpUqSL06dPnX++RiEqHPSxEZZSRkQEAsLGxKdXxP/30EwBg0qRJGu2TJ08GgGJzXRo2bIh27dqpP1erVg3169fH7du3y1zzy4rmvvzwww9QqVSl+s6jR48QFxeHwMBA2Nvbq9ubNm2Kd955R32fLxo1apTG53bt2uHJkyfqn2FpfPjhhzh27BiSkpJw9OhRJCUllTgcBDyf9yKVPv/HW2FhIZ48eaIe7jp37lyprymTyTBkyJBSHdulSxd88sknCA0NRb9+/WBpaYmvv/661Ncion/GwEJURnK5HADw7NmzUh1/7949SKVSuLu7a7QrlUrY2tri3r17Gu21atUqdg47Ozs8ffq0jBUXN3DgQPj4+GD48OFwdHTEoEGDsHPnzn8ML0V11q9fv9g+Dw8P/Pnnn8jKytJof/le7OzsAECre3n33XdhY2ODHTt2YOvWrWjdunWxn2URlUqFZcuWoW7dupDJZKhatSqqVauGixcvIj09vdTXfOONN7SaYLt48WLY29sjLi4O4eHhqF69eqm/S0T/jIGFqIzkcjmcnJxw+fJlrb738qTXVzEzMyuxXRCEMl+jaH5FESsrK0RGRuLXX3/Fxx9/jIsXL2LgwIF45513ih2rC13upYhMJkO/fv2wadMm7N2795W9KwAwf/58TJo0Ce3bt8d///tf/PLLLzh8+DAaNWpU6p4k4PnPRxvnz59HSkoKAODSpUtafZeI/hkDC5EOevTogVu3biEqKupfj3VxcYFKpUJCQoJGe3JyMtLS0tQrfvTBzs5OY0VNkZd7cQBAKpWic+fOWLp0Ka5evYp58+bh6NGj+O2330o8d1Gd8fHxxfZdv34dVatWhbW1tW438Aoffvghzp8/j2fPnpU4UbnI999/j06dOmH9+vUYNGgQunTpAl9f32I/k9KGx9LIysrCkCFD0LBhQ4wcORILFy7E2bNn9XZ+IlPHwEKkg2nTpsHa2hrDhw9HcnJysf23bt3CihUrADwf0gBQbCXP0qVLAQDdu3fXW1116tRBeno6Ll68qG579OgR9u7dq3Fcampqse8WPUDt5aXWRWrUqIFmzZph06ZNGgHg8uXLOHTokPo+y0OnTp0wZ84cfPXVV1Aqla88zszMrFjvza5du/Dw4UONtqJgVVK409b06dNx//59bNq0CUuXLoWrqysCAgJe+XMkIu3wwXFEOqhTpw62bduGgQMHwsPDQ+NJt6dPn8auXbsQGBgIAPD09ERAQADWrVuHtLQ0dOjQAb///js2bdqEPn36vHLJbFkMGjQI06dPR9++fTF+/HhkZ2djzZo1qFevnsak09DQUERGRqJ79+5wcXFBSkoKVq9ejZo1a6Jt27avPP+iRYvQrVs3eHt7Y9iwYfjrr7+wcuVKKBQKBAcH6+0+XiaVSvHFF1/863E9evRAaGgohgwZgjZt2uDSpUvYunUrateurXFcnTp1YGtri7Vr18LGxgbW1tbw8vKCm5ubVnUdPXoUq1evxuzZs9XLrDds2ICOHTsiKCgICxcu1Op8RFQCA69SInot3LhxQxgxYoTg6uoqWFhYCDY2NoKPj4+wcuVKIScnR31cfn6+EBISIri5uQmVKlUSnJ2dhZkzZ2ocIwjPlzV379692HVeXk77qmXNgiAIhw4dEho3bixYWFgI9evXF/773/8WW9Z85MgRoXfv3oKTk5NgYWEhODk5CR988IFw48aNYtd4eenvr7/+Kvj4+AhWVlaCXC4XevbsKVy9elXjmKLrvbxsesOGDQIA4c6dO6/8mQqC5rLmV3nVsubJkycLNWrUEKysrAQfHx8hKiqqxOXIP/zwg9CwYUPB3Nxc4z47dOggNGrUqMRrvniejIwMwcXFRWjRooWQn5+vcdzEiRMFqVQqREVF/eM9ENG/kwiCFrPeiIiIiAyAc1iIiIjI6DGwEBERkdFjYCEiIiKjx8BCRERERo+BhYiIiIweAwsREREZPT44roKoVCokJibCxsZGr48DJyKiiiUIAp49ewYnJyf1W8HLW05ODvLy8vRyLgsLC1haWurlXBWJgaWCJCYmwtnZ2dBlEBGRnjx48AA1a9Ys9+vk5OTAysYBKMjWy/mUSiXu3LkjutDCwFJBbGxsAAAWDQMgMSv96+qJxOb+scWGLoGoXD3LyIC7m7P6n+vlLS8vDyjIhqzREEDXf38U5iHpygbk5eUxsFDJioaBJGYWDCz0WpPL5YYugahCVPjwvh7+/SHmR9szsBAREYmBBICuIUnEUygZWIiIiMRAIn2+6XoOkRJv5URERGQy2MNCREQkBhKJHoaExDsmxMBCREQkBiY+JMTAQkREJAYm3sMi3qhFREREJoM9LERERKKghyEhEfdTMLAQERGJAYeEiIiIiIwbe1iIiIjEgKuEiIiIyOhxSIiIiIjIuLGHhYiISAw4JERERERGj0NCRERERMaNPSxERERiwCEhIiIiMnoSiR4CC4eEiIiIiMoNe1iIiIjEQCp5vul6DpFiYCEiIhIDzmEhIiIio8dlzURERETGjT0sREREYsAhISIiIjJ6HBIiIiIiKi4yMhI9e/aEk5MTJBIJIiIiih1z7do19OrVCwqFAtbW1mjdujXu37+v3p+Tk4MxY8bAwcEBVapUQf/+/ZGcnKx1LQwsREREYlA0JKTrpoWsrCx4enpi1apVJe6/desW2rZtiwYNGuDYsWO4ePEigoKCYGlpqT5m4sSJ2LdvH3bt2oXjx48jMTER/fr10/r2OSREREQkBgYYEurWrRu6dev2yv2ff/453n33XSxcuFDdVqdOHfXf09PTsX79emzbtg1vv/02AGDDhg3w8PDAmTNn8NZbb5W6FvawEBERmZiMjAyNLTc3V+tzqFQqHDhwAPXq1YOfnx+qV68OLy8vjWGj2NhY5Ofnw9fXV93WoEED1KpVC1FRUVpdj4GFiIhIDPQ4JOTs7AyFQqHewsLCtC4nJSUFmZmZ+PLLL9G1a1ccOnQIffv2Rb9+/XD8+HEAQFJSEiwsLGBra6vxXUdHRyQlJWl1PQ4JERERiYEeh4QePHgAuVyubpbJZFqfSqVSAQB69+6NiRMnAgCaNWuG06dPY+3atejQoYNutb6EPSxEREQmRi6Xa2xlCSxVq1aFubk5GjZsqNHu4eGhXiWkVCqRl5eHtLQ0jWOSk5OhVCq1uh4DCxERkSjoYzhIf//at7CwQOvWrREfH6/RfuPGDbi4uAAAWrZsiUqVKuHIkSPq/fHx8bh//z68vb21uh6HhIiIiMTAAKuEMjMzcfPmTfXnO3fuIC4uDvb29qhVqxamTp2KgQMHon379ujUqRMOHjyIffv24dixYwAAhUKBYcOGYdKkSbC3t4dcLse4cePg7e2t1QohgIGFiIhIHCQSPTyaX7vAEhMTg06dOqk/T5o0CQAQEBCAjRs3om/fvli7di3CwsIwfvx41K9fH7t370bbtm3V31m2bBmkUin69++P3Nxc+Pn5YfXq1dqXLgiCoPW3SGsZGRlQKBSQNRkBiZmFocshKjdPz35l6BKIylVGRgYcHRRIT0/XmLhantdTKBSQdVkISSUrnc4l5P+F3EPTKqx2fWIPCxERkRjw5YdERERk9PjyQyIiIiLjxh4WIiIiMeCQEBERERk9DgkRERERGTf2sBAREYkBh4SIiIjI6HFIiIiIiMi4sYeFiIhIBCQSCSQm3MPCwEJERCQCph5YOCRERERERo89LERERGIg+d+m6zlEioGFiIhIBEx9SIiBhYiISARMPbBwDgsREREZPfawEBERiYCp97AwsBAREYmAqQcWDgkRERGR0WMPCxERkRhwWTMREREZOw4JERERERk59rAQERGJgEQCPfSw6KcWQ2BgISIiEgEJ9DAkJOLEwiEhIiIiMnrsYSEiIhIBU590y8BCREQkBia+rJlDQkRERGT02MNCREQkBnoYEhI4JERERETlSR9zWHRfZWQ4DCxEREQiYOqBhXNYiIiIyOgxsBAREYmBRE+bFiIjI9GzZ084OTlBIpEgIiLilceOGjUKEokEy5cv12hPTU2Fv78/5HI5bG1tMWzYMGRmZmpXCBhYiIiIRKFoSEjXTRtZWVnw9PTEqlWr/vG4vXv34syZM3Byciq2z9/fH1euXMHhw4exf/9+REZGYuTIkVrVAXAOCxERkcnJyMjQ+CyTySCTyYod161bN3Tr1u0fz/Xw4UOMGzcOv/zyC7p3766x79q1azh48CDOnj2LVq1aAQBWrlyJd999F4sXLy4x4LwKe1iIiIhEQJ89LM7OzlAoFOotLCysTDWpVCp8/PHHmDp1Kho1alRsf1RUFGxtbdVhBQB8fX0hlUoRHR2t1bXYw0JERCQC+lwl9ODBA8jlcnV7Sb0rpbFgwQKYm5tj/PjxJe5PSkpC9erVNdrMzc1hb2+PpKQkra7FwEJERGRi5HK5RmApi9jYWKxYsQLnzp2rkOXSHBIiIiISAUNMuv0nJ06cQEpKCmrVqgVzc3OYm5vj3r17mDx5MlxdXQEASqUSKSkpGt8rKChAamoqlEqlVtdjDwsREZEYGNnLDz/++GP4+vpqtPn5+eHjjz/GkCFDAADe3t5IS0tDbGwsWrZsCQA4evQoVCoVvLy8tLoeAwsRERGVKDMzEzdv3lR/vnPnDuLi4mBvb49atWrBwcFB4/hKlSpBqVSifv36AAAPDw907doVI0aMwNq1a5Gfn4+xY8di0KBBWq0QAhhYiIiIRMEQj+aPiYlBp06d1J8nTZoEAAgICMDGjRtLdY6tW7di7Nix6Ny5M6RSKfr374/w8HCt6gAYWIiIiETBEIGlY8eOEASh1MffvXu3WJu9vT22bdum1XVLwsBCREQkAqb+8kMGFhIVnxZ1MHGwL1o0rIUa1RQYMHEd9h27qN7/1/mvSvzeZ8v2YtnmIwCA6wdC4OKkOe4aFP4DFm84XH6FE+nZyRORWLZkEc6di0XSo0fY8f1e9Ordx9BlEZUbBhYSFWsrGS7deIjNP0Rhx9Li76Jw9Z2p8bmLTyOsnf0h9h6J02gPWb0fG/acUn9+lpVbLvUSlZesrCw0aeqJwYFDMej9foYuhyqCka0SqmgMLCQqh05dxaFTV1+5P/nJM43PPTs2wfGzCbj78IlGe2ZWTrFjicTEr2s3+HX953e80OvF1IeE+OA4em1Vt7dB17aNsSkiqti+yUO64I/fFiDqu+mYOLgzzMz4fwUiImPGHhZ6bX3U0wvPsnMQcTROo331d8dx/toDPM3IwluetRE6rheU1RSYvmSPYQolIioFU+9hYWDRQmBgINLS0hAREQHg+XKvZs2aYfny5Qati0o2uPdb2PFzDHLzCjTaw/97VP33ywmJyMsvwFeff4Cg8B+Rl1/w8mmIiIyCBHoILCKexGLQfvDAwEBIJBJ8+eWXGu0RERFa/4/i6upaquDg6upa7L0KNWvW1OpaZPx8mtdBfTclNuw9/a/Hnr10F5UqmcHFyb4CKiMiorIw+MC9paUlFixYgKdPn1bYNUNDQ/Ho0SP1dv78+Qq7NlWMgD7eiL16H5duPPzXYz3r10RhoQqPUzkJl4iMl7G9/LCiGTyw+Pr6QqlUIiws7B+P2717Nxo1agSZTAZXV1csWbJEva9jx464d+8eJk6cWKr/QWxsbKBUKtVbtWrVUFhYiGHDhsHNzQ1WVlaoX78+VqxYoZd7JP2xtrJA03pvoGm9NwAArm84oGm9N+CstFMfY2NtiX7vNMfGEnpXvJq6YeyHHdGk3htwfcMBg7q1woIp/fHdT2eR9uyvCrsPIl1lZmbiQlwcLsTFAQDu3rmDC3FxuH//vmELo/Ij0dMmUgafw2JmZob58+fjww8/xPjx40scnomNjcWAAQMQHByMgQMH4vTp0xg9ejQcHBwQGBiIPXv2wNPTEyNHjsSIESPKVIdKpULNmjWxa9cuODg44PTp0xg5ciRq1KiBAQMGaH2+3Nxc5Ob+/WyPjIyMMtVFmlo0dMGhbz9Vf144pT8AYMuPZzBy9n8BAO/7tYQEEuw8GFPs+7l5+XjfryU+H/UuZJXMcTfxCVZu/Q3hW44WO5bImJ2LjYGf79/veJk+9fk7Xj76OADf/N9GA1VFVH4MHlgAoG/fvmjWrBlmz56N9evXF9u/dOlSdO7cGUFBQQCAevXq4erVq1i0aBECAwNhb28PMzMzdc/Jv5k+fTq++OIL9ef58+dj/PjxCAkJUbe5ubkhKioKO3fuLFNgCQsL0zgf6ceJ2ARYNR/7j8f8355T+L8XHgr3orjrf6BDwJIS9xGJSfsOHfFXfunf8ULiZ+qrhAw+JFRkwYIF2LRpE65du1Zs37Vr1+Dj46PR5uPjg4SEBBQWFmp9ralTpyIuLk69DR48GACwatUqtGzZEtWqVUOVKlWwbt26Mnevzpw5E+np6ertwYMHZToPERERwDksRtHDAgDt27eHn58fZs6cicDAwHK9VtWqVeHu7q7Rtn37dkyZMgVLliyBt7c3bGxssGjRIkRHR5fpGjKZDDKZTB/lEhERmTyjCSwA8OWXX6JZs2aoX7++RruHhwdOndLs4j916hTq1asHMzMzAICFhUWZeltePF+bNm0wevRoddutW7fKfD4iIiJ9kkieb7qeQ6yMZkgIAJo0aQJ/f3+Eh4drtE+ePBlHjhzBnDlzcOPGDWzatAlfffUVpkyZoj7G1dUVkZGRePjwIf7880+tr123bl3ExMTgl19+wY0bNxAUFISzZ8/qfE9ERET68Dyw6DokZOi7KDujCizA82ekqFQqjbYWLVpg586d2L59Oxo3boxZs2YhNDRUY+goNDQUd+/eRZ06dVCtWjWtr/vJJ5+gX79+GDhwILy8vPDkyRON3hYiIiKDkvzdy1LWTczLmiWCIHCaeQXIyMiAQqGArMkISMwsDF0OUbl5evYrQ5dAVK4yMjLg6KBAeno65HJ5hVxPoVCg9vjvYSaz1ulchblZuB3+XoXVrk9GNYeFiIiISmbqy5oZWIiIiESAk26JiIiIjBx7WIiIiERAKpVAKtWti0TQ8fuGxMBCREQkAhwSIiIiIjJy7GEhIiISAa4SIiIiIqPHISEiIiIiI8ceFiIiIhHgkBAREREZPQYWIiIiMnqcw0JERERk5BhYiIiIREACiXpYqMwbtOtiiYyMRM+ePeHk5ASJRIKIiAj1vvz8fEyfPh1NmjSBtbU1nJycMHjwYCQmJmqcIzU1Ff7+/pDL5bC1tcWwYcOQmZmp9f0zsBAREYlA0ZCQrps2srKy4OnpiVWrVhXbl52djXPnziEoKAjnzp3Dnj17EB8fj169emkc5+/vjytXruDw4cPYv38/IiMjMXLkSK3vn3NYiIiIqETdunVDt27dStynUChw+PBhjbavvvoKb775Ju7fv49atWrh2rVrOHjwIM6ePYtWrVoBAFauXIl3330XixcvhpOTU6lrYQ8LERGRCOg8HPTCKqOMjAyNLTc3Vy81pqenQyKRwNbWFgAQFRUFW1tbdVgBAF9fX0ilUkRHR2t1bgYWIiIiEdDnkJCzszMUCoV6CwsL07m+nJwcTJ8+HR988AHkcjkAICkpCdWrV9c4ztzcHPb29khKStLq/BwSIiIiMjEPHjxQhwoAkMlkOp0vPz8fAwYMgCAIWLNmja7llYiBhYiISAT0+eA4uVyuEVh0URRW7t27h6NHj2qcV6lUIiUlReP4goICpKamQqlUanUdDgkRERGJgCFWCf2borCSkJCAX3/9FQ4ODhr7vb29kZaWhtjYWHXb0aNHoVKp4OXlpdW12MNCREREJcrMzMTNmzfVn+/cuYO4uDjY29ujRo0aeO+993Du3Dns378fhYWF6nkp9vb2sLCwgIeHB7p27YoRI0Zg7dq1yM/Px9ixYzFo0CCtVggBDCxERESiYIh3CcXExKBTp07qz5MmTQIABAQEIDg4GD/++CMAoFmzZhrf++2339CxY0cAwNatWzF27Fh07twZUqkU/fv3R3h4uNa1M7AQERGJgT6GdLT8fseOHSEIwiv3/9O+Ivb29ti2bZt2Fy4B57AQERGR0WMPCxERkQgYYkjImDCwEBERiYA+VvmIOK8wsBAREYmBqfewcA4LERERGT32sBAREYkAh4SIiIjI6HFIiIiIiMjIsYeFiIhIBEy9h4WBhYiISARMfQ4Lh4SIiIjI6LGHhYiISAQ4JERERERGj0NCREREREaOPSxEREQiwCEhIiIiMnoS6GFISC+VGAYDCxERkQhIJRJIdUwsun7fkDiHhYiIiIwee1iIiIhEwNRXCTGwEBERiYCpT7rlkBAREREZPfawEBERiYBU8nzT9RxixcBCREQkBhI9DOmIOLBwSIiIiIiMHntYiIiIRICrhIiIiMjoSf73R9dziBWHhIiIiMjosYeFiIhIBLhKiIiIiIweHxxHREREZORK1cPy448/lvqEvXr1KnMxREREVDKuEiqFPn36lOpkEokEhYWFutRDREREJZBKJJDqmDh0/b4hlWpISKVSlWpjWCEiIiofRT0sum7aiIyMRM+ePeHk5ASJRIKIiAiN/YIgYNasWahRowasrKzg6+uLhIQEjWNSU1Ph7+8PuVwOW1tbDBs2DJmZmVrfv05zWHJycnT5OhERERmxrKwseHp6YtWqVSXuX7hwIcLDw7F27VpER0fD2toafn5+GvnA398fV65cweHDh7F//35ERkZi5MiRWteidWApLCzEnDlz8MYbb6BKlSq4ffs2ACAoKAjr16/XugAiIiL6d0WrhHTdtNGtWzfMnTsXffv2LbZPEAQsX74cX3zxBXr37o2mTZti8+bNSExMVPfEXLt2DQcPHsS3334LLy8vtG3bFitXrsT27duRmJioVS1aB5Z58+Zh48aNWLhwISwsLNTtjRs3xrfffqvt6YiIiKgU9DkklJGRobHl5uZqXc+dO3eQlJQEX19fdZtCoYCXlxeioqIAAFFRUbC1tUWrVq3Ux/j6+kIqlSI6Olqr62kdWDZv3ox169bB398fZmZm6nZPT09cv35d29MRERFRBXN2doZCoVBvYWFhWp8jKSkJAODo6KjR7ujoqN6XlJSE6tWra+w3NzeHvb29+pjS0vrBcQ8fPoS7u3uxdpVKhfz8fG1PR0RERKWgz1VCDx48gFwuV7fLZDKdzlsRtO5hadiwIU6cOFGs/fvvv0fz5s31UhQRERFpkuhpAwC5XK6xlSWwKJVKAEBycrJGe3JysnqfUqlESkqKxv6CggKkpqaqjyktrXtYZs2ahYCAADx8+BAqlQp79uxBfHw8Nm/ejP3792t7OiIiIhIhNzc3KJVKHDlyBM2aNQPwfG5MdHQ0/vOf/wAAvL29kZaWhtjYWLRs2RIAcPToUahUKnh5eWl1Pa0DS+/evbFv3z6EhobC2toas2bNQosWLbBv3z6888472p6OiIiISsEQ7xLKzMzEzZs31Z/v3LmDuLg42Nvbo1atWpgwYQLmzp2LunXrws3NDUFBQXByclI/cNbDwwNdu3bFiBEjsHbtWuTn52Ps2LEYNGgQnJyctKqlTC8/bNeuHQ4fPlyWrxIREVEZGOJtzTExMejUqZP686RJkwAAAQEB2LhxI6ZNm4asrCyMHDkSaWlpaNu2LQ4ePAhLS0v1d7Zu3YqxY8eic+fOkEql6N+/P8LDw7Wuvcxva46JicG1a9cAPJ/XUtTVQ0RERK+Hjh07QhCEV+6XSCQIDQ1FaGjoK4+xt7fHtm3bdK5F68Dyxx9/4IMPPsCpU6dga2sLAEhLS0ObNm2wfft21KxZU+eiiIiISJMhhoSMidarhIYPH478/Hxcu3YNqampSE1NxbVr16BSqTB8+PDyqJGIiIhQse8RMjZa97AcP34cp0+fRv369dVt9evXx8qVK9GuXTu9FkdEREQElCGwODs7l/iAuMLCQq1n/BIREVHpcEhIS4sWLcK4ceMQExOjbouJicGnn36KxYsX67U4IiIieq5olZCum1iVqofFzs5OI5VlZWXBy8sL5ubPv15QUABzc3MMHTpUvfaaiIiI9MfUe1hKFViWL19ezmUQERERvVqpAktAQEB510FERET/4MV3AelyDrEq84PjACAnJwd5eXkabS++/ZGIiIj0Q59vaxYjrSfdZmVlYezYsahevTqsra1hZ2ensRERERHpm9aBZdq0aTh69CjWrFkDmUyGb7/9FiEhIXBycsLmzZvLo0YiIiKTp+tD48T+8Dith4T27duHzZs3o2PHjhgyZAjatWsHd3d3uLi4YOvWrfD39y+POomIiEyaqa8S0rqHJTU1FbVr1wbwfL5KamoqAKBt27aIjIzUb3VEREREKENgqV27Nu7cuQMAaNCgAXbu3Angec9L0csQiYiISL9MfUhI68AyZMgQXLhwAQAwY8YMrFq1CpaWlpg4cSKmTp2q9wKJiIjo71VCum5ipfUclokTJ6r/7uvri+vXryM2Nhbu7u5o2rSpXosjIiIiAnR8DgsAuLi4wMXFRR+1EBER0SvoY0hHxB0spQss4eHhpT7h+PHjy1wMERERlczUVwmVKrAsW7asVCeTSCQMLP9izuKxsLK2MXQZROWmS/hJQ5dAVK4KcrIMcl0pyjDxtIRziFWpAkvRqiAiIiIiQ9B5DgsRERGVPw4JERERkdGTSACpCU+6FfNwFhEREZkI9rAQERGJgFQPPSy6ft+QGFiIiIhEwNTnsJRpSOjEiRP46KOP4O3tjYcPHwIAtmzZgpMnuZyRiIiI9E/rwLJ79274+fnBysoK58+fR25uLgAgPT0d8+fP13uBRERE9PeQkK6bWGkdWObOnYu1a9fim2++QaVKldTtPj4+OHfunF6LIyIiouf4tmYtxcfHo3379sXaFQoF0tLS9FETERERkQatA4tSqcTNmzeLtZ88eRK1a9fWS1FERESkSSqR6GUTK60Dy4gRI/Dpp58iOjoaEokEiYmJ2Lp1K6ZMmYL//Oc/5VEjERGRyZPqaRMrrZc1z5gxAyqVCp07d0Z2djbat28PmUyGKVOmYNy4ceVRIxEREZk4rcOWRCLB559/jtTUVFy+fBlnzpzB48ePMWfOnPKoj4iIiGCYSbeFhYUICgqCm5sbrKysUKdOHcyZMweCIKiPEQQBs2bNQo0aNWBlZQVfX18kJCTo+e51eHCchYUFGjZsqM9aiIiI6BWk0H0OihTafX/BggVYs2YNNm3ahEaNGiEmJgZDhgyBQqHA+PHjAQALFy5EeHg4Nm3aBDc3NwQFBcHPzw9Xr16FpaWlTvW+SOvA0qlTp398Ut7Ro0d1KoiIiIiK08eyZG2/f/r0afTu3Rvdu3cHALi6uuK7777D77//DuB578ry5cvxxRdfoHfv3gCAzZs3w9HRERERERg0aJBuBb9A6yGhZs2awdPTU701bNgQeXl5OHfuHJo0aaK3woiIiKh8ZGRkaGxFD4F9WZs2bXDkyBHcuHEDAHDhwgWcPHkS3bp1AwDcuXMHSUlJ8PX1VX9HoVDAy8sLUVFReq1Z6x6WZcuWldgeHByMzMxMnQsiIiKi4vT58kNnZ2eN9tmzZyM4OLjY8TNmzEBGRgYaNGgAMzMzFBYWYt68efD39wcAJCUlAQAcHR01vufo6Kjepy96e/nhRx99hDfffBOLFy/W1ymJiIjofyQS6DyHpejrDx48gFwuV7fLZLISj9+5cye2bt2Kbdu2oVGjRoiLi8OECRPg5OSEgIAAnWrRlt4CS1RUlF4n1xAREVH5kMvlGoHlVaZOnYoZM2ao56I0adIE9+7dQ1hYGAICAqBUKgEAycnJqFGjhvp7ycnJaNasmV5r1jqw9OvXT+OzIAh49OgRYmJiEBQUpLfCiIiI6G+GmHSbnZ0NqVRzuquZmRlUKhUAwM3NDUqlEkeOHFEHlIyMDERHR+v9YbJaBxaFQqHxWSqVon79+ggNDUWXLl30VhgRERH9TZ9zWEqrZ8+emDdvHmrVqoVGjRrh/PnzWLp0KYYOHQrg+bPZJkyYgLlz56Ju3brqZc1OTk7o06ePbsW+RKvAUlhYiCFDhqBJkyaws7PTayFERERkXFauXImgoCCMHj0aKSkpcHJywieffIJZs2apj5k2bRqysrIwcuRIpKWloW3btjh48KDep4loFVjMzMzQpUsXXLt2jYGFiIioAkn+90fXc2jDxsYGy5cvx/Lly199TokEoaGhCA0N1am2f6P1c1gaN26M27dvl0ctRERE9ApFQ0K6bmKldWCZO3cupkyZgv379+PRo0fFHj5DREREpG+lHhIKDQ3F5MmT8e677wIAevXqpfGIfkEQIJFIUFhYqP8qiYiITJwhJt0ak1IHlpCQEIwaNQq//fZbedZDREREJZBIJP/4Lr/SnkOsSh1Yil4l3aFDh3IrhoiIiEpm6j0sWs1hEXMyIyIiIvHSallzvXr1/jW0pKam6lQQERERFWeIJ90aE60CS0hISLEn3RIREVH5k0okOr/8UNfvG5JWgWXQoEGoXr16edVCREREVKJSBxbOXyEiIjIcU590q/UqISIiIjIAPcxh0fHJ/gZV6sBS9CppIiIiooqm1RwWIiIiMgwpJJDq2EWi6/cNiYGFiIhIBEx9WbPWLz8kIiIiqmjsYSEiIhIBrhIiIiIio2fqD47jkBAREREZPfawEBERiYCpT7plYCEiIhIBKfQwJMRlzURERFSeTL2HhXNYiIiIyOixh4WIiEgEpNC9l0HMvRQMLERERCIgkUgg0XFMR9fvG5KYwxYRERGZCPawEBERiYDkf5uu5xArBhYiIiIR4JNuiYiIiIwce1iIiIhEQrz9I7pjYCEiIhIBPjiOiIiIyMgxsBAREYlA0XNYdN209fDhQ3z00UdwcHCAlZUVmjRpgpiYGPV+QRAwa9Ys1KhRA1ZWVvD19UVCQoI+bx0AAwsREZEoSPW0aePp06fw8fFBpUqV8PPPP+Pq1atYsmQJ7Ozs1McsXLgQ4eHhWLt2LaKjo2FtbQ0/Pz/k5OTodL8v4xwWIiIiETDEk24XLFgAZ2dnbNiwQd3m5uam/rsgCFi+fDm++OIL9O7dGwCwefNmODo6IiIiAoMGDdKp3hexh4WIiMjEZGRkaGy5ubklHvfjjz+iVatWeP/991G9enU0b94c33zzjXr/nTt3kJSUBF9fX3WbQqGAl5cXoqKi9FozAwsREZEISPS0AYCzszMUCoV6CwsLK/Gat2/fxpo1a1C3bl388ssv+M9//oPx48dj06ZNAICkpCQAgKOjo8b3HB0d1fv0hUNCREREIqDPIaEHDx5ALper22UyWYnHq1QqtGrVCvPnzwcANG/eHJcvX8batWsREBCgUy3aYg8LERGRiZHL5RrbqwJLjRo10LBhQ402Dw8P3L9/HwCgVCoBAMnJyRrHJCcnq/fpCwMLERGRCBhilZCPjw/i4+M12m7cuAEXFxcAzyfgKpVKHDlyRL0/IyMD0dHR8Pb21vJq/4xDQkRERCJgiFVCEydORJs2bTB//nwMGDAAv//+O9atW4d169apzzdhwgTMnTsXdevWhZubG4KCguDk5IQ+ffroVOvLGFiIiIioRK1bt8bevXsxc+ZMhIaGws3NDcuXL4e/v7/6mGnTpiErKwsjR45EWloa2rZti4MHD8LS0lKvtTCwEBERicCLq3x0OYe2evTogR49erz6nBIJQkNDERoaWvbCSoGBhYiISAT48kMiIiIiI8ceFiIiIhGQQgKpjoNCun7fkBhYiIiIRIBDQkRERERGjj0sREREIiD53x9dzyFWDCxEREQiYOpDQgwsREREIiDRw6RbMfewcA4LERERGT32sBAREYkAh4SIiIjI6Jl6YOGQEBERERk99rAQERGJAJc1ExERkdGTSp5vup5DrDgkREREREaPPSxEREQiwCEhIiIiMnqmvkqIgYVEbfZ77ZCa9LBYe7u+H2HA5FCc+uE7xBz+EX/cuIKc7Ews+DkOlW3kBqiUqPQ835BjUKuaqO9ojapVZPjsh6s4eStVvb+9uwN6N1WinmMVKKwqYeiW87j5OEvjHFN866BlLVtUrWKBv/JUuJyYgbUn7uL+078q+naI9IKBhURtyjcREFQq9efE2/FYNXEwmnd6FwCQl/sXPLzaw8OrPfZ9vchQZRJpxbKSGW49zsRPV5Ixr5dHCfuluJiYgaM3/sT0LnVLPEd8ciYOX3uM5Ge5kFuaY4h3LSzp3wgD18dAJZT3HVB5kED3IR0Rd7AwsJC42dg5aHw+/N81qPqGC9ybewEAOg0YCgBIOHemwmsjKqvou08RfffpK/cfuvYYAKCUy155zL5Lyeq/J2Xk4ptT97BxcAso5ZZITM/RX7FUYbhKiOg1UZCfh7OHfsBb3d+DRMwDtUR6ZmkuxbuNHJGYloOUZ7mGLofKSKKnP2LFwFJKGzduhK2trfpzcHAwmjVrZrB6qLiLkYfxV2YG3nr3PUOXQmQU+ngqcXCsNw6NbwMvNztM2n0ZBRwPIpEyucASGBgIiURSbLt586ahSyMdRR3YiYZeHaCo6mjoUoiMwuFrjzH8v+cxbsdF/PH0L4T0aAALM/H+F7apK1olpOsmViYXWACga9euePTokcbm5uZm6LJIB6lJDxEfcwrePQcauhQio5GVV4g/0nJw4WEGgvZdRy17K7Rzd/j3L5JRkuhpEyuTDCwymQxKpVJjW7FiBZo0aQJra2s4Oztj9OjRyMzMNHSpVEpnDuyCjZ0DGnl3MnQpREZJInn+L6tKZib5j316DXCV0P9IpVKEh4fDzc0Nt2/fxujRozFt2jSsXr26TOfLzc1Fbu7fk9syMjL0VSq9RKVS4cxP3+PNrv1gZq75K53x5DEyUh/j8cN7AIDE29dhWbkK7BydYC23NUC1RP/OqpIUb9haqT/XUFjCvZo1MnIKkPIsFzaW5nC0kaFqFQsAQC2758emZuUhNTsfNRQyvF2vGs7ee4q0vwpQvYoF/N+sidwCFc7cefXqIzJuUkgg1XFMRyriPhaTDCz79+9HlSpV1J+7deuGXbt2qT+7urpi7ty5GDVqVJkDS1hYGEJCQnSulf5dfMwpPE1OhHf394vtOxmxFT9vCFd/XjFmEADA/7OFnJxLRqu+ow3CBzRRfx7XsTYA4OcryQj7JQE+te3xWdd66v3BPRoAADZE3ceGqPvIKxDgWVOO91s4wcbSHE+z83Hhj3SM3n4RaX/lV+zNkN7oY0hHvHHFRANLp06dsGbNGvVna2tr/PrrrwgLC8P169eRkZGBgoIC5OTkIDs7G5UrV9b6GjNnzsSkSZPUnzMyMuDs7KyX+kmTx5vtsPLk7RL3vTtsAt4dNqFiCyLSUdwf6Wi/9OQr9x+8moKDV1Neuf9JVh6m7b1aHqURGYxJDmZaW1vD3d1dveXm5qJHjx5o2rQpdu/ejdjYWKxatQoAkJeXV6ZryGQyyOVyjY2IiKjMTHzWrUn2sLwsNjYWKpUKS5YsgVT6PMPt3LnTwFURERH9zdTf1mySPSwvc3d3R35+PlauXInbt29jy5YtWLt2raHLIiIiov9hYAHg6emJpUuXYsGCBWjcuDG2bt2KsLAwQ5dFRET0N308NE68HSymF1g2btyIiIiIYu0TJ05EYmIisrOzcfDgQXz88ccQBEH9OP7AwECkpaWpjw8ODkZcXFyF1ExERGToKSxffvklJBIJJkyYoG7LycnBmDFj4ODggCpVqqB///5ITk5+9Ul0YHKBhYiIiLRz9uxZfP3112jatKlG+8SJE7Fv3z7s2rULx48fR2JiIvr161cuNTCwEBERiYGBulgyMzPh7++Pb775BnZ2dur29PR0rF+/HkuXLsXbb7+Nli1bYsOGDTh9+jTOnDlT9vt8BQYWIiIiEZDo6Q/w/NlgL24vPpn9ZWPGjEH37t3h6+ur0R4bG4v8/HyN9gYNGqBWrVqIiorS+/0zsBAREYmAPt/W7OzsDIVCod5etdBk+/btOHfuXIn7k5KSYGFhoZ7rWcTR0RFJSUn6vn0+h4WIiMjUPHjwQOOBpjKZrMRjPv30Uxw+fBiWlpYVWV6J2MNCREQkAvqcwvLyk9hLCiyxsbFISUlBixYtYG5uDnNzcxw/fhzh4eEwNzeHo6Mj8vLyNFbQAkBycjKUSqXe7589LERERGJQwW8/7Ny5My5duqTRNmTIEDRo0ADTp0+Hs7MzKlWqhCNHjqB///4AgPj4eNy/fx/e3t46FlocAwsREREVY2Njg8aNG2u0WVtbw8HBQd0+bNgwTJo0Cfb29pDL5Rg3bhy8vb3x1ltv6b0eBhYiIiIRMMZ3CS1btgxSqRT9+/dHbm4u/Pz8sHr1ar1eowgDCxERkQi8uMpHl3Po4tixYxqfLS0tsWrVKqxatUq3E5cCJ90SERGR0WMPCxERkQhU8Jxbo8PAQkREJAYmnlg4JERERERGjz0sREREImCMq4QqEgMLERGRCBjDKiFD4pAQERERGT32sBAREYmAic+5ZWAhIiISBRNPLAwsREREImDqk245h4WIiIiMHntYiIiIRMDUVwkxsBAREYmAiU9h4ZAQERERGT/2sBAREYmBiXexMLAQERGJAFcJERERERk59rAQERGJAFcJERERkdEz8SksHBIiIiIi48ceFiIiIjEw8S4WBhYiIiIRMPVVQgwsREREYqCHSbciziucw0JERETGjz0sREREImDiU1gYWIiIiETBxBMLh4SIiIjI6LGHhYiISAS4SoiIiIiMnqk/mp9DQkRERGT02MNCREQkAiY+55aBhYiISBRMPLFwSIiIiIhKFBYWhtatW8PGxgbVq1dHnz59EB8fr3FMTk4OxowZAwcHB1SpUgX9+/dHcnKy3mthYCEiIhIBiZ7+aOP48eMYM2YMzpw5g8OHDyM/Px9dunRBVlaW+piJEydi37592LVrF44fP47ExET069dP37fPISEiIiIxkEAPq4S0PP7gwYManzdu3Ijq1asjNjYW7du3R3p6OtavX49t27bh7bffBgBs2LABHh4eOHPmDN566y3dCn4Be1iIiIhMTEZGhsaWm5tbqu+lp6cDAOzt7QEAsbGxyM/Ph6+vr/qYBg0aoFatWoiKitJrzQwsREREIiDR0wYAzs7OUCgU6i0sLOxfr69SqTBhwgT4+PigcePGAICkpCRYWFjA1tZW41hHR0ckJSXpdsMv4ZAQERGRCOjzwXEPHjyAXC5Xt8tksn/97pgxY3D58mWcPHlStyLKiIGFiIhIFPS3rlkul2sEln8zduxY7N+/H5GRkahZs6a6XalUIi8vD2lpaRq9LMnJyVAqlTrWqolDQkRERFQiQRAwduxY7N27F0ePHoWbm5vG/pYtW6JSpUo4cuSIui0+Ph7379+Ht7e3XmthDwsREZEIGOJdQmPGjMG2bdvwww8/wMbGRj0vRaFQwMrKCgqFAsOGDcOkSZNgb28PuVyOcePGwdvbW68rhAAGFiIiIlEwxINu16xZAwDo2LGjRvuGDRsQGBgIAFi2bBmkUin69++P3Nxc+Pn5YfXq1TpWWhwDCxEREZVIEIR/PcbS0hKrVq3CqlWryrUWBhYiIiIRMMSQkDFhYCEiIhKBsjxav6RziBVXCREREZHRYw8LERGRGBhi1q0RYWAhIiISARPPKxwSIiIiIuPHHhYiIiIR4CohIiIiMnqmvkqIgYWIiEgMTHwSC+ewEBERkdFjDwsREZEImHgHCwMLERGRGJj6pFsOCREREZHRYw8LERGRKOi+SkjMg0IMLERERCLAISEiIiIiI8fAQkREREaPQ0JEREQiwCEhIiIiIiPHHhYiIiIR4LuEiIiIyOhxSIiIiIjIyLGHhYiISAT4LiEiIiIyfiaeWBhYiIiIRMDUJ91yDgsREREZPfawEBERiYCprxJiYCEiIhIBE5/CwiEhIiIiMn7sYSEiIhIDE+9iYWAhIiISAa4SIiIiIjJy7GGpIIIgAABysjINXAlR+SrIyTJ0CUTlquh3vOif6xXl2bMMnVf5PHuWoZ9iDICBpYI8e/YMADCrn4+BKyEiIn149uwZFApFuV/HwsICSqUSdd2c9XI+pVIJCwsLvZyrIkmEio6IJkqlUiExMRE2NjaQiHkhvIhkZGTA2dkZDx48gFwuN3Q5ROWCv+cVTxAEPHv2DE5OTpBKK2ZmRU5ODvLy8vRyLgsLC1haWurlXBWJPSwVRCqVombNmoYuwyTJ5XL+g5xee/w9r1gV0bPyIktLS1GGDH3ipFsiIiIyegwsREREZPQYWOi1JZPJMHv2bMhkMkOXQlRu+HtOpoKTbomIiMjosYeFiIiIjB4DCxERERk9BhYiIiIyegwsREREZPQYWIj+5+bNm4YugYiIXoGBhQjA1q1bERAQgH379hm6FCKdqFQqQ5dAVC4YWIgAuLm5wczMDOvWrcP+/fsNXQ6R1n766ScAz18DwtBCryMGFjJpBw8eRGpqKtq0aYMlS5YgKysLq1evZmghUYmJicGoUaMwdOhQAAwt9HpiYCGTFRUVhYkTJ2LmzJlIS0tD69at8eWXXyInJ4ehhUSldu3amDRpEi5cuIDhw4cDYGih1w8DC5ms1q1b46OPPsLVq1fx2Wef4enTp3jzzTcZWkg0VqxYgZMnT8Le3h6BgYEICAhATEwMQwu9lhhYyCSpVCqYm5tj+vTp6N69O86fP4/PP/+coYVE488//8TPP/+MXr164ffff4etrS0GDx6MoUOHMrTQa4mBhUySVCpFYWEhzM3NMWXKFPTq1atYaFmwYAFycnKwbt067Nmzx9AlE2moWrUqlixZAj8/P/Ts2RPR0dEMLfRaY2Ahk2VmZgYAMDc3x9SpU9GzZ0+N0NK6dWssXLgQf/zxB7Zv347MzEwDV0z0XNE7axs1aoSgoCB06NABvXr1Ymih1xrf1kwmRRAESCQSXL58GfHx8VAoFHBxcUHdunWRn5+PhQsXYv/+/WjevDnmz58PW1tbnDt3Dg4ODnBxcTF0+URqKpUKUunz/+a8fPkyQkNDcfz4cfz444/w8vJCWloaNm/ejM2bN6NOnTrYsWOHgSsm0g0DC732ikJKQUEBzM3NsWfPHowbNw4ODg5QqVRwcnLC9OnT0blzZ3VoOXjwIFxdXfHVV19BoVAY+haI1Ip+n1928eJFzJ07t1ho+frrr3HgwAHs2LEDNWrUMEDFRPrBwEKvraL/Ak1LS4OtrS0A4LfffsOAAQMQEhKC0aNHY9euXRg6dCicnZ2xaNEidO/eHfn5+QgODsbZs2exefNmKJVKw94I0f8UhZWTJ0+qn8rs4eGBwMBAAMClS5cwZ84cHD9+HPv27cObb76J9PR0qFQq2NnZGbByIt0xsNBrqSisxMXF4e2338aRI0fQoEEDjB8/HnZ2dli4cCEePnyItm3bwtPTE4WFhUhISMDq1avx9ttvo6CgAOnp6XBwcDD0rZAJK/o9zsrKgrW1NQBgz549GDFiBNq3bw8bGxv88MMPmDhxIoKDgwE8Dy1hYWHYuXMnoqOj0bJlSwPeAZEeCUSvmcLCQkEQBCEuLk6wtrYWZsyYod538eJF4cSJE8LTp0+F5s2bC8OHDxcEQRB27NghmJubC46OjsKBAwcMUjfRi4p+j2NiYoQ6deoIjx8/Fs6ePSs4OzsLa9asEQRBEG7cuCEoFApBIpEI48aNU3/33LlzQmBgoBAfH2+Q2onKg7mhAxORPhX9F+mlS5fg7e2NKVOmIDQ0VL2/du3asLa2xv79+yGTyTB79mwAgJOTE9q3bw9PT080aNDAUOUTAfj79/jChQvo1KkThg4diqpVq2Lfvn0YMGAARo0ahQcPHqBLly4YMGAAWrdujU8++QR2dnYICQlB8+bN8fXXX8PCwsLQt0KkNwws9FqRSqW4d+8evL290bt3b42wsnTpUmRkZCA4OBjZ2dm4evUqEhMTUbNmTfz000+oXbs2Zs+ezUm2ZFBFYeXixYto06YNJkyYgHnz5gEAhgwZguPHj6v/3qlTJ6xbtw5//PEHnJycMGfOHGRnZ2PRokUMK/TaYWCh144gCLCzs0Nubi5OnDiBdu3aYfHixQgKCsKBAwcAPJ+o2LZtW7z//vtwdXVFbGwsoqKiGFbI4KRSKR48eIDOnTujR48e6rACAGvWrMHdu3dRs2ZNPHnyBCEhIQCAypUr45133oGvry9atWplqNKJyhUfHEevFZVKBVdXV/z666+4ceMGli9fjlGjRiEsLAw//fQT3n77bQBAkyZNMG3aNIwbNw6tW7dGTEwMmjRpYuDqiZ4rLCyEm5sbcnJycOrUKQBAWFgYZsyYge7du8PS0hJXrlzB6dOnkZ2djcWLF+PSpUvo1q0b6tevb+DqicoHVwnRa6eoS/369esYOHAgLl26hMWLF2PSpEkAoH4eC5ExS0hIwPjx42FhYQFHR0f88MMP2LJlC7p06QIAWLx4MaZNmwZ3d3ekpqbi8OHDaN68uYGrJio/DCz0WioKLbdu3UKfPn3g6uqKadOmoV27dhr7gVc/iIvI0G7cuIGxY8fi5MmTmDNnDiZPnqzel5eXh8uXL+PBgwdo0aIFnJ2dDVgpUfljYCHRK3o/StG7UoqCyIs9Le+99x5cXFwwc+ZMtG3b1pDlEmnl1q1bGD16NMzMzPDZZ5+pf39f/F0nMgX8bSfRKQooOTk5AJ4HlYSEBPXfixQFmAYNGuD777/Hw4cPMWPGDERFRVV80URlVKdOHXz11VcQBAFz585Vz2lhWCFTw994Eh2pVIrbt29jwoQJePjwIb7//nt4eHjgypUrJR5bFFq2bt0KlUqFmjVrGqBqorKrW7cuwsPDUalSJUyZMgVnzpwxdElEFY5DQiRKkZGR6NOnDzw9PREVFYV169Zh8ODBr5yPUlhYCDMzM+Tn56NSpUoGqJhId9evX0dQUBCWLFmCWrVqGbocogrFwEKiUxRKFixYgJkzZ+Ktt97C5s2b4e7urrH/n75LJFZ5eXl8KByZJA4JkegUFhYCACwtLTFr1iwkJycjODgY58+fBwBIJBK8mMOL5rwU7SMSM4YVMlXsYSHRKOodefk5KocOHcInn3yCNm3aYNq0afD09AQAREVFwdvb21DlEhGRHjGwkCgUhZUjR45g7969ePr0KRo2bIgRI0agevXqOHToEEaNGgUfHx8MGjQI586dw+zZs5GUlIRq1aqxZ4WISOQYWEg0IiIi8MEHH+Cjjz7CvXv38PTpUzx+/BiRkZGoVasWjhw5gilTpkClUiEjIwPff/89WrZsaeiyiYhIDxhYyCi9PDn2zz//xDvvvIMPP/wQU6dOBQBcvnwZkydPRkJCAn7//XdUrVoVd+/eRUZGBqpVq4YaNWoYqnwiItIzTrolo1KUn7OzswH8PWE2MzMTjx49QrNmzdTHenh4YOHChbCzs8P27dsBAK6urmjatCnDChHRa4aBhYyKRCJBSkoKXF1dsXPnTvXTPJVKJZydnXH8+HH1sWZmZmjatCnMzc0RHx9vqJKJiKgCMLCQ0ZFKpejVqxc+/vhj/PDDD+o2Ly8vHD16FHv27FEfK5FI8MYbb8DW1haCIIAjnEREryfOYSGDK+lhbikpKZg3bx5WrlyJ3bt3o2/fvnjy5An8/f2Rnp4OLy8v+Pj4IDIyEps3b0Z0dDQaNGhgoDsgIqLyxsBCBlX0xtmsrCwUFhZCLper9z169Ajz58/HqlWrsGvXLvTv3x9PnjzBl19+iVOnTuHPP/+EUqlEeHi4xtwWIiJ6/TCwkMElJCRgwIABqFKlCkaMGAGlUokuXboAAHJzczF58mSsXr0aO3bswPvvv4+CggJIJBKkpqaicuXKsLa2NvAdEBFReTP/90OIyo9KpcLGjRtx4cIFWFpaIi0tDdnZ2bC3t8ebb76JoUOHYsiQIXBwcMDAgQMhl8vh5+cHAKhWrZqBqycioorCHhYyuKSkJCxYsAC3bt2Cu7s7xowZg61bt+LEiRO4ePEi7O3tUbt2bcTGxiIlJQXHjh1D+/btDV02ERFVIPawkMEplUpMnToV8+fPx8mTJ1G3bl3MmjULABAdHY3ExESsW7cO1atXR0pKCqpWrWrgiomIqKKxh4WMRtEk2+joaPTp0wefffaZel9+fj5UKhXS09NRvXp1A1ZJRESGwMBCRiUpKQnz5s3D2bNn0adPH8yYMQMAir2hmYiITAsDCxmdotBy/vx5dO7cGSEhIYYuiYiIDIxPuiWjo1Qq8fnnn6Nu3bo4ffo0njx5YuiSiIjIwNjDQkYrOTkZAODo6GjgSoiIyNAYWIiIiMjocUiIiIiIjB4DCxERERk9BhYiIiIyegwsREREZPQYWIiIiMjoMbAQERGR0WNgISIiIqPHwEJkYgIDA9GnTx/1544dO2LChAkVXsexY8cgkUiQlpb2ymMkEgkiIiJKfc7g4GA0a9ZMp7ru3r0LiUSCuLg4nc5DRPrFwEJkBAIDAyGRSCCRSGBhYQF3d3eEhoaioKCg3K+9Z88ezJkzp1THliZkEBGVB77+lshIdO3aFRs2bEBubi5++uknjBkzBpUqVcLMmTOLHZuXlwcLCwu9XNfe3l4v5yEiKk/sYSEyEjKZDEqlEi4uLvjPf/4DX19f/PjjjwD+HsaZN28enJycUL9+fQDAgwcPMGDAANja2sLe3h69e/fG3bt31ecsLCzEpEmTYGtrCwcHB0ybNg0vv43j5SGh3NxcTJ8+Hc7OzpDJZHB3d8f69etx9+5ddOrUCQBgZ2cHiUSCwMBAAIBKpUJYWBjc3NxgZWUFT09PfP/99xrX+emnn1CvXj1YWVmhU6dOGnWW1vTp01GvXj1UrlwZtWvXRlBQEPLz84sd9/XXX8PZ2RmVK1fGgAEDkJ6errH/22+/hYeHBywtLdGgQQOsXr1a61qIqGIxsBAZKSsrK+Tl5ak/HzlyBPHx8Th8+DD279+P/Px8+Pn5wcbGBidOnMCpU6dQpUoVdO3aVf29JUuWYOPGjfi///s/nDx5Eqmpqdi7d+8/Xnfw4MH47rvvEB4ejmvXruHrr79GlSpV4OzsjN27dwMA4uPj8ejRI6xYsQIAEBYWhs2bN2Pt2rW4cuUKJk6ciI8++gjHjx8H8DxY9evXDz179kRcXByGDx+OGTNmaP0zsbGxwcaNG3H16lWsWLEC33zzDZYtW6ZxzM2bN7Fz507s27cPBw8exPnz5zF69Gj1/q1bt2LWrFmYN28erl27hvnz5yMoKAibNm3Suh4iqkACERlcQECA0Lt3b0EQBEGlUgmHDx8WZDKZMGXKFPV+R0dHITc3V/2dLVu2CPXr1xdUKpW6LTc3V7CyshJ++eUXQRAEoUaNGsLChQvV+/Pz84WaNWuqryUIgtChQwfh008/FQRBEOLj4wUAwuHDh0us87fffhMACE+fPlW35eTkCJUrVxZOnz6tceywYcOEDz74QBAEQZg5c6bQsGFDjf3Tp08vdq6XARD27t37yv2LFi0SWrZsqf48e/ZswczMTPjjjz/UbT///LMglUqFR48eCYIgCHXq1BG2bdumcZ45c+YI3t7egiAIwp07dwQAwvnz5195XSKqeJzDQmQk9u/fjypVqiA/Px8qlQoffvghgoOD1fubNGmiMW/lwoULuHnzJmxsbDTOk5OTg1u3biE9PR2PHj2Cl5eXep+5uTlatWpVbFioSFxcHMzMzNChQ4dS133z5k1kZ2fjnXfe0WjPy8tD8+bNAQDXrl3TqAMAvL29S32NIjt27EB4eDhu3bqFzMxMFBQUQC6XaxxTq1YtvPHGGxrXUalUiI+Ph42NDW7duoVhw4ZhxIgR6mMKCgqgUCi0roeIKg4DC5GR6NSpE9asWQMLCws4OTnB3Fzz/57W1tYanzMzM9GyZUts3bq12LmqVatWphqsrKy0/k5mZiYA4MCBAxpBAXg+L0dfoqKi4O/vj5CQEPj5+UGhUGD79u1YsmSJ1rV+8803xQKUmZmZ3molIv1jYCEyEtbW1nB3dy/18S1atMCOHTtQvXr1Yr0MRWrUqIHo6Gi0b98ewPOehNjYWLRo0aLE45s0aQKVSoXjx4/D19e32P6iHp7CwkJ1W8OGDSGTyXD//v1X9sx4eHioJxAXOXPmzL/f5AtOnz4NFxcXfP755+q2e/fuFTvu/v37SExMhJOTk/o6UqkU9evXh6OjI5ycnHD79m34+/trdX0iMixOuiUSKX9/f1StWhW9e/fGiRMncOfOHRw7dgzjx4/HH3/8AQD49NNP8eWXXyIiIgLXr1/H6NGj//EZKq6urggICMDQoUMRERGhPufOnTsBAC4uLpBIJNi/fz8eP36MzMxM2NjYYMqUKZg4cSI2bdqEW7du4dy5c1i5cqV6IuuoUaOQkJCAqVOnIj4+Htu2bcPGjRu1ut+6devi/v372L59O27duoXw8PASJxBbWloiICAAFy5cwIkTJzB+/HgMGDAASqUSABASEoKwsDCEh4fjxo0buHTpEjZs2IClS5dqVQ8RVSwGFiKRqly5MiIjI1GrVi3069cPHh4eGDZsGHJyctQ9LpMnT8bHH3+MgIAAeHt7w8bGBn379v3H865ZswbvvfceRo8ejQYNGmDEiBHIysoCALzxxhsICQnBjBkz4OjoiLFjxwIA5syZg6CgIISFhcHDwwNdu3bFgQMH4ObmBuD5vJLdu3cjIiICnp6eWLt2LebPn6/V/fbq1QsTJ07E2LFj0axZM5w+fRpBQUHFjnN3d0e/fv3w7rvvokuXLmjatKnGsuXhw4fj22+/xYYNG9CkSRN06NABGzduVNdKRMZJIrxq9h0RERGRkWAPCxERERk9BhYiIiIyegwsREREZPQYWIiIiMjoMbAQERGR0WNgISIiIqPHwEJERERGj4GFiIiIjB4DCxERERk9BhYiIiIyegwsREREZPT+H1ax+CxyZoNgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "# f1 score\n",
    "precision_fp = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall_fp = cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "f1_score_fp = 2 * precision_fp * recall_fp / (precision_fp + recall_fp)\n",
    "print('f1_score: ', f1_score_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpfwhtw61x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpfwhtw61x/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "2023-12-30 23:24:32.774594: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-30 23:24:32.774609: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-30 23:24:32.774836: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpfwhtw61x\n",
      "2023-12-30 23:24:32.781762: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-30 23:24:32.781780: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpfwhtw61x\n",
      "2023-12-30 23:24:32.797223: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-30 23:24:33.007488: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpfwhtw61x\n",
      "2023-12-30 23:24:33.064393: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 289557 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 42, Total Ops 75, % non-converted = 56.00 %\n",
      " * 42 ARITH ops\n",
      "\n",
      "- arith.constant:   42 occurrences  (f32: 36, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 17)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139192"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if \"LSTM\" in model_name:\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "model_tflite = converter.convert()\n",
    "# save the model\n",
    "open('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ConvLSTM model\n",
    "if \"LSTM\" in model_name:\n",
    "    def representative_data_gen():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_q_model = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    print('input: ', input_type)\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "    print('output: ', output_type)\n",
    "    # Save the quantized model to disk\n",
    "    open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_1 (Quantize  (None, 50, 6)                3         ['input_3[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_2 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_1[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_34 (QuantizeW  (None, 1, 48, 64)            1347      ['quant_reshape_2[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_2 (Qua  (None, 1, 24, 64)            1         ['quant_conv2d_34[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_36 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_max_pooling2d_2[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_36[0][0]']     \n",
      " 24 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_24 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_24\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_37 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_24[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_37[0][0]']     \n",
      " 25 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_25 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_25\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_38 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_25[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_38[0][0]']     \n",
      " 26 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_35 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_max_pooling2d_2[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_8 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_26\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_35[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_26 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_8[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_40 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_26[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_40[0][0]']     \n",
      " 27 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_27 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_27\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_41 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_27[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_41[0][0]']     \n",
      " 28 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_28 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_28\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_42 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_28[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_42[0][0]']     \n",
      " 29 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_39 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_26[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_9 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_29\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_39[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_29 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_9[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_44 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_29[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_44[0][0]']     \n",
      " 30 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_30 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_30\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_45 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_30[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_45[0][0]']     \n",
      " 31 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_31 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_31\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_46 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_31[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_46[0][0]']     \n",
      " 32 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_43 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_29[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_10 (QuantizeWrap  (None, 1, 24, 64)            1         ['quant_batch_normalization_32\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_43[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_32 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_10[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_48 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_32[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_48[0][0]']     \n",
      " 33 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_33 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_33\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_49 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_33[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_49[0][0]']     \n",
      " 34 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_34 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_34\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_50 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_34[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_50[0][0]']     \n",
      " 35 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_47 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_32[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_11 (QuantizeWrap  (None, 1, 24, 64)            1         ['quant_batch_normalization_35\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_47[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_35 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_11[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_2   (None, 1, 12, 64)            3         ['quant_re_lu_35[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_2 (QuantizeW  (None, 768)                  1         ['quant_average_pooling2d_2[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_2 (QuantizeWra  (None, 2)                    1543      ['quant_flatten_2[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34087 (133.15 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 2277 (8.89 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model = tfmot.quantization.keras.quantize_model(model)\n",
    "q_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi0y4ufjv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi0y4ufjv/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-30 23:24:40.199876: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-30 23:24:40.199892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-30 23:24:40.200088: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi0y4ufjv\n",
      "2023-12-30 23:24:40.211552: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-30 23:24:40.211569: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi0y4ufjv\n",
      "2023-12-30 23:24:40.242821: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-30 23:24:40.481339: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi0y4ufjv\n",
      "2023-12-30 23:24:40.565649: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 365559 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 77, % non-converted = 7.79 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 4)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 17)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 18, uq_32: 18)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68896"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_q'+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_q_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48600, 2)\n",
      "y_val.shape:  (12151, 2)\n",
      "Epoch 1/50\n",
      "147/760 [====>.........................] - ETA: 10s - loss: 1.1562 - accuracy: 0.9175"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_val.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, y_val\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m q_history \u001b[38;5;241m=\u001b[39m \u001b[43mq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "q_history = q_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_qat'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_qat_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "#accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "#print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "ups = pruning_callbacks.UpdatePruningStep()\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "pruned_model_unstructured.save('./saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))\n",
    "\n",
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "unstructured_pruned_model_path = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(unstructured_pruned_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(unstructured_pruned_model_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "print(batch_size)\n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_qat_model.save('./saved_models/'+model_name+'_pqat'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_dynR.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_pqat_FullInt'+('_Rescaled' if train_with_int else '') +'.tflite', \"wb\").write(pruned_qat_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "pqat_model_path = './saved_models/'+model_name+'_pqat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "qat_model_path = './saved_models/'+model_name+'_qat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name+'.tflite'\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size(pqat_model_path))\n",
    "print('Size of the QAT model: ', get_gzipped_model_size(qat_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(pqat_model_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

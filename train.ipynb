{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import plot_confusion_matrix, plot_confusion_matrix, get_gzipped_model_size, rescale_data\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from models.ConvLSTM import ConvLSTM\n",
    "from models.ConvLSTM_VGG import ConvLSTM_VGG\n",
    "from models.TinyFallNet import TinyFallNet\n",
    "from models.ResNet24 import ResNet24\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "\n",
    "data_path = config['data_path_win']\n",
    "# data_path = config['data_path_linux']\n",
    "# data_path = config['data_path_mac']\n",
    "sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# data mode. Combination of sensor data.\n",
    "# data_mode = 'ACC+GYRO' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "window_size = config['window_size'] # window size\n",
    "fall_threshold = config['fall_threshold'] # threshold for windows labeled as fall\n",
    "num_window_fall_data = config['num_window_fall_data']   # number of windows labeled as fall\n",
    "num_window_not_fall_data = config['num_window_not_fall_data']    # number of windows labeled as not fall\n",
    "acc_max = config['acc_max'] \n",
    "gyro_max = config['gyro_max'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patience:  5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyFallNet_6axis\" # \"ConvLSTM\" or \"ConvLSTM_VGG\" or \"TinyFallNet\" or \"ResNet24\" or \"TinyFallNet_6axis\"\n",
    "# when train_with_int is True, scaled data will be used for training, generate full integer quantized model(int8 input, int8 output)\n",
    "# when train_with_int is False, original data will be used for training, generate three models: dynamic range quantized model(float32 input, float32 output)\n",
    "#                                                                                               full integer quantized model(int8 input, int8 output)\n",
    "#                                                                                               full integer quantized model(float32 input, int8 output)\n",
    "train_with_int = True\n",
    "use_saved_data = True\n",
    "# use_float_input = True\n",
    "load_from_checkpoint = config['load_from_checkpoint']\n",
    "\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = models.load_model('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "else:\n",
    "    if model_name == \"ConvLSTM\":\n",
    "        model = ConvLSTM()\n",
    "        data_mode = 'ACC+GYRO+MAG' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ConvLSTM_6axis\":\n",
    "        model = ConvLSTM(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"ConvLSTM_VGG\":\n",
    "        model = ConvLSTM_VGG()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ConvLSTM_VGG_6axis\":\n",
    "        model = ConvLSTM_VGG(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"TinyFallNet\":\n",
    "        model = TinyFallNet()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ResNet24\":\n",
    "        model = ResNet24()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ResNet24_6axis\":\n",
    "        model = ResNet24(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"TinyFallNet_6axis\":\n",
    "        model = TinyFallNet(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    else:\n",
    "        print(\"Please select a valid model name\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = config['learning_rate']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "lr_factor = config['lr_factor']\n",
    "patience = config['patience']\n",
    "print('patience: ', patience)\n",
    "\n",
    "# create checkpoints folder if not exists\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_fall_size:  75060\n",
      "fall_size:  864\n",
      "Data shape:  (75924, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "if not use_saved_data:\n",
    "    data, label = DataOrganizer(sensor_data_folder, \n",
    "                                label_data_folder, \n",
    "                                window_size, \n",
    "                                fall_threshold, \n",
    "                                num_window_fall_data, \n",
    "                                num_window_not_fall_data,\n",
    "                                data_mode)\n",
    "else:\n",
    "    data = np.load('./saved_data/data.npy')\n",
    "    label = np.load('./saved_data/label.npy', allow_pickle=True)\n",
    "    B_size = (label == 0).sum()\n",
    "    A_size = (label == 1).sum()\n",
    "    print('not_fall_size: ', B_size)\t\n",
    "    print('fall_size: ', A_size)\n",
    "\n",
    "\n",
    "print(\"Data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the rescaling is done only once\n",
    "if train_with_int==True and data.dtype!=np.int8:\n",
    "    dtype_out = np.int8 # rescaled input data type\n",
    "    data = rescale_data(data, dtype_out, acc_max=acc_max, gyro_max=gyro_max)\n",
    "else:\n",
    "    data = data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_saved_data:    \n",
    "    in_channels = data.shape[2]\n",
    "    print(\"Data shape: \", data.shape)\n",
    "    print(\"Data dtype: \", data.dtype)\n",
    "    print('in_channels: ', in_channels)\n",
    "\n",
    "    label = label.astype(np.int64)\n",
    "    data_copy = data.reshape(data.shape[0], 50, in_channels)\n",
    "\n",
    "    B_size = (label == 0).sum()\n",
    "    A_size = (label == 1).sum()\n",
    "    print('not_fall_size: ', B_size)\t\n",
    "    print('fall_size: ', A_size)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_copy, label, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    #print(np.unique(y_train)) # [0 1]\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    y_test = y_test.astype(np.int64)\n",
    "\n",
    "    # select the test data that is not zero\n",
    "    X_test_true = X_test[y_test != 0]\n",
    "    y_test_true = y_test[y_test != 0]\n",
    "    # length of the test data\n",
    "    test_len = X_test_true.shape[0]\n",
    "    X_test_false = X_test[y_test == 0]\n",
    "    y_test_false = y_test[y_test == 0]\n",
    "    # X_test.shape:  (17, 50, 9)\n",
    "    # randomly len number of test data that is zero\n",
    "    index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "\n",
    "    X_test_false = X_test[index]\n",
    "    y_test_false = y_test[index]\n",
    "\n",
    "    # concatenate the true and false test data\n",
    "    X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "    y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "else:\n",
    "    X_train = np.load('./saved_data/X_train.npy')\n",
    "    X_val = np.load('./saved_data/X_val.npy')\n",
    "    X_test = np.load('./saved_data/X_test.npy')\n",
    "    y_train = np.load('./saved_data/y_train.npy')\n",
    "    y_val = np.load('./saved_data/y_val.npy')\n",
    "    y_test = np.load('./saved_data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_saved_data:\n",
    "    # create the saved_data folder if it does not exist\n",
    "    if not os.path.exists('./saved_data'):\n",
    "        os.makedirs('./saved_data')\n",
    "    # save the test data, train data and validation data\n",
    "    np.save('./saved_data/X_test.npy', X_test)\n",
    "    np.save('./saved_data/y_test.npy', y_test)\n",
    "    np.save('./saved_data/X_train.npy', X_train)\n",
    "    np.save('./saved_data/y_train.npy', y_train)\n",
    "    np.save('./saved_data/X_val.npy', X_val)\n",
    "    np.save('./saved_data/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)         (None, 1, 50, 6)             0         ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)         (None, 1, 48, 64)            1216      ['reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 1, 24, 64)            0         ['conv2d_159[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)         (None, 1, 24, 16)            1040      ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_123 (B  (None, 1, 24, 16)            64        ['conv2d_161[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_123 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_123[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)         (None, 1, 24, 16)            784       ['re_lu_123[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_124 (B  (None, 1, 24, 16)            64        ['conv2d_162[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_124 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_124[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)         (None, 1, 24, 64)            1088      ['re_lu_124[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_125 (B  (None, 1, 24, 64)            256       ['conv2d_163[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)         (None, 1, 24, 64)            4160      ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " add_40 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_125[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_160[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_125 (ReLU)            (None, 1, 24, 64)            0         ['add_40[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)         (None, 1, 24, 16)            1040      ['re_lu_125[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_126 (B  (None, 1, 24, 16)            64        ['conv2d_165[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_126 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_126[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)         (None, 1, 24, 16)            784       ['re_lu_126[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_127 (B  (None, 1, 24, 16)            64        ['conv2d_166[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_127 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_127[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)         (None, 1, 24, 64)            1088      ['re_lu_127[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_128 (B  (None, 1, 24, 64)            256       ['conv2d_167[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)         (None, 1, 24, 64)            4160      ['re_lu_125[0][0]']           \n",
      "                                                                                                  \n",
      " add_41 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_128[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_164[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_128 (ReLU)            (None, 1, 24, 64)            0         ['add_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)         (None, 1, 24, 16)            1040      ['re_lu_128[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_129 (B  (None, 1, 24, 16)            64        ['conv2d_169[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_129 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_129[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)         (None, 1, 24, 16)            784       ['re_lu_129[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_130 (B  (None, 1, 24, 16)            64        ['conv2d_170[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_130 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_130[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)         (None, 1, 24, 64)            1088      ['re_lu_130[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_131 (B  (None, 1, 24, 64)            256       ['conv2d_171[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)         (None, 1, 24, 64)            4160      ['re_lu_128[0][0]']           \n",
      "                                                                                                  \n",
      " add_42 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_131[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_168[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_131 (ReLU)            (None, 1, 24, 64)            0         ['add_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)         (None, 1, 24, 16)            1040      ['re_lu_131[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_132 (B  (None, 1, 24, 16)            64        ['conv2d_173[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_132 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_132[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)         (None, 1, 24, 16)            784       ['re_lu_132[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_133 (B  (None, 1, 24, 16)            64        ['conv2d_174[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_133 (ReLU)            (None, 1, 24, 16)            0         ['batch_normalization_133[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)         (None, 1, 24, 64)            1088      ['re_lu_133[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_134 (B  (None, 1, 24, 64)            256       ['conv2d_175[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)         (None, 1, 24, 64)            4160      ['re_lu_131[0][0]']           \n",
      "                                                                                                  \n",
      " add_43 (Add)                (None, 1, 24, 64)            0         ['batch_normalization_134[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_172[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_134 (ReLU)            (None, 1, 24, 64)            0         ['add_43[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (Avera  (None, 1, 12, 64)            0         ['re_lu_134[0][0]']           \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)         (None, 768)                  0         ['average_pooling2d_7[0][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 2)                    1538      ['flatten_8[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32578 (127.26 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "            loss='categorical_crossentropy',\n",
    "            #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 50, 9))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48591, 2)\n",
      "y_val.shape:  (12148, 2)\n",
      "X_train.shape:  (48591, 50, 6)\n",
      "y_train.shape:  (48591, 2)\n"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint_path = './checkpoints/'+model_name+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1334 - accuracy: 0.9679\n",
      "Epoch 1: val_loss improved from inf to 0.07464, saving model to ./checkpoints/ResNet24_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 18s 23ms/step - loss: 0.1334 - accuracy: 0.9679 - val_loss: 0.0746 - val_accuracy: 0.9769 - lr: 2.5000e-04\n",
      "Epoch 2/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9742\n",
      "Epoch 2: val_loss did not improve from 0.07464\n",
      "760/760 [==============================] - 17s 23ms/step - loss: 0.1109 - accuracy: 0.9742 - val_loss: 0.0997 - val_accuracy: 0.9704 - lr: 2.5000e-04\n",
      "Epoch 3/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9679\n",
      "Epoch 3: val_loss did not improve from 0.07464\n",
      "760/760 [==============================] - 17s 23ms/step - loss: 0.1478 - accuracy: 0.9679 - val_loss: 0.0942 - val_accuracy: 0.9672 - lr: 2.5000e-04\n",
      "Epoch 4/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9750\n",
      "Epoch 4: val_loss did not improve from 0.07464\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.1136 - accuracy: 0.9749 - val_loss: 0.0901 - val_accuracy: 0.9688 - lr: 2.5000e-04\n",
      "Epoch 5/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9752\n",
      "Epoch 5: val_loss did not improve from 0.07464\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.1048 - accuracy: 0.9752 - val_loss: 0.1089 - val_accuracy: 0.9669 - lr: 2.5000e-04\n",
      "Epoch 6/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9764\n",
      "Epoch 6: val_loss improved from 0.07464 to 0.05976, saving model to ./checkpoints/ResNet24_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 17s 22ms/step - loss: 0.1070 - accuracy: 0.9764 - val_loss: 0.0598 - val_accuracy: 0.9803 - lr: 2.5000e-04\n",
      "Epoch 7/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.9662\n",
      "Epoch 7: val_loss did not improve from 0.05976\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.1676 - accuracy: 0.9662 - val_loss: 0.0824 - val_accuracy: 0.9717 - lr: 2.5000e-04\n",
      "Epoch 8/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9795\n",
      "Epoch 8: val_loss improved from 0.05976 to 0.04280, saving model to ./checkpoints/ResNet24_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.0907 - accuracy: 0.9795 - val_loss: 0.0428 - val_accuracy: 0.9865 - lr: 2.5000e-04\n",
      "Epoch 9/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9749\n",
      "Epoch 9: val_loss improved from 0.04280 to 0.04228, saving model to ./checkpoints/ResNet24_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 17s 22ms/step - loss: 0.1211 - accuracy: 0.9749 - val_loss: 0.0423 - val_accuracy: 0.9878 - lr: 2.5000e-04\n",
      "Epoch 10/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9806\n",
      "Epoch 10: val_loss did not improve from 0.04228\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.0984 - accuracy: 0.9806 - val_loss: 0.1385 - val_accuracy: 0.9700 - lr: 2.5000e-04\n",
      "Epoch 11/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9732\n",
      "Epoch 11: val_loss did not improve from 0.04228\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.1324 - accuracy: 0.9732 - val_loss: 0.1104 - val_accuracy: 0.9735 - lr: 2.5000e-04\n",
      "Epoch 12/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9747\n",
      "Epoch 12: val_loss did not improve from 0.04228\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.1200 - accuracy: 0.9746 - val_loss: 0.1447 - val_accuracy: 0.9645 - lr: 2.5000e-04\n",
      "Epoch 13/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9798\n",
      "Epoch 13: val_loss did not improve from 0.04228\n",
      "760/760 [==============================] - 17s 22ms/step - loss: 0.0880 - accuracy: 0.9799 - val_loss: 0.0544 - val_accuracy: 0.9823 - lr: 2.5000e-04\n",
      "Epoch 14/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9793\n",
      "Epoch 14: val_loss improved from 0.04228 to 0.03773, saving model to ./checkpoints/ResNet24_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 17s 22ms/step - loss: 0.1045 - accuracy: 0.9793 - val_loss: 0.0377 - val_accuracy: 0.9882 - lr: 2.5000e-04\n",
      "Epoch 15/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9747\n",
      "Epoch 15: val_loss did not improve from 0.03773\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.1321 - accuracy: 0.9747 - val_loss: 0.1302 - val_accuracy: 0.9579 - lr: 2.5000e-04\n",
      "Epoch 16/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9809\n",
      "Epoch 16: val_loss did not improve from 0.03773\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.0870 - accuracy: 0.9809 - val_loss: 0.0638 - val_accuracy: 0.9821 - lr: 2.5000e-04\n",
      "Epoch 17/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9791\n",
      "Epoch 17: val_loss did not improve from 0.03773\n",
      "760/760 [==============================] - 16s 22ms/step - loss: 0.1025 - accuracy: 0.9791 - val_loss: 0.0497 - val_accuracy: 0.9878 - lr: 2.5000e-04\n",
      "Epoch 18/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9773\n",
      "Epoch 18: val_loss did not improve from 0.03773\n",
      "760/760 [==============================] - 18s 24ms/step - loss: 0.1187 - accuracy: 0.9773 - val_loss: 0.1203 - val_accuracy: 0.9689 - lr: 2.5000e-04\n",
      "Epoch 19/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9785\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.03773\n",
      "760/760 [==============================] - 17s 22ms/step - loss: 0.1050 - accuracy: 0.9785 - val_loss: 0.0562 - val_accuracy: 0.9830 - lr: 2.5000e-04\n",
      "Epoch 19: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs, checkpoint],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 121 variables. \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer 'conv2d_125' expected 2 variables, but received 0 variables during loading. Expected: ['conv2d_125/kernel:0', 'conv2d_125/bias:0']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/keras/src/engine/base_layer.py:3531\u001b[0m, in \u001b[0;36mLayer.load_own_variables\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m   3529\u001b[0m all_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_weights \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_trainable_weights\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[0;32m-> 3531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[v\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mall_vars]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3536\u001b[0m     )\n\u001b[1;32m   3537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[1;32m   3538\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n\u001b[1;32m   3539\u001b[0m     v\u001b[38;5;241m.\u001b[39massign(store[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Layer 'conv2d_125' expected 2 variables, but received 0 variables during loading. Expected: ['conv2d_125/kernel:0', 'conv2d_125/bias:0']"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_true' parameter of confusion_matrix must be an array-like. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# plot the confusion matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:204\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[0;32m--> 204\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'y_true' parameter of confusion_matrix must be an array-like. Got 0 instead."
     ]
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "# f1 score\n",
    "precision_fp = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall_fp = cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "f1_score_fp = 2 * precision_fp * recall_fp / (precision_fp + recall_fp)\n",
    "print('f1_score: ', f1_score_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp3i_ofo9d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp3i_ofo9d/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "2024-01-04 15:45:03.792990: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-04 15:45:03.793141: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-04 15:45:03.794672: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp3i_ofo9d\n",
      "2024-01-04 15:45:03.804544: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-04 15:45:03.804557: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp3i_ofo9d\n",
      "2024-01-04 15:45:03.836001: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-04 15:45:04.163511: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp3i_ofo9d\n",
      "2024-01-04 15:45:04.252921: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 458248 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229684"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if \"LSTM\" in model_name:\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "model_tflite = converter.convert()\n",
    "# save the model\n",
    "open('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ConvLSTM model\n",
    "if \"LSTM\" in model_name:\n",
    "    def representative_data_gen():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "            yield [input_value]\n",
    " \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    if train_with_int:\n",
    "        converter.inference_input_type = tf.int8\n",
    "    else:\n",
    "        converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.int8\n",
    " \n",
    "    tflite_q_model = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    print('input: ', input_type)\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "    print('output: ', output_type)\n",
    "    # Save the quantized model to disk\n",
    "    open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)\n",
    " \n",
    "    # test the quantized model\n",
    "    if train_with_int:\n",
    "        print('model name: ', model_name)\n",
    "        # Load the model into an interpreter\n",
    "        interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_q.tflite')\n",
    "        X_test_qat = X_test.astype('int8')\n",
    "        y_test_qat = y_test.astype('int8')\n",
    "        assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "    else:\n",
    "        interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_q.tflite')\n",
    "        X_test_qat = X_test.astype('float32')\n",
    "        y_test_qat = y_test.astype('int8')\n",
    "        assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    " \n",
    "    # Allocate memory for the model's input Tensor(s)\n",
    "    interpreter.allocate_tensors()\n",
    "    # Get the model input and output details\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    print(\"input: \", input_details)\n",
    "    print(\"output: \", output_details)\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    for i, test_data in enumerate(X_test_qat):\n",
    "        test_data = np.expand_dims(test_data, axis=0)\n",
    "        #print(test_data.shape)\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        if i%100 == 0:\n",
    "            # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "            print('Evaluated on ', i, '.')\n",
    "        predictions[i] = output.argmax()\n",
    " \n",
    "    gt = np.argmax(y_test_qat, axis=-1)\n",
    " \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(gt, predictions)\n",
    " \n",
    "    print(cm)\n",
    "    # plot the confusion matrix\n",
    "    plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    " \n",
    "    accuracy = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "    print('accuracy: ', accuracy)\n",
    " \n",
    "    f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    print('f1_score: ', f1_score)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_2 (Quantize  (None, 50, 6)                3         ['input_4[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_3 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_2[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_81 (QuantizeW  (None, 1, 48, 64)            1347      ['quant_reshape_3[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_82 (QuantizeW  (None, 1, 46, 64)            12483     ['quant_conv2d_81[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_3 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_82[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_84 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_84[0][0]']     \n",
      " 66 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_66 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_66\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_85 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_66[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_85[0][0]']     \n",
      " 67 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_67 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_67\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_86 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_67[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_86[0][0]']     \n",
      " 68 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_83 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_21 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_68\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_83[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_68 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_21[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_88 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_88[0][0]']     \n",
      " 69 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_69 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_69\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_89 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_69[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_89[0][0]']     \n",
      " 70 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_70 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_70\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_90 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_70[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_90[0][0]']     \n",
      " 71 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_87 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_22 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_71\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_87[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_71 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_22[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_91 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_91[0][0]']     \n",
      " 72 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_72 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_72\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_92 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_72[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_92[0][0]']     \n",
      " 73 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_73 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_73\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_93 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_73[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_93[0][0]']     \n",
      " 74 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_23 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_74\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_74 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_23[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_95 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_95[0][0]']     \n",
      " 75 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_75 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_75\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_96 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_75[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_96[0][0]']     \n",
      " 76 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_76 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_76\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_97 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_76[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_97[0][0]']     \n",
      " 77 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_94 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_24 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_77\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_94[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_77 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_24[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_98 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_77[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_98[0][0]']     \n",
      " 78 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_78 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_78\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_99 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_78[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_99[0][0]']     \n",
      " 79 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_79 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_79\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_100 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_79[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_100[0][0]']    \n",
      " 80 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_25 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_80\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_77[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_80 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_25[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_102 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_102[0][0]']    \n",
      " 81 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_81 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_81\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_103 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_81[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_103[0][0]']    \n",
      " 82 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_82 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_82\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_104 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_82[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_104[0][0]']    \n",
      " 83 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_101 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_26 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_83\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_101[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_83 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_26[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_105 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_83[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_105[0][0]']    \n",
      " 84 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_84 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_84\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_106 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_84[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_106[0][0]']    \n",
      " 85 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_85 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_85\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_107 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_85[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_107[0][0]']    \n",
      " 86 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_27 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_86\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_83[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_86 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_27[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_3   (None, 1, 11, 64)            3         ['quant_re_lu_86[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_4 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_3[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_5 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_4[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56960 (222.50 KB)\n",
      "Trainable params: 53346 (208.38 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model = tfmot.quantization.keras.quantize_model(model)\n",
    "q_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpgwgjzzdw/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpgwgjzzdw/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-04 15:45:16.228280: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-04 15:45:16.228348: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-04 15:45:16.229595: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpgwgjzzdw\n",
      "2024-01-04 15:45:16.250678: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-04 15:45:16.250693: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpgwgjzzdw\n",
      "2024-01-04 15:45:16.302928: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-04 15:45:16.659642: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpgwgjzzdw\n",
      "2024-01-04 15:45:16.788432: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 558874 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 110, % non-converted = 5.45 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108104"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_q'+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_q_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis\n",
      "input:  {'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([ 1, 50,  6], dtype=int32), 'shape_signature': array([-1, 50,  6], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (3.921568847431445e-09, -1), 'quantization_parameters': {'scales': array([3.921569e-09], dtype=float32), 'zero_points': array([-1], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 72, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_true' parameter of confusion_matrix must be an array-like. Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m gt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test_qat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# plot the confusion matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:204\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[0;32m--> 204\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'y_true' parameter of confusion_matrix must be an array-like. Got 0 instead."
     ]
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_q_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_q_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48591, 2)\n",
      "y_val.shape:  (12148, 2)\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "# Define the checkpoint\n",
    "checkpoint_qat_path = './checkpoints/'+model_name+'_qat'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "checkpoint_qat = ModelCheckpoint(checkpoint_qat_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.6381 - accuracy: 0.9071\n",
      "Epoch 1: val_loss improved from inf to 0.14857, saving model to ./checkpoints/ResNet24_6axis_qat_Rescaled.keras\n",
      "760/760 [==============================] - 29s 31ms/step - loss: 0.6373 - accuracy: 0.9071 - val_loss: 0.1486 - val_accuracy: 0.9540 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9650\n",
      "Epoch 2: val_loss improved from 0.14857 to 0.08300, saving model to ./checkpoints/ResNet24_6axis_qat_Rescaled.keras\n",
      "760/760 [==============================] - 23s 30ms/step - loss: 0.1705 - accuracy: 0.9650 - val_loss: 0.0830 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9719\n",
      "Epoch 3: val_loss did not improve from 0.08300\n",
      "760/760 [==============================] - 24s 32ms/step - loss: 0.1392 - accuracy: 0.9719 - val_loss: 0.5005 - val_accuracy: 0.8449 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9714\n",
      "Epoch 4: val_loss did not improve from 0.08300\n",
      "760/760 [==============================] - 23s 30ms/step - loss: 0.1345 - accuracy: 0.9714 - val_loss: 0.0937 - val_accuracy: 0.9707 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9563\n",
      "Epoch 5: val_loss did not improve from 0.08300\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.2500 - accuracy: 0.9563 - val_loss: 0.1923 - val_accuracy: 0.9533 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.9457\n",
      "Epoch 6: val_loss did not improve from 0.08300\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.3232 - accuracy: 0.9458 - val_loss: 0.1188 - val_accuracy: 0.9635 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9717\n",
      "Epoch 7: val_loss improved from 0.08300 to 0.05512, saving model to ./checkpoints/ResNet24_6axis_qat_Rescaled.keras\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.1323 - accuracy: 0.9717 - val_loss: 0.0551 - val_accuracy: 0.9854 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9646\n",
      "Epoch 8: val_loss did not improve from 0.05512\n",
      "760/760 [==============================] - 27s 35ms/step - loss: 0.1826 - accuracy: 0.9646 - val_loss: 0.0605 - val_accuracy: 0.9812 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.3960 - accuracy: 0.9377\n",
      "Epoch 9: val_loss did not improve from 0.05512\n",
      "760/760 [==============================] - 29s 38ms/step - loss: 0.3960 - accuracy: 0.9377 - val_loss: 0.0972 - val_accuracy: 0.9691 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9663\n",
      "Epoch 10: val_loss did not improve from 0.05512\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.1661 - accuracy: 0.9663 - val_loss: 0.2464 - val_accuracy: 0.9255 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.9671\n",
      "Epoch 11: val_loss did not improve from 0.05512\n",
      "760/760 [==============================] - 36s 47ms/step - loss: 0.1643 - accuracy: 0.9671 - val_loss: 0.1834 - val_accuracy: 0.9559 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9605\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05512\n",
      "760/760 [==============================] - 47s 62ms/step - loss: 0.2114 - accuracy: 0.9605 - val_loss: 0.1032 - val_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, checkpoint_qat],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp9cokw5lz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp9cokw5lz/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-04 15:51:23.635432: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-04 15:51:23.635699: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-04 15:51:23.638621: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp9cokw5lz\n",
      "2024-01-04 15:51:23.676348: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-04 15:51:23.676371: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp9cokw5lz\n",
      "2024-01-04 15:51:23.811605: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-04 15:51:25.148247: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp9cokw5lz\n",
      "2024-01-04 15:51:25.498934: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1860692 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108264"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.load_weights(checkpoint_qat_path)\n",
    "\n",
    "q_model.save('./saved_models/'+model_name+'_qat'+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_qat_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  ResNet24_6axis\n",
      "input:  {'name': 'serving_default_input_4:0', 'index': 0, 'shape': array([ 1, 50,  6], dtype=int32), 'shape_signature': array([-1, 50,  6], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 106, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[171   2]\n",
      " [ 14 163]]\n",
      "Confusion matrix, without normalization\n",
      "[[171   2]\n",
      " [ 14 163]]\n",
      "accuracy:  0.9542857142857143\n",
      "f1_score:  0.9532163742690059\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHpCAYAAAChumdzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPNElEQVR4nO3deVxV1frH8e9BBAwZxAEkEXDIIU3NzHA2STPHtMyyQnOom0POZoUDpqQ5pTlk9XPo6i0rtbQyTcuRzAlTM+cpFS0NSQ1Ezv79YZw6gcWRA+ds+bx97dfLs/Y6az/by7XHZ621t8UwDEMAAABuzMPVAQAAAPwbEhYAAOD2SFgAAIDbI2EBAABuj4QFAAC4PRIWAADg9khYAACA2yNhAQAAbo+EBQAAuD0SFsDkDh48qObNmysgIEAWi0XLli1z6vjHjh2TxWLRvHnznDqumTVp0kRNmjRxdRhAgULCAjjB4cOH9eyzz6pcuXLy8fGRv7+/6tevrzfeeEO///57nl47JiZGu3fv1tixY/Xee+/pnnvuydPr5aeuXbvKYrHI398/2z/HgwcPymKxyGKxaOLEiQ6Pf/r0aY0aNUqJiYlOiBZAXvJ0dQCA2X322Wd69NFH5e3traefflrVqlXT1atXtXHjRg0ZMkR79+7VnDlz8uTav//+uxISEvTyyy+rT58+eXKN8PBw/f777ypcuHCejP9vPD09deXKFS1fvlydOnWyO7dw4UL5+PgoNTX1psY+ffq0Ro8erYiICNWsWTPH31u1atVNXQ/AzSNhAXLh6NGj6ty5s8LDw7V27VqVLl3adq537946dOiQPvvsszy7/s8//yxJCgwMzLNrWCwW+fj45Nn4/8bb21v169fX//73vywJy6JFi9SqVSt9/PHH+RLLlStXdNttt8nLyytfrgfgT0wJAbkwYcIEXbp0Se+++65dspKpQoUKeuGFF2yfr127pjFjxqh8+fLy9vZWRESEXnrpJaWlpdl9LyIiQq1bt9bGjRt17733ysfHR+XKldOCBQtsfUaNGqXw8HBJ0pAhQ2SxWBQRESHp+lRK5u//atSoUbJYLHZtq1evVoMGDRQYGKiiRYuqUqVKeumll2znb7SGZe3atWrYsKF8fX0VGBiodu3aad++fdle79ChQ+ratasCAwMVEBCgbt266cqVKzf+g/2bJ554Ql988YWSk5NtbVu3btXBgwf1xBNPZOl/4cIFDR48WNWrV1fRokXl7++vli1bateuXbY+33zzjerUqSNJ6tatm21qKfM+mzRpomrVqmn79u1q1KiRbrvtNtufy9/XsMTExMjHxyfL/bdo0ULFihXT6dOnc3yvALJHwgLkwvLly1WuXDnVq1cvR/179OihESNG6O6779aUKVPUuHFjxcfHq3Pnzln6Hjp0SI888ogeeOABTZo0ScWKFVPXrl21d+9eSVKHDh00ZcoUSdLjjz+u9957T1OnTnUo/r1796p169ZKS0tTXFycJk2apLZt22rTpk3/+L2vvvpKLVq00Llz5zRq1CgNHDhQmzdvVv369XXs2LEs/Tt16qTffvtN8fHx6tSpk+bNm6fRo0fnOM4OHTrIYrFoyZIltrZFixapcuXKuvvuu7P0P3LkiJYtW6bWrVtr8uTJGjJkiHbv3q3GjRvbkocqVaooLi5OktSrVy+99957eu+999SoUSPbOOfPn1fLli1Vs2ZNTZ06VU2bNs02vjfeeEMlS5ZUTEyMMjIyJElvvfWWVq1apenTpys0NDTH9wrgBgwAN+XixYuGJKNdu3Y56p+YmGhIMnr06GHXPnjwYEOSsXbtWltbeHi4IclYv369re3cuXOGt7e3MWjQIFvb0aNHDUnG66+/bjdmTEyMER4eniWGkSNHGn/9v/2UKVMMScbPP/98w7gzrzF37lxbW82aNY1SpUoZ58+ft7Xt2rXL8PDwMJ5++uks13vmmWfsxnz44YeN4sWL3/Caf70PX19fwzAM45FHHjGaNWtmGIZhZGRkGCEhIcbo0aOz/TNITU01MjIystyHt7e3ERcXZ2vbunVrlnvL1LhxY0OSMXv27GzPNW7c2K7tyy+/NCQZr776qnHkyBGjaNGiRvv27f/1HgHkDBUW4CalpKRIkvz8/HLU//PPP5ckDRw40K590KBBkpRlrUvVqlXVsGFD2+eSJUuqUqVKOnLkyE3H/HeZa18++eQTWa3WHH3nzJkzSkxMVNeuXRUUFGRrv+uuu/TAAw/Y7vOvnnvuObvPDRs21Pnz521/hjnxxBNP6JtvvlFSUpLWrl2rpKSkbKeDpOvrXjw8rv/1lpGRofPnz9umu3bs2JHja3p7e6tbt2456tu8eXM9++yziouLU4cOHeTj46O33norx9cC8M9IWICb5O/vL0n67bffctT/+PHj8vDwUIUKFezaQ0JCFBgYqOPHj9u1ly1bNssYxYoV06+//nqTEWf12GOPqX79+urRo4eCg4PVuXNnLV68+B+Tl8w4K1WqlOVclSpV9Msvv+jy5ct27X+/l2LFikmSQ/fy0EMPyc/PTx988IEWLlyoOnXqZPmzzGS1WjVlyhRVrFhR3t7eKlGihEqWLKnvv/9eFy9ezPE1b7/9docW2E6cOFFBQUFKTEzUtGnTVKpUqRx/F8A/I2EBbpK/v79CQ0O1Z88eh77390WvN1KoUKFs2w3DuOlrZK6vyFSkSBGtX79eX331lZ566il9//33euyxx/TAAw9k6ZsbubmXTN7e3urQoYPmz5+vpUuX3rC6Iknjxo3TwIED1ahRI/33v//Vl19+qdWrV+vOO+/McSVJuv7n44idO3fq3LlzkqTdu3c79F0A/4yEBciF1q1b6/Dhw0pISPjXvuHh4bJarTp48KBd+9mzZ5WcnGzb8eMMxYoVs9tRk+nvVRxJ8vDwULNmzTR58mT98MMPGjt2rNauXauvv/4627Ez49y/f3+Wcz/++KNKlCghX1/f3N3ADTzxxBPauXOnfvvtt2wXKmf66KOP1LRpU7377rvq3Lmzmjdvrujo6Cx/JjlNHnPi8uXL6tatm6pWrapevXppwoQJ2rp1q9PGBwo6EhYgF4YOHSpfX1/16NFDZ8+ezXL+8OHDeuONNyRdn9KQlGUnz+TJkyVJrVq1clpc5cuX18WLF/X999/b2s6cOaOlS5fa9btw4UKW72Y+QO3vW60zlS5dWjVr1tT8+fPtEoA9e/Zo1apVtvvMC02bNtWYMWP05ptvKiQk5Ib9ChUqlKV68+GHH+rUqVN2bZmJVXbJnaOGDRumEydOaP78+Zo8ebIiIiIUExNzwz9HAI7hwXFALpQvX16LFi3SY489pipVqtg96Xbz5s368MMP1bVrV0lSjRo1FBMTozlz5ig5OVmNGzfWd999p/nz56t9+/Y33DJ7Mzp37qxhw4bp4YcfVr9+/XTlyhXNmjVLd9xxh92i07i4OK1fv16tWrVSeHi4zp07p5kzZ6pMmTJq0KDBDcd//fXX1bJlS0VFRal79+76/fffNX36dAUEBGjUqFFOu4+/8/Dw0CuvvPKv/Vq3bq24uDh169ZN9erV0+7du7Vw4UKVK1fOrl/58uUVGBio2bNny8/PT76+vqpbt64iIyMdimvt2rWaOXOmRo4cadtmPXfuXDVp0kSxsbGaMGGCQ+MByIaLdykBt4QDBw4YPXv2NCIiIgwvLy/Dz8/PqF+/vjF9+nQjNTXV1i89Pd0YPXq0ERkZaRQuXNgICwszhg8fbtfHMK5va27VqlWW6/x9O+2NtjUbhmGsWrXKqFatmuHl5WVUqlTJ+O9//5tlW/OaNWuMdu3aGaGhoYaXl5cRGhpqPP7448aBAweyXOPvW3+/+uoro379+kaRIkUMf39/o02bNsYPP/xg1yfzen/fNj137lxDknH06NEb/pkahv225hu50bbmQYMGGaVLlzaKFCli1K9f30hISMh2O/Inn3xiVK1a1fD09LS7z8aNGxt33nlnttf86zgpKSlGeHi4cffddxvp6el2/QYMGGB4eHgYCQkJ/3gPAP6dxTAcWPUGAADgAqxhAQAAbo+EBQAAuD0SFgAA4PZIWAAAgNsjYQEAAG6PhAUAALg9HhyXT6xWq06fPi0/Pz+nPg4cAJC/DMPQb7/9ptDQUNtbwfNaamqqrl696pSxvLy85OPj45Sx8hMJSz45ffq0wsLCXB0GAMBJTp48qTJlyuT5dVJTU1XEr7h07YpTxgsJCdHRo0dNl7SQsOQTPz8/SZJX1RhZCuX8dfWA2Zz4ZqKrQwDy1G8pKaoQGWb7ez2vXb16Vbp2Rd53dpNy+9+PjKtK2jtXV69eJWFB9jKngSyFvEhYcEvz9/d3dQhAvsj36X0n/PfDzI+2J2EBAMAMLJJymySZeAklCQsAAGZg8bh+5HYMkzJv5AAAoMCgwgIAgBlYLE6YEjLvnBAJCwAAZlDAp4RIWAAAMIMCXmExb6oFAAAKDCosAACYghOmhExcpyBhAQDADJgSAgAAcG9UWAAAMAN2CQEAALfHlBAAAIB7o8ICAIAZMCUEAADcHlNCAAAA7o0KCwAAZlDAp4TMGzkAAAWJxfJn0nLTh2NTQuvXr1ebNm0UGhoqi8WiZcuWZemzb98+tW3bVgEBAfL19VWdOnV04sQJ2/nU1FT17t1bxYsXV9GiRdWxY0edPXvW4dsnYQEAANm6fPmyatSooRkzZmR7/vDhw2rQoIEqV66sb775Rt9//71iY2Pl4+Nj6zNgwAAtX75cH374odatW6fTp0+rQ4cODsfClBAAAGbgYbl+5HYMB7Rs2VItW7a84fmXX35ZDz30kCZMmGBrK1++vO33Fy9e1LvvvqtFixbp/vvvlyTNnTtXVapU0bfffqv77rsv56E7FDkAAHCNXE8H/bkGJiUlxe5IS0tzOByr1arPPvtMd9xxh1q0aKFSpUqpbt26dtNG27dvV3p6uqKjo21tlStXVtmyZZWQkODQ9UhYAAAwg8xtzbk9JIWFhSkgIMB2xMfHOxzOuXPndOnSJb322mt68MEHtWrVKj388MPq0KGD1q1bJ0lKSkqSl5eXAgMD7b4bHByspKQkh67HlBAAAAXMyZMn5e/vb/vs7e3t8BhWq1WS1K5dOw0YMECSVLNmTW3evFmzZ89W48aNnRPsH0hYAAAwAydua/b397dLWG5GiRIl5OnpqapVq9q1V6lSRRs3bpQkhYSE6OrVq0pOTrarspw9e1YhISEOXY8pIQAAzMCJU0LO4OXlpTp16mj//v127QcOHFB4eLgkqXbt2ipcuLDWrFljO79//36dOHFCUVFRDl2PCgsAAMjWpUuXdOjQIdvno0ePKjExUUFBQSpbtqyGDBmixx57TI0aNVLTpk21cuVKLV++XN98840kKSAgQN27d9fAgQMVFBQkf39/9e3bV1FRUQ7tEJJIWAAAMAcXPOl227Ztatq0qe3zwIEDJUkxMTGaN2+eHn74Yc2ePVvx8fHq16+fKlWqpI8//lgNGjSwfWfKlCny8PBQx44dlZaWphYtWmjmzJmOh24YhuHwt+CwlJQUBQQEyLt6T1kKebk6HCDP/Lr1TVeHAOSplJQUBRcP0MWLF3O9DiSn1wsICJD3/WNk8fT59y/8A+NaqtLWxuZb7M7EGhYAAOD2mBICAMAMCvjLD0lYAAAwA2fs8nHiLqH8Zt5UCwAAFBhUWAAAMAUnTAmZuE5BwgIAgBkU8CkhEhYAAMzAYnHColvzJizmrQ0BAIACgwoLAABmwLZmAADg9gr4GhbzploAAKDAoMICAIAZMCUEAADcHlNCAAAA7o0KCwAAZsCUEAAAcHtMCQEAALg3KiwAAJiAxWKRpQBXWEhYAAAwgYKesDAlBAAA3B4VFgAAzMDyx5HbMUyKhAUAABMo6FNCJCwAAJhAQU9YWMMCAADcHhUWAABMoKBXWEhYAAAwgYKesDAlBAAA3B4VFgAAzIBtzQAAwN0xJQQAAODmqLAAAGACFoucUGFxTiyuQMICAIAJWOSEKSETZyxMCQEAALdHwgIAgAlkLrrN7eGI9evXq02bNgoNDZXFYtGyZctu2Pe5556TxWLR1KlT7dovXLigLl26yN/fX4GBgerevbsuXbrk8P2TsAAAYAYWJx0OuHz5smrUqKEZM2b8Y7+lS5fq22+/VWhoaJZzXbp00d69e7V69WqtWLFC69evV69evRwLRKxhAQAAN9CyZUu1bNnyH/ucOnVKffv21ZdffqlWrVrZndu3b59WrlyprVu36p577pEkTZ8+XQ899JAmTpyYbYJzI1RYAAAwA2dMB/0xJZSSkmJ3pKWl3VRIVqtVTz31lIYMGaI777wzy/mEhAQFBgbakhVJio6OloeHh7Zs2eLQtUhYAAAwAWeuYQkLC1NAQIDtiI+Pv6mYxo8fL09PT/Xr1y/b80lJSSpVqpRdm6enp4KCgpSUlOTQtZgSAgDABJzxpNvM7588eVL+/v62dm9vb4fH2r59u9544w3t2LHDCdut/x0VFgAAChh/f3+742YSlg0bNujcuXMqW7asPD095enpqePHj2vQoEGKiIiQJIWEhOjcuXN237t27ZouXLigkJAQh65HhQUAADNws5cfPvXUU4qOjrZra9GihZ566il169ZNkhQVFaXk5GRt375dtWvXliStXbtWVqtVdevWdeh6JCwAAJiAM6eEcurSpUs6dOiQ7fPRo0eVmJiooKAglS1bVsWLF7frX7hwYYWEhKhSpUqSpCpVqujBBx9Uz549NXv2bKWnp6tPnz7q3LmzQzuEJKaEAADADWzbtk21atVSrVq1JEkDBw5UrVq1NGLEiByPsXDhQlWuXFnNmjXTQw89pAYNGmjOnDkOx0KFBQAAE3BFhaVJkyYyDCPH/Y8dO5alLSgoSIsWLXLoutkhYQEAwARckbC4E6aEAACA26PCAgCACRT0CgsJCwAAZuBm25rzG1NCAADA7VFhAQDABJgSAgAAbo+EBQAAuD0SFsBE6t9dXgOejtbdVcuqdMkAdRowR8u/+d52/vedb2b7vZemLNWUBWskSUO7t1DLhnfqrjvK6Oq1ayrdaGi+xA44y+vj47Vs6RId2P+jihQporpR9TR23Hjd8cfj0IFbEYtuYSq+Rby1+8Ap9Y//INvzEdHD7Y5eI/8rq9WqpWsSbX28ChfSktU79fZHG/IpasC5Nqxfp+f+01vrNn6rFV+s1rX0dLV+qLkuX77s6tCQlyxOOkyKCgtMZdWmH7Rq0w83PH/2/G92n9s0qa51Ww/q2KnztrZXZ38uSXqyjWNvCgXcxaefrbT7POfdeSobWko7d2xXg4aNXBQV8lpBnxKiwoJbVqkgPz3YoJrmL0twdShAnkq5eFGSVKxYkIsjAfIOFRbcsp5sU1e/XUnVsrWJrg4FyDNWq1VDBvVXVL36urNaNVeHgzxEhQU51rVrV7Vv3972uUmTJurfv7/L4sE/e7rdffrgi21Ku3rN1aEAeaZ/397au3ePFix839WhII9ZZLElLTd9mHgRi0sTlq5du8pisei1116za1+2bJnDWWBERISmTp2ao35//x+wTJkyDl0L7q9+rfKqFBmiuUs3uzoUIM/079dHn3++Ql+u/pq/x3DLc3mFxcfHR+PHj9evv/6ab9eMi4vTmTNnbMfOnTvz7drIHzHto7T9hxPafeCUq0MBnM4wDPXv10effrJUK1etVURkpKtDQj7IdXXFCVNKruTyhCU6OlohISGKj4//x34ff/yx7rzzTnl7eysiIkKTJk2ynWvSpImOHz+uAQMG5Oh/ED8/P4WEhNiOkiVLKiMjQ927d1dkZKSKFCmiSpUq6Y033nDKPcJ5fIt46a47btddd9wuSYq4vbjuuuN2hYUUs/Xx8/VRhwdqad4NqithIcWuf6d0MRXy8LCN51vEK1/uAcit/n176/1F/9X89xapqJ+fkpKSlJSUpN9//93VoSEvsa3ZtQoVKqRx48bpiSeeUL9+/bIta27fvl2dOnXSqFGj9Nhjj2nz5s16/vnnVbx4cXXt2lVLlixRjRo11KtXL/Xs2fOm4rBarSpTpow+/PBDFS9eXJs3b1avXr1UunRpderUyeHx0tLSlJaWZvuckpJyU3HB3t1Vw7XqnRdsnycM7ihJeu/Tb9Vr5H8lSY+2qC2LLFq8clu2Y8T+p5Weanuf7fOWD4ZLkpr3eEMbth/Mq9ABp5nz1ixJUvNmTezb35mrp2K65n9AQD5wecIiSQ8//LBq1qypkSNH6t13381yfvLkyWrWrJliY2MlSXfccYd++OEHvf766+ratauCgoJUqFAhW+Xk3wwbNkyvvPKK7fO4cePUr18/jR492tYWGRmphIQELV68+KYSlvj4eLvx4Bwbth9UkVp9/rHP/y3ZpP9bsumG53uN/K8tuQHM6Pd0w9UhwAXYJeQmxo8fr/nz52vfvn1Zzu3bt0/169e3a6tfv74OHjyojIwMh681ZMgQJSYm2o6nn35akjRjxgzVrl1bJUuWVNGiRTVnzhydOHHipu5n+PDhunjxou04efLkTY0DAIDEGha3qLBIUqNGjdSiRQsNHz5cXbt2zdNrlShRQhUqVLBre//99zV48GBNmjRJUVFR8vPz0+uvv64tW7bc1DW8vb3l7e3tjHABACjw3CZhkaTXXntNNWvWVKW/vcCrSpUq2rTJvsS/adMm3XHHHSpUqJAkycvL66aqLX8dr169enr++edtbYcPH77p8QAAcCaL5fqR2zHMym2mhCSpevXq6tKli6ZNm2bXPmjQIK1Zs0ZjxozRgQMHNH/+fL355psaPHiwrU9ERITWr1+vU6dO6ZdffnH42hUrVtS2bdv05Zdf6sCBA4qNjdXWrVtzfU8AADjD9YQlt1NCrr6Lm+dWCYt0/RkpVqvVru3uu+/W4sWL9f7776tatWoaMWKE4uLi7KaO4uLidOzYMZUvX14lS5Z0+LrPPvusOnTooMcee0x169bV+fPn7aotAAC4lOXPKsvNHmbe1mwxDIPl5vkgJSVFAQEB8q7eU5ZCPO8Dt65ft77p6hCAPJWSkqLg4gG6ePGi/P398+V6AQEBKtfvIxXy9s3VWBlpl3Vk2iP5FrszudUaFgAAkL2Cvq2ZhAUAABNg0S0AAICbo8ICAIAJeHhY5OGRuxKJkcvvuxIJCwAAJsCUEAAAgJujwgIAgAmwSwgAALg9poQAAACysX79erVp00ahoaGyWCxatmyZ7Vx6erqGDRum6tWry9fXV6GhoXr66ad1+vRpuzEuXLigLl26yN/fX4GBgerevbsuXbrkcCwkLAAAmEDu3yPk+JTS5cuXVaNGDc2YMSPLuStXrmjHjh2KjY3Vjh07tGTJEu3fv19t27a169elSxft3btXq1ev1ooVK7R+/Xr16tXL4ftnSggAABNwxRqWli1bqmXLltmeCwgI0OrVq+3a3nzzTd177706ceKEypYtq3379mnlypXaunWr7rnnHknS9OnT9dBDD2nixIkKDQ3NcSxUWAAAMIHcvvjwr2tgUlJS7I60tDSnxHjx4kVZLBYFBgZKkhISEhQYGGhLViQpOjpaHh4e2rJli0Njk7AAAFDAhIWFKSAgwHbEx8fneszU1FQNGzZMjz/+uO3FiklJSSpVqpRdP09PTwUFBSkpKcmh8ZkSAgDABCxywpSQrn//5MmTdm9r9vb2ztW46enp6tSpkwzD0KxZs3I11o2QsAAAYALO3Nbs7+9vl7DkRmaycvz4ca1du9Zu3JCQEJ07d86u/7Vr13ThwgWFhIQ4dB2mhAAAwE3JTFYOHjyor776SsWLF7c7HxUVpeTkZG3fvt3WtnbtWlmtVtWtW9eha1FhAQDABFyxS+jSpUs6dOiQ7fPRo0eVmJiooKAglS5dWo888oh27NihFStWKCMjw7YuJSgoSF5eXqpSpYoefPBB9ezZU7Nnz1Z6err69Omjzp07O7RDSCJhAQDAFFzxpNtt27apadOmts8DBw6UJMXExGjUqFH69NNPJUk1a9a0+97XX3+tJk2aSJIWLlyoPn36qFmzZvLw8FDHjh01bdo0h2MnYQEAANlq0qSJDMO44fl/OpcpKChIixYtynUsJCwAAJgALz8EAABuj5cfAgAAuDkqLAAAmABTQgAAwP05YUpI5s1XmBICAADujwoLAAAmwJQQAABwewV9lxAJCwAAJlDQKyysYQEAAG6PCgsAACbAlBAAAHB7TAkBAAC4OSosAACYQEGvsJCwAABgAgV9DQtTQgAAwO1RYQEAwASYEgIAAG6PKSEAAAA3R4UFAAATYEoIAAC4PYucMCXklEhcg4QFAAAT8LBY5JHLjCW333cl1rAAAAC3R4UFAAATKOi7hEhYAAAwgYK+6JYpIQAA4PaosAAAYAIelutHbscwKxIWAADMwOKEKR0TJyxMCQEAALdHhQUAABNglxAAAHB7lj9+5XYMs2JKCAAAuD0qLAAAmEBB3yVEhQUAABPIfHBcbg9HrF+/Xm3atFFoaKgsFouWLVtmd94wDI0YMUKlS5dWkSJFFB0drYMHD9r1uXDhgrp06SJ/f38FBgaqe/fuunTpksP3T8ICAACydfnyZdWoUUMzZszI9vyECRM0bdo0zZ49W1u2bJGvr69atGih1NRUW58uXbpo7969Wr16tVasWKH169erV69eDseSoymhTz/9NMcDtm3b1uEgAADAP3PFLqGWLVuqZcuW2Z4zDENTp07VK6+8onbt2kmSFixYoODgYC1btkydO3fWvn37tHLlSm3dulX33HOPJGn69Ol66KGHNHHiRIWGhuY4lhwlLO3bt8/RYBaLRRkZGTm+OAAAyBkPi0UeucxYMr+fkpJi1+7t7S1vb2+Hxjp69KiSkpIUHR1tawsICFDdunWVkJCgzp07KyEhQYGBgbZkRZKio6Pl4eGhLVu26OGHH8557DnpZLVac3SQrAAAkDcyKyy5PSQpLCxMAQEBtiM+Pt7heJKSkiRJwcHBdu3BwcG2c0lJSSpVqpTdeU9PTwUFBdn65FSudgmlpqbKx8cnN0MAAIB8dvLkSfn7+9s+O1pdcQWHF91mZGRozJgxuv3221W0aFEdOXJEkhQbG6t3333X6QECAADn7hLy9/e3O24mYQkJCZEknT171q797NmztnMhISE6d+6c3flr167pwoULtj455XDCMnbsWM2bN08TJkyQl5eXrb1atWp65513HB0OAADkgDOnhJwhMjJSISEhWrNmja0tJSVFW7ZsUVRUlCQpKipKycnJ2r59u63P2rVrZbVaVbduXYeu53DCsmDBAs2ZM0ddunRRoUKFbO01atTQjz/+6OhwAADATV26dEmJiYlKTEyUdH2hbWJiok6cOCGLxaL+/fvr1Vdf1aeffqrdu3fr6aefVmhoqG2zTpUqVfTggw+qZ8+e+u6777Rp0yb16dNHnTt3dmiHkHQTa1hOnTqlChUqZGm3Wq1KT093dDgAAJADztwllFPbtm1T06ZNbZ8HDhwoSYqJidG8efM0dOhQXb58Wb169VJycrIaNGiglStX2q1vXbhwofr06aNmzZrJw8NDHTt21LRp0xyO3eGEpWrVqtqwYYPCw8Pt2j/66CPVqlXL4QAAAMC/s/xx5HYMRzRp0kSGYdx4PItFcXFxiouLu2GfoKAgLVq0yMErZ+VwwjJixAjFxMTo1KlTslqtWrJkifbv368FCxZoxYoVuQ4IAADg7xxew9KuXTstX75cX331lXx9fTVixAjt27dPy5cv1wMPPJAXMQIAUOC54l1C7uSmnsPSsGFDrV692tmxAACAGyjob2u+6QfHbdu2Tfv27ZN0fV1L7dq1nRYUAADAXzmcsPz00096/PHHtWnTJgUGBkqSkpOTVa9ePb3//vsqU6aMs2MEAKDAc8aUjpmnhBxew9KjRw+lp6dr3759unDhgi5cuKB9+/bJarWqR48eeREjAACQ+zw0zhUcrrCsW7dOmzdvVqVKlWxtlSpV0vTp09WwYUOnBgcAACDdRMISFhaW7QPiMjIyHH5qHQAAyBmmhBz0+uuvq2/fvtq2bZutbdu2bXrhhRc0ceJEpwYHAACuy9wllNvDrHJUYSlWrJhdVnb58mXVrVtXnp7Xv37t2jV5enrqmWeesb0/AAAAOE9Br7DkKGGZOnVqHocBAABwYzlKWGJiYvI6DgAA8A9c8S4hd3LTD46TpNTUVF29etWuzd/fP1cBAQCArFzxtmZ34vCi28uXL6tPnz4qVaqUfH19VaxYMbsDAADA2RxOWIYOHaq1a9dq1qxZ8vb21jvvvKPRo0crNDRUCxYsyIsYAQAo8HL70DizPzzO4Smh5cuXa8GCBWrSpIm6deumhg0bqkKFCgoPD9fChQvVpUuXvIgTAIACraDvEnK4wnLhwgWVK1dO0vX1KhcuXJAkNWjQQOvXr3dudAAAALqJhKVcuXI6evSoJKly5cpavHixpOuVl8yXIQIAAOcq6FNCDics3bp1065duyRJL774ombMmCEfHx8NGDBAQ4YMcXqAAADgz11CuT3MyuE1LAMGDLD9Pjo6Wj/++KO2b9+uChUq6K677nJqcAAAAFIun8MiSeHh4QoPD3dGLAAA4AacMaVj4gJLzhKWadOm5XjAfv363XQwAAAgewV9l1COEpYpU6bkaDCLxULC8i/2fB4vP54GjFtYsaYjXB0CkKeMa2kuua6HbmLhaTZjmFWOEpbMXUEAAACukOs1LAAAIO8xJQQAANyexSJ5FOBFt2aezgIAAAUEFRYAAEzAwwkVltx+35VIWAAAMIGCvoblpqaENmzYoCeffFJRUVE6deqUJOm9997Txo0bnRocAACAdBMJy8cff6wWLVqoSJEi2rlzp9LSru9Hv3jxosaNG+f0AAEAwJ9TQrk9zMrhhOXVV1/V7Nmz9fbbb6tw4cK29vr162vHjh1ODQ4AAFzH25odtH//fjVq1ChLe0BAgJKTk50REwAAgB2HE5aQkBAdOnQoS/vGjRtVrlw5pwQFAADseVgsTjkckZGRodjYWEVGRqpIkSIqX768xowZI8MwbH0Mw9CIESNUunRpFSlSRNHR0Tp48KCzb9/xhKVnz5564YUXtGXLFlksFp0+fVoLFy7U4MGD9Z///MfpAQIAgD/fJZTbwxHjx4/XrFmz9Oabb2rfvn0aP368JkyYoOnTp9v6TJgwQdOmTdPs2bO1ZcsW+fr6qkWLFkpNTc3V/f6dw9uaX3zxRVmtVjVr1kxXrlxRo0aN5O3trcGDB6tv375ODQ4AALjO5s2b1a5dO7Vq1UqSFBERof/973/67rvvJF2vrkydOlWvvPKK2rVrJ0lasGCBgoODtWzZMnXu3NlpsThcYbFYLHr55Zd14cIF7dmzR99++61+/vlnjRkzxmlBAQAAe85cdJuSkmJ3ZO74/bt69eppzZo1OnDggCRp165d2rhxo1q2bCnp+suRk5KSFB0dbftOQECA6tatq4SEBKfe/00/OM7Ly0tVq1Z1ZiwAAOAGPOT4GpTsxpCksLAwu/aRI0dq1KhRWfq/+OKLSklJUeXKlVWoUCFlZGRo7Nix6tKliyQpKSlJkhQcHGz3veDgYNs5Z3E4YWnatOk/Pilv7dq1uQoIAABk5YxtyZnfP3nypPz9/W3t3t7e2fZfvHixFi5cqEWLFunOO+9UYmKi+vfvr9DQUMXExOQuGAc5nLDUrFnT7nN6eroSExO1Z8+efA8eAAA4zt/f3y5huZEhQ4boxRdftK1FqV69uo4fP674+HjFxMQoJCREknT27FmVLl3a9r2zZ89myRdyy+GEZcqUKdm2jxo1SpcuXcp1QAAAICtXvPzwypUr8vCwX+5aqFAhWa1WSVJkZKRCQkK0Zs0aW4KSkpKiLVu2OH3nsNNefvjkk0/q3nvv1cSJE501JAAA+IPFolyvYXH0623atNHYsWNVtmxZ3Xnnndq5c6cmT56sZ5555o/xLOrfv79effVVVaxYUZGRkYqNjVVoaKjat2+fq1j/zmkJS0JCgnx8fJw1HAAAcLHp06crNjZWzz//vM6dO6fQ0FA9++yzGjFihK3P0KFDdfnyZfXq1UvJyclq0KCBVq5c6fScwOGEpUOHDnafDcPQmTNntG3bNsXGxjotMAAA8CdnLrrNKT8/P02dOlVTp079hzEtiouLU1xcXO6C+xcOJywBAQF2nz08PFSpUiXFxcWpefPmTgsMAAD8yRVrWNyJQwlLRkaGunXrpurVq6tYsWJ5FRMAAIAdh550W6hQITVv3py3MgMAkM8sTvplVg4/mr9atWo6cuRIXsQCAABuIHNKKLeHWTmcsLz66qsaPHiwVqxYoTNnzmR5HwEAAICz5XgNS1xcnAYNGqSHHnpIktS2bVu7R/QbhiGLxaKMjAznRwkAQAHHotscGj16tJ577jl9/fXXeRkPAADIhsVi+cd3+eV0DLPKccJiGIYkqXHjxnkWDAAAyF5Br7A4tIbFzJkZAAAwL4eew3LHHXf8a9Jy4cKFXAUEAACycsWTbt2JQwnL6NGjszzpFgAA5D0PiyXXLz/M7fddyaGEpXPnzipVqlRexQIAAJCtHCcsrF8BAMB1CvqiW4d3CQEAABdwwhoWEz+ZP+cJi9Vqzcs4AAAAbsihNSwAAMA1PGSRRy5LJLn9viuRsAAAYAIFfVuzwy8/BAAAyG9UWAAAMAF2CQEAALdX0B8cx5QQAABwe1RYAAAwgYK+6JaEBQAAE/CQE6aE2NYMAADyUkGvsLCGBQAAuD0qLAAAmICHcl9lMHOVgoQFAAATsFgssuRyTie333clMydbAACggKDCAgCACVj+OHI7hlmRsAAAYAI86RYAAMDNUWEBAMAkzFsfyT0SFgAATIAHxwEAANzAqVOn9OSTT6p48eIqUqSIqlevrm3bttnOG4ahESNGqHTp0ipSpIiio6N18OBBp8dBwgIAgAlkPoclt4cjfv31V9WvX1+FCxfWF198oR9++EGTJk1SsWLFbH0mTJigadOmafbs2dqyZYt8fX3VokULpaamOvX+mRICAMAEXPGk2/HjxyssLExz5861tUVGRtp+bxiGpk6dqldeeUXt2rWTJC1YsEDBwcFatmyZOnfunMuI/0SFBQAAE3BmhSUlJcXuSEtLy/aan376qe655x49+uijKlWqlGrVqqW3337bdv7o0aNKSkpSdHS0rS0gIEB169ZVQkKCU++fhAUAgAImLCxMAQEBtiM+Pj7bfkeOHNGsWbNUsWJFffnll/rPf/6jfv36af78+ZKkpKQkSVJwcLDd94KDg23nnIUpIQAATMCZT7o9efKk/P39be3e3t7Z9rdarbrnnns0btw4SVKtWrW0Z88ezZ49WzExMbmMxjFUWAAAMAFnTgn5+/vbHTdKWEqXLq2qVavatVWpUkUnTpyQJIWEhEiSzp49a9fn7NmztnPOQsICAACyVb9+fe3fv9+u7cCBAwoPD5d0fQFuSEiI1qxZYzufkpKiLVu2KCoqyqmxMCUEAIAJuGKX0IABA1SvXj2NGzdOnTp10nfffac5c+Zozpw5kq5Xffr3769XX31VFStWVGRkpGJjYxUaGqr27dvnMlp7JCwAAJjAzTxHJbsxHFGnTh0tXbpUw4cPV1xcnCIjIzV16lR16dLF1mfo0KG6fPmyevXqpeTkZDVo0EArV66Uj49PrmL9OxIWAABwQ61bt1br1q1veN5isSguLk5xcXF5GgcJCwAAJuDMXUJmRMICAIAJ8PJDAAAAN0eFBQAAE/CQRR65nNTJ7fddiYQFAAATYEoIAADAzVFhAQDABCx//MrtGGZFwgIAgAkU9CkhEhYAAEzA4oRFt2ausLCGBQAAuD0qLAAAmABTQgAAwO0V9ISFKSEAAOD2qLAAAGACbGsGAABuz8Ny/cjtGGbFlBAAAHB7VFgAADABpoQAAIDbY5cQYHIJmzboqcfaq0alcIUEeOmLFZ/csO/Q/r0VEuClOTOn5WOEgGPq1wjXR6910ZGlg/X7hji1aVg5S59K4SX0YfwTSvriJf2y6hVtnPOswkoF2M5PH9xGe9/vrwtfxerE8mFaPO5x3VG2RH7eBuBUJCwwvStXLuvOancpfuIb/9jv8+XLtH3bFoWUDs2nyICb4+vjpd2HktR/8mfZno8MLaY1M3rowIlf1KLf/6lO1xmKn/+NUq9es/XZuf+0esUvVc0np6vtoAWyWCxaMflpeZh51WUBZ9Gf00I3/8u8mBKC6TV74EE1e+DBf+xz5vQpvTx0gP63ZIWe7NQ+fwIDbtKqLQe1asvBG54f3StaX357QC/PWmVrO3r6V7s+/7d8u+33J5KSNfqdNdo6r7fCQwKz9IU5sEsIuMVZrVb16dVNz/cbqMpV7nR1OECuWCwWPRh1hw6ePK9PJz2t458O1fq3emU7bZTpNp/CevqhWjp6+oJ+OpeSj9HCmXJfXTF3jYWEJYfmzZunwMBA2+dRo0apZs2aLosHOffmlNfl6empHs/1cXUoQK6VKuYrv9u8NbhLQ63eclBtBi7Qp+v36f1XO6tBzQi7vr3a19HPX76s86tj1bxuRbUaMF/p1zJcEziQSwUuYenatassFkuW49ChQ64ODXlg184denv2m3pj1juymHl5PPAHjz9+jlds/FHTFyfo+0NJmrhwgz7ffEA9291j1/f91d/rvu6zFN3nXR08eV7/jXtM3l6sBDCrzF1CuT3MqsAlLJL04IMP6syZM3ZHZGSkq8NCHtiSsFG//HxOte8sr9uDiuj2oCL66cRxjXp5qO6pXtHV4QEO++XiFaVfy9C+Yz/bte8//rPCggPt2lIup+nwTxe0addxPRH7gSqVLaF2DavkY7RwJouTDrMqkKm2t7e3QkJC7NomT56suXPn6siRIwoKClKbNm00YcIEFS1a1EVRwhke6dxFDZvcb9f2eIfWeuSxJ9T5yRgXRQXcvPRrGdq+75TuKFvcrr1iWHGdSEq+4fcy/3Xt5VUojyME8kaBTFiy4+HhoWnTpikyMlJHjhzR888/r6FDh2rmzJk3NV5aWprS0tJsn1NSWOiWVy5fuqSjR/6c0jtx/Jj2fJ+owGJBKhNWVkFB9n+xexYurFLBIapQsVJ+hwrkiG8RL5W/Pcj2OaJ0Md1VIUS/pvyuk+cuasr/Num90Y9q467jWrfjqJrXraCH6lVSi35zbf0faVZNa747pF+Sr+j2Uv4a1KWhfk+7pi8Tbrz7CO7NQxbblGBuxjCrApmwrFixwq5y0rJlS3344Ye2zxEREXr11Vf13HPP3XTCEh8fr9GjR+c6Vvy7xJ3b1bH1A7bPI18aIknq9MRTmjbrXVeFBdy0uyuFatX0Z2yfJ/RtKUl674ud6jVuqT7dsE99Jy7XkCcbadILD+nAiV/0eOwH2rz7hCQp7eo11b8rXH0ejVIxPx+du3BZG3cdU9P/vK2fky+75J6Qe86Y0jFvulJAE5amTZtq1qxZts++vr766quvFB8frx9//FEpKSm6du2aUlNTdeXKFd12220OX2P48OEaOHCg7XNKSorCwsKcEj/s1W/YWEkXr+a4/7bd/AsT7m1D4jEVaTjiH/ss+HynFny+M9tzZ87/poeH/jcvQgNcpkAuuvX19VWFChVsR1pamlq3bq277rpLH3/8sbZv364ZM2ZIkq5ezfl/CP/K29tb/v7+dgcAADetgK+6LZAVlr/bvn27rFarJk2aJA+P6znc4sWLXRwVAAB/Kuhvay6QFZa/q1ChgtLT0zV9+nQdOXJE7733nmbPnu3qsAAAwB9IWCTVqFFDkydP1vjx41WtWjUtXLhQ8fHxrg4LAIA/OeOhceYtsBS8hGXevHlatmxZlvYBAwbo9OnTunLlilauXKmnnnpKhmHYHsfftWtXJScn2/qPGjVKiYmJ+RIzAACuXsLy2muvyWKxqH///ra21NRU9e7dW8WLF1fRokXVsWNHnT17NhdXubECl7AAAADHbN26VW+99Zbuuusuu/YBAwZo+fLl+vDDD7Vu3TqdPn1aHTp0yJMYSFgAADADF5VYLl26pC5duujtt99WsWLFbO0XL17Uu+++q8mTJ+v+++9X7dq1NXfuXG3evFnffvvtzd/nDZCwAABgAhYn/ZKuPxvsr8dfn8z+d71791arVq0UHR1t1759+3alp6fbtVeuXFlly5ZVQkKC0++fhAUAABNw5tuaw8LCFBAQYDtutNHk/fff144dO7I9n5SUJC8vL9taz0zBwcFKSkpy9u3zHBYAAAqakydP2j3Q1NvbO9s+L7zwglavXi0fH5/8DC9bVFgAADABZy5h+fuT2LNLWLZv365z587p7rvvlqenpzw9PbVu3TpNmzZNnp6eCg4O1tWrV+120ErS2bNnFRIS4vT7p8ICAIAZ5PPbD5s1a6bdu3fbtXXr1k2VK1fWsGHDFBYWpsKFC2vNmjXq2LGjJGn//v06ceKEoqKichloViQsAAAgCz8/P1WrVs2uzdfXV8WLF7e1d+/eXQMHDlRQUJD8/f3Vt29fRUVF6b777nN6PCQsAACYgDu+S2jKlCny8PBQx44dlZaWphYtWmjmzJlOvUYmEhYAAEzgr7t8cjNGbnzzzTd2n318fDRjxgzNmDEjdwPnAItuAQCA26PCAgCACeTzmlu3Q8ICAIAZFPCMhSkhAADg9qiwAABgAu64Syg/kbAAAGAC7rBLyJWYEgIAAG6PCgsAACZQwNfckrAAAGAKBTxjIWEBAMAECvqiW9awAAAAt0eFBQAAEyjou4RIWAAAMIECvoSFKSEAAOD+qLAAAGAGBbzEQsICAIAJsEsIAADAzVFhAQDABNglBAAA3F4BX8LClBAAAHB/VFgAADCDAl5iIWEBAMAECvouIRIWAADMwAmLbk2cr7CGBQAAuD8qLAAAmEABX8JCwgIAgCkU8IyFKSEAAOD2qLAAAGAC7BICAABur6A/mp8pIQAA4PaosAAAYAIFfM0tCQsAAKZQwDMWpoQAAIDbI2EBAMAELE765Yj4+HjVqVNHfn5+KlWqlNq3b6/9+/fb9UlNTVXv3r1VvHhxFS1aVB07dtTZs2edeeuSSFgAADAFi/7cKXTTh4PXXLdunXr37q1vv/1Wq1evVnp6upo3b67Lly/b+gwYMEDLly/Xhx9+qHXr1un06dPq0KGDU+9dYg0LAAC4gZUrV9p9njdvnkqVKqXt27erUaNGunjxot59910tWrRI999/vyRp7ty5qlKlir799lvdd999TouFCgsAACZgcdIhSSkpKXZHWlpajmK4ePGiJCkoKEiStH37dqWnpys6OtrWp3LlyipbtqwSEhJyc7tZkLAAAGACuZ4O+suD58LCwhQQEGA74uPj//X6VqtV/fv3V/369VWtWjVJUlJSkry8vBQYGGjXNzg4WElJSU69f6aEAAAwBeftaz558qT8/f1trd7e3v/6zd69e2vPnj3auHFjLmO4OSQsAAAUMP7+/nYJy7/p06ePVqxYofXr16tMmTK29pCQEF29elXJycl2VZazZ88qJCTEmSEzJQQAgBk4c0oopwzDUJ8+fbR06VKtXbtWkZGRdudr166twoULa82aNba2/fv368SJE4qKinLGbdtQYQEAwARc8aDb3r17a9GiRfrkk0/k5+dnW5cSEBCgIkWKKCAgQN27d9fAgQMVFBQkf39/9e3bV1FRUU7dISSRsAAAgBuYNWuWJKlJkyZ27XPnzlXXrl0lSVOmTJGHh4c6duyotLQ0tWjRQjNnznR6LCQsAACYwM1M6WQ3hiMMw/jXPj4+PpoxY4ZmzJhxk1HlDAkLAAAmcDOP1s9uDLNi0S0AAHB7VFgAADADV6y6dSMkLAAAmEABz1eYEgIAAO6PCgsAACbgil1C7oSEBQAAEyjou4RIWAAAMIMCvoiFNSwAAMDtUWEBAMAECniBhYQFAAAzKOiLbpkSAgAAbo8KCwAAppD7XUJmnhQiYQEAwASYEgIAAHBzJCwAAMDtMSUEAIAJMCUEAADg5qiwAABgArxLCAAAuD2mhAAAANwcFRYAAEyAdwkBAAD3V8AzFhIWAABMoKAvumUNCwAAcHtUWAAAMIGCvkuIhAUAABMo4EtYmBICAADujwoLAABmUMBLLCQsAACYALuEAAAA3BwVlnxiGIYk6bfffnNxJEDeMq6luToEIE9l/oxn/r2eX377LSXXu3x++y3FOcG4AAlLPslMVO6uGuniSAAAzvDbb78pICAgz6/j5eWlkJAQVYwMc8p4ISEh8vLycspY+cli5HeKWEBZrVadPn1afn5+sph5I7yJpKSkKCwsTCdPnpS/v7+rwwHyBD/n+c8wDP32228KDQ2Vh0f+rKxITU3V1atXnTKWl5eXfHx8nDJWfqLCkk88PDxUpkwZV4dRIPn7+/MXOW55/Jznr/yorPyVj4+PKZMMZ2LRLQAAcHskLAAAwO2RsOCW5e3trZEjR8rb29vVoQB5hp9zFBQsugUAAG6PCgsAAHB7JCwAAMDtkbAAAAC3R8ICAADcHgkL8IdDhw65OgQAwA2QsACSFi5cqJiYGC1fvtzVoQC5YrVaXR0CkCdIWABJkZGRKlSokObMmaMVK1a4OhzAYZ9//rmk668BIWnBrYiEBQXaypUrdeHCBdWrV0+TJk3S5cuXNXPmTJIWmMq2bdv03HPP6ZlnnpFE0oJbEwkLCqyEhAQNGDBAw4cPV3JysurUqaPXXntNqampJC0wlXLlymngwIHatWuXevToIYmkBbceEhYUWHXq1NGTTz6pH374QS+99JJ+/fVX3XvvvSQtMI033nhDGzduVFBQkLp27aqYmBht27aNpAW3JBIWFEhWq1Wenp4aNmyYWrVqpZ07d+rll18maYFp/PLLL/riiy/Utm1bfffddwoMDNTTTz+tZ555hqQFtyQSFhRIHh4eysjIkKenpwYPHqy2bdtmSVrGjx+v1NRUzZkzR0uWLHF1yICdEiVKaNKkSWrRooXatGmjLVu2kLTglkbCggKrUKFCkiRPT08NGTJEbdq0sUta6tSpowkTJuinn37S+++/r0uXLrk4YuC6zHfW3nnnnYqNjVXjxo3Vtm1bkhbc0nhbMwoUwzBksVi0Z88e7d+/XwEBAQoPD1fFihWVnp6uCRMmaMWKFapVq5bGjRunwMBA7dixQ8WLF1d4eLirwwdsrFarPDyu/5tzz549iouL07p16/Tpp5+qbt26Sk5O1oIFC7RgwQKVL19eH3zwgYsjBnKHhAW3vMwk5dq1a/L09NSSJUvUt29fFS9eXFarVaGhoRo2bJiaNWtmS1pWrlypiIgIvfnmmwoICHD1LQA2mT/Pf/f999/r1VdfzZK0vPXWW/rss8/0wQcfqHTp0i6IGHAOEhbcsjL/BZqcnKzAwEBJ0tdff61OnTpp9OjRev755/Xhhx/qmWeeUVhYmF5//XW1atVK6enpGjVqlLZu3aoFCxYoJCTEtTcC/CEzWdm4caPtqcxVqlRR165dJUm7d+/WmDFjtG7dOi1fvlz33nuvLl68KKvVqmLFirkwciD3SFhwS8pMVhITE3X//fdrzZo1qly5svr166dixYppwoQJOnXqlBo0aKAaNWooIyNDBw8e1MyZM3X//ffr2rVrunjxoooXL+7qW0EBlvlzfPnyZfn6+kqSlixZop49e6pRo0by8/PTJ598ogEDBmjUqFGSrict8fHxWrx4sbZs2aLatWu78A4AJzKAW0xGRoZhGIaRmJho+Pr6Gi+++KLt3Pfff29s2LDB+PXXX41atWoZPXr0MAzDMD744APD09PTCA4ONj777DOXxA38VebP8bZt24zy5csbP//8s7F161YjLCzMmDVrlmEYhnHgwAEjICDAsFgsRt++fW3f3bFjh9G1a1dj//79LokdyAuerk6YAGfK/Bfp7t27FRUVpcGDBysuLs52vly5cvL19dWKFSvk7e2tkSNHSpJCQ0PVqFEj1ahRQ5UrV3ZV+ICkP3+Od+3apaZNm+qZZ55RiRIltHz5cnXq1EnPPfecTp48qebNm6tTp06qU6eOnn32WRUrVkyjR49WrVq19NZbb8nLy8vVtwI4DQkLbikeHh46fvy4oqKi1K5dO7tkZfLkyUpJSdGoUaN05coV/fDDDzp9+rTKlCmjzz//XOXKldPIkSNZZAuXykxWvv/+e9WrV0/9+/fX2LFjJUndunXTunXrbL9v2rSp5syZo59++kmhoaEaM2aMrly5otdff51kBbccEhbccgzDULFixZSWlqYNGzaoYcOGmjhxomJjY/XZZ59Jur5QsUGDBnr00UcVERGh7du3KyEhgWQFLufh4aGTJ0+qWbNmat26tS1ZkaRZs2bp2LFjKlOmjM6fP6/Ro0dLkm677TY98MADio6O1j333OOq0IE8xYPjcEuxWq2KiIjQV199pQMHDmjq1Kl67rnnFB8fr88//1z333+/JKl69eoaOnSo+vbtqzp16mjbtm2qXr26i6MHrsvIyFBkZKRSU1O1adMmSVJ8fLxefPFFtWrVSj4+Ptq7d682b96sK1euaOLEidq9e7datmypSpUquTh6IG+wSwi3nMyS+o8//qjHHntMu3fv1sSJEzVw4EBJsj2PBXBnBw8eVL9+/eTl5aXg4GB98skneu+999S8eXNJ0sSJEzV06FBVqFBBFy5c0OrVq1WrVi0XRw3kHRIW3JIyk5bDhw+rffv2ioiI0NChQ9WwYUO789KNH8QFuNqBAwfUp08fbdy4UWPGjNGgQYNs565evao9e/bo5MmTuvvuuxUWFubCSIG8R8IC08t8P0rmu1IyE5G/VloeeeQRhYeHa/jw4WrQoIErwwUccvjwYT3//PMqVKiQXnrpJdvP719/1oGCgJ92mE5mgpKamirpeqJy8OBB2+8zZSYwlStX1kcffaRTp07pxRdfVEJCQv4HDdyk8uXL680335RhGHr11Vdta1pIVlDQ8BMP0/Hw8NCRI0fUv39/nTp1Sh999JGqVKmivXv3Zts3M2lZuHChrFarypQp44KogZtXsWJFTZs2TYULF9bgwYP17bffujokIN8xJQRTWr9+vdq3b68aNWooISFBc+bM0dNPP33D9SgZGRkqVKiQ0tPTVbhwYRdEDOTejz/+qNjYWE2aNElly5Z1dThAviJhgelkJiXjx4/X8OHDdd9992nBggWqUKGC3fl/+i5gVlevXuWhcCiQmBKC6WRkZEiSfHx8NGLECJ09e1ajRo3Szp07JUkWi0V/zcMz17xkngPMjGQFBRUVFphGZnXk789RWbVqlZ599lnVq1dPQ4cOVY0aNSRJCQkJioqKclW4AAAnImGBKWQmK2vWrNHSpUv166+/qmrVqurZs6dKlSqlVatW6bnnnlP9+vXVuXNn7dixQyNHjlRSUpJKlixJZQUATI6EBaaxbNkyPf7443ryySd1/Phx/frrr/r555+1fv16lS1bVmvWrNHgwYNltVqVkpKijz76SLVr13Z12AAAJyBhgVv6++LYX375RQ888ICeeOIJDRkyRJK0Z88eDRo0SAcPHtR3332nEiVK6NixY0pJSVHJkiVVunRpV4UPAHAyFt3CrWTmz1euXJH054LZS5cu6cyZM6pZs6atb5UqVTRhwgQVK1ZM77//viQpIiJCd911F8kKANxiSFjgViwWi86dO6eIiAgtXrzY9jTPkJAQhYWFad26dba+hQoV0l133SVPT0/t37/fVSEDAPIBCQvcjoeHh9q2baunnnpKn3zyia2tbt26Wrt2rZYsWWLra7FYdPvttyswMFCGYYgZTgC4NbGGBS6X3cPczp07p7Fjx2r69On6+OOP9fDDD+v8+fPq0qWLLl68qLp166p+/fpav369FixYoC1btqhy5couugMAQF4jYYFLZb5x9vLly8rIyJC/v7/t3JkzZzRu3DjNmDFDH374oTp27Kjz58/rtdde06ZNm/TLL78oJCRE06ZNs1vbAgC49ZCwwOUOHjyoTp06qWjRourZs6dCQkLUvHlzSVJaWpoGDRqkmTNn6oMPPtCjjz6qa9euyWKx6MKFC7rtttvk6+vr4jsAAOQ1z3/vAuQdq9WqefPmadeuXfLx8VFycrKuXLmioKAg3XvvvXrmmWfUrVs3FS9eXI899pj8/f3VokULSVLJkiVdHD0AIL9QYYHLJSUlafz48Tp8+LAqVKig3r17a+HChdqwYYO+//57BQUFqVy5ctq+fbvOnTunb775Ro0aNXJ12ACAfESFBS4XEhKiIUOGaNy4cdq4caMqVqyoESNGSJK2bNmi06dPa86cOSpVqpTOnTunEiVKuDhiAEB+o8ICt5G5yHbLli1q3769XnrpJdu59PR0Wa1WXbx4UaVKlXJhlAAAVyBhgVtJSkrS2LFjtXXrVrVv314vvviiJGV5QzMAoGAhYYHbyUxadu7cqWbNmmn06NGuDgkA4GI86RZuJyQkRC+//LIqVqyozZs36/z5864OCQDgYlRY4LbOnj0rSQoODnZxJAAAVyNhAQAAbo8pIQAA4PZIWAAAgNsjYQEAAG6PhAUAALg9EhYAAOD2SFgAAIDbI2EBAABuj4QFKGC6du2q9u3b2z43adJE/fv3z/c4vvnmG1ksFiUnJ9+wj8Vi0bJly3I85qhRo1SzZs1cxXXs2DFZLBYlJibmahwAzkXCAriBrl27ymKxyGKxyMvLSxUqVFBcXJyuXbuW59desmSJxowZk6O+OUkyACAv8PpbwE08+OCDmjt3rtLS0vT555+rd+/eKly4sIYPH56l79WrV+Xl5eWU6wYFBTllHADIS1RYADfh7e2tkJAQhYeH6z//+Y+io6P16aefSvpzGmfs2LEKDQ1VpUqVJEknT55Up06dFBgYqKCgILVr107Hjh2zjZmRkaGBAwcqMDBQxYsX19ChQ/X3t3H8fUooLS1Nw4YNU1hYmLy9vVWhQgW9++67OnbsmJo2bSpJKlasmCwWi7p27SpJslqtio+PV2RkpIoUKaIaNWroo48+srvO559/rjvuuENFihRR06ZN7eLMqWHDhumOO+7QbbfdpnLlyik2Nlbp6elZ+r311lsKCwvTbbfdpk6dOunixYt259955x1VqVJFPj4+qly5smbOnOlwLADyFwkL4KaKFCmiq1ev2j6vWbNG+/fv1+rVq7VixQqlp6erRYsW8vPz04YNG7Rp0yYVLVpUDz74oO17kyZN0rx58/R///d/2rhxoy5cuKClS5f+43Wffvpp/e9//9O0adO0b98+vfXWWypatKjCwsL08ccfS5L279+vM2fO6I033pAkxcfHa8GCBZo9e7b27t2rAQMG6Mknn9S6deskXU+sOnTooDZt2igxMVE9evTQiy++6PCfiZ+fn+bNm6cffvhBb7zxht5++21NmTLFrs+hQ4e0ePFiLV++XCtXrtTOnTv1/PPP284vXLhQI0aM0NixY7Vv3z6NGzdOsbGxmj9/vsPxAMhHBgCXi4mJMdq1a2cYhmFYrVZj9erVhre3tzF48GDb+eDgYCMtLc32nffee8+oVKmSYbVabW1paWlGkSJFjC+//NIwDMMoXbq0MWHCBNv59PR0o0yZMrZrGYZhNG7c2HjhhRcMwzCM/fv3G5KM1atXZxvn119/bUgyfv31V1tbamqqcdtttxmbN2+269u9e3fj8ccfNwzDMIYPH25UrVrV7vywYcOyjPV3koylS5fe8Pzrr79u1K5d2/Z55MiRRqFChYyffvrJ1vbFF18YHh4expkzZwzDMIzy5csbixYtshtnzJgxRlRUlGEYhnH06FFDkrFz584bXhdA/mMNC+AmVqxYoaJFiyo9PV1Wq1VPPPGERo0aZTtfvXp1u3Uru3bt0qFDh+Tn52c3Tmpqqg4fPqyLFy/qzJkzqlu3ru2cp6en7rnnnizTQpkSExNVqFAhNW7cOMdxHzp0SFeuXNEDDzxg13716lXVqlVLkrRv3z67OCQpKioqx9fI9MEHH2jatGk6fPiwLl26pGvXrsnf39+uT9myZXX77bfbXcdqtWr//v3y8/PT4cOH1b17d/Xs2dPW59q1awoICHA4HgD5h4QFcBNNmzbVrFmz5OXlpdDQUHl62v/f09fX1+7zpUuXVLt2bS1cuDDLWCVLlrypGIoUKeLwdy5duiRJ+uyzz+wSBen6uhxnSUhIUJcuXTR69Gi1aNFCAQEBev/99zVp0iSHY3377bezJFCFChVyWqwAnI+EBXATvr6+qlChQo7733333frggw9UqlSpLFWGTKVLl9aWLVvUqFEjSdcrCdu3b9fdd9+dbf/q1avLarVq3bp1io6OznI+s8KTkZFha6tataq8vb114sSJG1ZmqlSpYltAnOnbb7/995v8i82bNys8PFwvv/yyre348eNZ+p04cUKnT59WaGio7ToeHh6qVKmSgoODFRoaqiNHjqhLly4OXR+Aa7HoFjCpLl26qESJEmrXrp02bNigo0eP6ptvvlG/fv30008/SZJeeOEFvfbaa1q2bJl+/PFHPf/88//4DJWIiAjFxMTomWee0bJly2xjLl68WJIUHh4ui8WiFStW6Oeff9alS5fk5+enwYMHa8CAAZo/f74OHz6sHTt2aPr06baFrM8995wOHjyoIUOGaP/+/Vq0aJHmzZvn0P1WrFhRJ06c0Pvvv6/Dhw9r2rRp2S4g9vHxUUxMjHbt2qUNGzaoX79+6tSpk0JCQiRJo0ePVnx8vKZNm6YDBw5o9+7dmjt3riZPnuxQPADyFwkLYFK33Xab1q9fr7Jly6pDhw6qUqWKunfvrtTUVFvFZdCgQXrqqacUExOjqKgo+fn56eGHH/7HcWfNmqVHHnlEzz//vCpXrqyePXvq8uXLkqTbb79do0eP1osvvqjg4GD16dNHkjRmzBjFxsYqPj5eVapU0YMPPqjPPvtMkZGRkq6vK/n444+1bNky1ahRQ7Nnz9a4ceMcut+2bdtqwIAB6tOnj2rWrKnNmzcrNjY2S78KFSqoQ4cOeuihh9S8eXPddddddtuWe/TooXfeeUdz585V9erV1bhxY82bN88WKwD3ZDFutPoOAADATVBhAQAAbo+EBQAAuD0SFgAA4PZIWAAAgNsjYQEAAG6PhAUAALg9EhYAAOD2SFgAAIDbI2EBAABuj4QFAAC4PRIWAADg9v4f2c0aLNiMqKYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 6)             1         ['input_4[0][0]']             \n",
      " e_3 (PruneLowMagnitude)                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            2370      ['prune_low_magnitude_reshape_\n",
      " _81 (PruneLowMagnitude)                                            3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 46, 64)            24642     ['prune_low_magnitude_conv2d_8\n",
      " _82 (PruneLowMagnitude)                                            1[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 23, 64)            1         ['prune_low_magnitude_conv2d_8\n",
      " oling2d_3 (PruneLowMagnitu                                         2[0][0]']                     \n",
      " de)                                                                                              \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _84 (PruneLowMagnitude)                                            ing2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_66 (PruneLow                                         4[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 66 (PruneLowMagnitude)                                             rmalization_66[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_66\n",
      " _85 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_67 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 67 (PruneLowMagnitude)                                             rmalization_67[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_67\n",
      " _86 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_8\n",
      " normalization_68 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _83 (PruneLowMagnitude)                                            ing2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_21  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_68[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_8\n",
      "                                                                    3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_21[0\n",
      " 68 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_68\n",
      " _88 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_69 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 69 (PruneLowMagnitude)                                             rmalization_69[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_69\n",
      " _89 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_70 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 70 (PruneLowMagnitude)                                             rmalization_70[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_70\n",
      " _90 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_71 (PruneLow                                         0[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_68\n",
      " _87 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_22  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_71[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_8\n",
      "                                                                    7[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_22[0\n",
      " 71 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_71\n",
      " _91 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_72 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 72 (PruneLowMagnitude)                                             rmalization_72[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_72\n",
      " _92 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_73 (PruneLow                                         2[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 73 (PruneLowMagnitude)                                             rmalization_73[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_73\n",
      " _93 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_74 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_23  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_74[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_71\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_23[0\n",
      " 74 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_74\n",
      " _95 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_75 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 75 (PruneLowMagnitude)                                             rmalization_75[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_75\n",
      " _96 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_76 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 76 (PruneLowMagnitude)                                             rmalization_76[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_76\n",
      " _97 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_77 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_74\n",
      " _94 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_24  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_77[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_9\n",
      "                                                                    4[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_24[0\n",
      " 77 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_77\n",
      " _98 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_78 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 78 (PruneLowMagnitude)                                             rmalization_78[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_78\n",
      " _99 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_79 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 79 (PruneLowMagnitude)                                             rmalization_79[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_79\n",
      " _100 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_80 (PruneLow                                         00[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_25  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_80[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_77\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_25[0\n",
      " 80 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_80\n",
      " _102 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_81 (PruneLow                                         02[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 81 (PruneLowMagnitude)                                             rmalization_81[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_81\n",
      " _103 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_82 (PruneLow                                         03[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 82 (PruneLowMagnitude)                                             rmalization_82[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_82\n",
      " _104 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_83 (PruneLow                                         04[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_80\n",
      " _101 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_26  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_83[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    01[0][0]']                    \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_26[0\n",
      " 83 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_83\n",
      " _105 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_84 (PruneLow                                         05[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 84 (PruneLowMagnitude)                                             rmalization_84[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_84\n",
      " _106 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_85 (PruneLow                                         06[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 85 (PruneLowMagnitude)                                             rmalization_85[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_85\n",
      " _107 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_86 (PruneLow                                         07[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_27  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_86[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_83\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_27[0\n",
      " 86 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 11, 64)            1         ['prune_low_magnitude_re_lu_86\n",
      " e_pooling2d_3 (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 704)                  1         ['prune_low_magnitude_average_\n",
      " n_4 (PruneLowMagnitude)                                            pooling2d_3[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense_  (None, 2)                    2820      ['prune_low_magnitude_flatten_\n",
      " 5 (PruneLowMagnitude)                                              4[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 105743 (413.38 KB)\n",
      "Trainable params: 53346 (208.38 KB)\n",
      "Non-trainable params: 52397 (204.99 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "ups = pruning_callbacks.UpdatePruningStep()\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()\n",
    "\n",
    "checkpoint_prune_path = './checkpoints/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "# Define the checkpoint\n",
    "checkpoint_prune = ModelCheckpoint(checkpoint_prune_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.3376 - accuracy: 0.9511\n",
      "Epoch 1: val_loss improved from inf to 0.40768, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 43s 32ms/step - loss: 0.3376 - accuracy: 0.9511 - val_loss: 0.4077 - val_accuracy: 0.9059 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9616\n",
      "Epoch 2: val_loss improved from 0.40768 to 0.15341, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 20s 27ms/step - loss: 0.1797 - accuracy: 0.9616 - val_loss: 0.1534 - val_accuracy: 0.9511 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9658\n",
      "Epoch 3: val_loss improved from 0.15341 to 0.14931, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 32s 42ms/step - loss: 0.1628 - accuracy: 0.9658 - val_loss: 0.1493 - val_accuracy: 0.9500 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9749\n",
      "Epoch 4: val_loss improved from 0.14931 to 0.11797, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 21s 28ms/step - loss: 0.1052 - accuracy: 0.9749 - val_loss: 0.1180 - val_accuracy: 0.9634 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9759\n",
      "Epoch 5: val_loss improved from 0.11797 to 0.06816, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 20s 26ms/step - loss: 0.1185 - accuracy: 0.9759 - val_loss: 0.0682 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9809\n",
      "Epoch 6: val_loss did not improve from 0.06816\n",
      "760/760 [==============================] - 19s 25ms/step - loss: 0.0842 - accuracy: 0.9809 - val_loss: 0.0979 - val_accuracy: 0.9723 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9801\n",
      "Epoch 7: val_loss did not improve from 0.06816\n",
      "760/760 [==============================] - 17s 23ms/step - loss: 0.0904 - accuracy: 0.9801 - val_loss: 0.9549 - val_accuracy: 0.7522 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9765\n",
      "Epoch 8: val_loss improved from 0.06816 to 0.05818, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 18s 23ms/step - loss: 0.1108 - accuracy: 0.9765 - val_loss: 0.0582 - val_accuracy: 0.9835 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9789\n",
      "Epoch 9: val_loss improved from 0.05818 to 0.04912, saving model to ./checkpoints/ResNet24_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.1075 - accuracy: 0.9789 - val_loss: 0.0491 - val_accuracy: 0.9846 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9814\n",
      "Epoch 10: val_loss did not improve from 0.04912\n",
      "760/760 [==============================] - 135s 178ms/step - loss: 0.0882 - accuracy: 0.9814 - val_loss: 0.0596 - val_accuracy: 0.9848 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9789\n",
      "Epoch 11: val_loss did not improve from 0.04912\n",
      "760/760 [==============================] - 97s 128ms/step - loss: 0.1110 - accuracy: 0.9789 - val_loss: 0.0992 - val_accuracy: 0.9765 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9753\n",
      "Epoch 12: val_loss did not improve from 0.04912\n",
      "760/760 [==============================] - 18s 24ms/step - loss: 0.1055 - accuracy: 0.9753 - val_loss: 0.1202 - val_accuracy: 0.9702 - lr: 5.0000e-04\n",
      "Epoch 13/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1486 - accuracy: 0.9705\n",
      "Epoch 13: val_loss did not improve from 0.04912\n",
      "760/760 [==============================] - 18s 24ms/step - loss: 0.1487 - accuracy: 0.9705 - val_loss: 0.1029 - val_accuracy: 0.9714 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9793\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.04912\n",
      "760/760 [==============================] - 18s 23ms/step - loss: 0.0970 - accuracy: 0.9793 - val_loss: 0.0525 - val_accuracy: 0.9849 - lr: 5.0000e-04\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3025f1880>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups, checkpoint_prune],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.08181576430797577\n",
      "Pruned model accuracy:  0.9657142758369446\n",
      "Full-precision model accuracy:  0.9885714285714285\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "pruned_model_unstructured.load_weights(checkpoint_prune_path)\n",
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpbpk6gahi/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpbpk6gahi/assets\n",
      "2024-01-04 16:00:00.347615: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-04 16:00:00.347753: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-04 16:00:00.349109: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpbpk6gahi\n",
      "2024-01-04 16:00:00.358929: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-04 16:00:00.358942: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpbpk6gahi\n",
      "2024-01-04 16:00:00.381967: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-04 16:00:00.547456: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpbpk6gahi\n",
      "2024-01-04 16:00:00.608439: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 259330 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "pruned_model_unstructured.save('./saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))\n",
    "\n",
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  126870\n",
      "Size of the full-precision model:  202360\n",
      "The achieved compression ratio is 1.60x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "unstructured_pruned_model_path = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(unstructured_pruned_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(unstructured_pruned_model_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_3 (Quantize  (None, 50, 6)                3         ['input_4[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_3 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_3[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_81 (QuantizeW  (None, 1, 48, 64)            1347      ['quant_reshape_3[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_82 (QuantizeW  (None, 1, 46, 64)            12483     ['quant_conv2d_81[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_3 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_82[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_84 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_84[0][0]']     \n",
      " 66 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_66 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_66\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_85 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_66[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_85[0][0]']     \n",
      " 67 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_67 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_67\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_86 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_67[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_86[0][0]']     \n",
      " 68 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_83 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_21 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_68\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_83[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_68 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_21[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_88 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_88[0][0]']     \n",
      " 69 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_69 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_69\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_89 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_69[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_89[0][0]']     \n",
      " 70 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_70 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_70\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_90 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_70[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_90[0][0]']     \n",
      " 71 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_87 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_22 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_71\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_87[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_71 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_22[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_91 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_91[0][0]']     \n",
      " 72 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_72 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_72\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_92 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_72[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_92[0][0]']     \n",
      " 73 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_73 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_73\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_93 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_73[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_93[0][0]']     \n",
      " 74 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_23 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_74\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_74 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_23[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_95 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_95[0][0]']     \n",
      " 75 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_75 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_75\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_96 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_75[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_96[0][0]']     \n",
      " 76 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_76 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_76\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_97 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_76[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_97[0][0]']     \n",
      " 77 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_94 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_24 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_77\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_94[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_77 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_24[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_98 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_77[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_98[0][0]']     \n",
      " 78 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_78 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_78\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_99 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_78[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_99[0][0]']     \n",
      " 79 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_79 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_79\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_100 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_79[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_100[0][0]']    \n",
      " 80 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_25 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_80\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_77[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_80 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_25[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_102 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_102[0][0]']    \n",
      " 81 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_81 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_81\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_103 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_81[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_103[0][0]']    \n",
      " 82 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_82 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_82\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_104 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_82[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_104[0][0]']    \n",
      " 83 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_101 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_26 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_83\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_101[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_83 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_26[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_105 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_83[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_105[0][0]']    \n",
      " 84 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_84 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_84\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_106 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_84[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_106[0][0]']    \n",
      " 85 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_85 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_85\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_107 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_85[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_107[0][0]']    \n",
      " 86 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_27 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_86\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_83[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_86 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_27[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_3   (None, 1, 11, 64)            3         ['quant_re_lu_86[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_4 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_3[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_5 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_4[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56960 (222.50 KB)\n",
      "Trainable params: 53346 (208.38 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()\n",
    "\n",
    "checkpoint_pqat_path = './checkpoints/'+model_name+'_pqat'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "\n",
    "checkpoint_pqat = ModelCheckpoint(checkpoint_pqat_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (48591, 50, 6)\n",
      "y_train.shape:  (48591, 2)\n",
      "Epoch 1/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.6993 - accuracy: 0.9071\n",
      "Epoch 1: val_loss improved from inf to 0.10706, saving model to ./checkpoints/ResNet24_6axis_pqat_Rescaled.keras\n",
      "760/760 [==============================] - 25s 28ms/step - loss: 0.6982 - accuracy: 0.9072 - val_loss: 0.1071 - val_accuracy: 0.9624 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9681\n",
      "Epoch 2: val_loss improved from 0.10706 to 0.07142, saving model to ./checkpoints/ResNet24_6axis_pqat_Rescaled.keras\n",
      "760/760 [==============================] - 23s 30ms/step - loss: 0.1415 - accuracy: 0.9681 - val_loss: 0.0714 - val_accuracy: 0.9760 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9760\n",
      "Epoch 3: val_loss improved from 0.07142 to 0.06062, saving model to ./checkpoints/ResNet24_6axis_pqat_Rescaled.keras\n",
      "760/760 [==============================] - 23s 31ms/step - loss: 0.1129 - accuracy: 0.9760 - val_loss: 0.0606 - val_accuracy: 0.9810 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9879\n",
      "Epoch 4: val_loss did not improve from 0.06062\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.0562 - accuracy: 0.9878 - val_loss: 0.0790 - val_accuracy: 0.9753 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9895\n",
      "Epoch 5: val_loss improved from 0.06062 to 0.03142, saving model to ./checkpoints/ResNet24_6axis_pqat_Rescaled.keras\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.0480 - accuracy: 0.9895 - val_loss: 0.0314 - val_accuracy: 0.9917 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9873\n",
      "Epoch 6: val_loss did not improve from 0.03142\n",
      "760/760 [==============================] - 23s 30ms/step - loss: 0.0671 - accuracy: 0.9872 - val_loss: 0.1139 - val_accuracy: 0.9692 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9830\n",
      "Epoch 7: val_loss did not improve from 0.03142\n",
      "760/760 [==============================] - 22s 30ms/step - loss: 0.0918 - accuracy: 0.9829 - val_loss: 0.1365 - val_accuracy: 0.9738 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9797\n",
      "Epoch 8: val_loss did not improve from 0.03142\n",
      "760/760 [==============================] - 23s 30ms/step - loss: 0.1294 - accuracy: 0.9797 - val_loss: 0.0779 - val_accuracy: 0.9810 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9829\n",
      "Epoch 9: val_loss did not improve from 0.03142\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.0883 - accuracy: 0.9829 - val_loss: 0.0438 - val_accuracy: 0.9886 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9859\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.03142\n",
      "760/760 [==============================] - 22s 29ms/step - loss: 0.0739 - accuracy: 0.9859 - val_loss: 0.0347 - val_accuracy: 0.9902 - lr: 5.0000e-04\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d2980bb0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "    \n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups, checkpoint_pqat],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmph3y2jgez/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmph3y2jgez/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-01-04 16:04:05.026195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-01-04 16:04:05.026393: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-01-04 16:04:05.027930: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmph3y2jgez\n",
      "2024-01-04 16:04:05.051478: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-01-04 16:04:05.051495: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmph3y2jgez\n",
      "2024-01-04 16:04:05.112556: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-01-04 16:04:05.651785: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmph3y2jgez\n",
      "2024-01-04 16:04:05.847413: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 819485 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108264"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "pruned_qat_model.load_weights(checkpoint_pqat_path)\n",
    "\n",
    "pruned_qat_model.save('./saved_models/'+model_name+'_pqat'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_dynR.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_pqat_FullInt'+('_Rescaled' if train_with_int else '') +'.tflite', \"wb\").write(pruned_qat_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  ResNet24_6axis\n",
      "input:  {'name': 'serving_default_input_4:0', 'index': 0, 'shape': array([ 1, 50,  6], dtype=int32), 'shape_signature': array([-1, 50,  6], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 106, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[172   1]\n",
      " [  4 173]]\n",
      "accuracy:  0.9857142857142858\n",
      "Confusion matrix, without normalization\n",
      "[[172   1]\n",
      " [  4 173]]\n",
      "f1_score:  0.9857549857549858\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# accuracy\n",
    "accuracy = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned QAT model:  33530\n",
      "Size of the QAT model:  42553\n",
      "Size of the full-precision model:  120198\n",
      "The achieved compression ratio is 3.58x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "pqat_model_path = './saved_models/'+model_name+'_pqat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "qat_model_path = './saved_models/'+model_name+'_qat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name +('_Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size(pqat_model_path))\n",
    "print('Size of the QAT model: ', get_gzipped_model_size(qat_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(pqat_model_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import plot_confusion_matrix, plot_confusion_matrix, get_gzipped_model_size, rescale_data\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from models.ConvLSTM import ConvLSTM\n",
    "from models.ConvLSTM_VGG import ConvLSTM_VGG\n",
    "from models.TinyFallNet import TinyFallNet\n",
    "from models.ResNet24 import ResNet24\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "\n",
    "data_path = config['data_path_win']\n",
    "# data_path = config['data_path_linux']\n",
    "# data_path = config['data_path_mac']\n",
    "sensor_data_folder = os.path.join(data_path, 'sensor_data')\n",
    "label_data_folder = os.path.join(data_path, 'label_data')\n",
    "\n",
    "# data mode. Combination of sensor data.\n",
    "# data_mode = 'ACC+GYRO' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "window_size = config['window_size'] # window size\n",
    "fall_threshold = config['fall_threshold'] # threshold for windows labeled as fall\n",
    "num_window_fall_data = config['num_window_fall_data']   # number of windows labeled as fall\n",
    "num_window_not_fall_data = config['num_window_not_fall_data']    # number of windows labeled as not fall\n",
    "acc_max = config['acc_max'] \n",
    "gyro_max = config['gyro_max'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patience:  1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TinyFallNet_6axis\" # \"ConvLSTM\" or \"ConvLSTM_VGG\" or \"TinyFallNet\" or \"ResNet24\" or \"TinyFallNet_6axis\"\n",
    "# when train_with_int is True, scaled data will be used for training, generate full integer quantized model(int8 input, int8 output)\n",
    "# when train_with_int is False, original data will be used for training, generate three models: dynamic range quantized model(float32 input, float32 output)\n",
    "#                                                                                               full integer quantized model(int8 input, int8 output)\n",
    "#                                                                                               full integer quantized model(float32 input, int8 output)\n",
    "train_with_int = True\n",
    "# use_float_input = True\n",
    "load_from_checkpoint = config['load_from_checkpoint']\n",
    "\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = models.load_model('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "else:\n",
    "    if model_name == \"ConvLSTM\":\n",
    "        model = ConvLSTM()\n",
    "        data_mode = 'ACC+GYRO+MAG' # 'ACC' or 'ACC+GYRO' or 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ConvLSTM_6axis\":\n",
    "        model = ConvLSTM(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    elif model_name == \"ConvLSTM_VGG\":\n",
    "        model = ConvLSTM_VGG()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet\":\n",
    "        model = TinyFallNet()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"ResNet24\":\n",
    "        model = ResNet24()\n",
    "        data_mode = 'ACC+GYRO+MAG'\n",
    "    elif model_name == \"TinyFallNet_6axis\":\n",
    "        model = TinyFallNet(6)\n",
    "        data_mode = 'ACC+GYRO'\n",
    "    else:\n",
    "        print(\"Please select a valid model name\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = config['learning_rate']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "lr_factor = config['lr_factor']\n",
    "patience = config['patience']\n",
    "print('patience: ', patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n",
      "Data shape:  (75940, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            fall_threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data,\n",
    "                            data_mode)\n",
    "\n",
    "print(\"Data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the rescaling is done only once\n",
    "if train_with_int==True and data.dtype!=np.int8:\n",
    "    dtype_out = np.int8 # rescaled input data type\n",
    "    data = rescale_data(data, dtype_out, acc_max=acc_max, gyro_max=gyro_max)\n",
    "else:\n",
    "    data = data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (75940, 50, 6)\n",
      "Data dtype:  int8\n",
      "in_channels:  6\n",
      "not_fall_size:  75060\n",
      "fall_size:  880\n"
     ]
    }
   ],
   "source": [
    "in_channels = data.shape[2]\n",
    "print(\"Data shape: \", data.shape)\n",
    "print(\"Data dtype: \", data.dtype)\n",
    "print('in_channels: ', in_channels)\n",
    "\n",
    "label = label.astype(np.int64)\n",
    "data_copy = data.reshape(data.shape[0], 50, in_channels)\n",
    "\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('not_fall_size: ', B_size)\t\n",
    "print('fall_size: ', A_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_copy, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the saved_data folder if it does not exist\n",
    "if not os.path.exists('./saved_data'):\n",
    "    os.makedirs('./saved_data')\n",
    "# save the test data, train data and validation data\n",
    "np.save('./saved_data/X_test.npy', X_test)\n",
    "np.save('./saved_data/y_test.npy', y_test)\n",
    "np.save('./saved_data/X_train.npy', X_train)\n",
    "np.save('./saved_data/y_train.npy', y_train)\n",
    "np.save('./saved_data/X_val.npy', X_val)\n",
    "np.save('./saved_data/y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 50, 6)             0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 1, 48, 64)            1216      ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 24, 64)            0         ['conv2d_17[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 1, 24, 16)            1040      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 1, 24, 16)            64        ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 1, 24, 16)            64        ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 1, 24, 64)            256       ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 1, 24, 64)            4160      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_14[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_18[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 1, 24, 64)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 1, 24, 16)            64        ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 1, 24, 16)            64        ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 1, 24, 64)            256       ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_22[0][0]']           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " re_lu_17 (ReLU)             (None, 1, 24, 64)            0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 1, 24, 16)            64        ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1, 24, 16)            64        ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_19[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 1, 24, 64)            256       ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_26[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)             (None, 1, 24, 64)            0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 1, 24, 16)            1040      ['re_lu_20[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 1, 24, 16)            64        ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 1, 24, 16)            784       ['re_lu_21[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 1, 24, 16)            64        ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)             (None, 1, 24, 16)            0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 1, 24, 64)            1088      ['re_lu_22[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 1, 24, 64)            256       ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 1, 24, 64)            4160      ['re_lu_20[0][0]']            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 1, 24, 64)            0         ['batch_normalization_23[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_30[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)             (None, 1, 24, 64)            0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (Avera  (None, 1, 12, 64)            0         ['re_lu_23[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 768)                  0         ['average_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2)                    1538      ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32578 (127.26 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "            loss='categorical_crossentropy',\n",
    "            #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 50, 9))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48601, 2)\n",
      "y_val.shape:  (12151, 2)\n",
      "X_train.shape:  (48601, 50, 6)\n",
      "y_train.shape:  (48601, 2)\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint_path = './checkpoints/'+model_name+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_factor, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.5921 - accuracy: 0.8958\n",
      "Epoch 1: val_loss improved from inf to 0.36536, saving model to ./checkpoints\\TinyFallNet_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 16s 15ms/step - loss: 0.5921 - accuracy: 0.8958 - val_loss: 0.3654 - val_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "757/760 [============================>.] - ETA: 0s - loss: 0.3403 - accuracy: 0.9331\n",
      "Epoch 2: val_loss improved from 0.36536 to 0.20920, saving model to ./checkpoints\\TinyFallNet_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 11s 14ms/step - loss: 0.3400 - accuracy: 0.9331 - val_loss: 0.2092 - val_accuracy: 0.9266 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "757/760 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9389\n",
      "Epoch 3: val_loss improved from 0.20920 to 0.10328, saving model to ./checkpoints\\TinyFallNet_6axis_Rescaled.keras\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.2978 - accuracy: 0.9390 - val_loss: 0.1033 - val_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9479\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.10328\n",
      "760/760 [==============================] - 10s 13ms/step - loss: 0.2556 - accuracy: 0.9479 - val_loss: 0.1691 - val_accuracy: 0.9471 - lr: 5.0000e-04\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs, checkpoint],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (350, 50, 6)\n",
      "11/11 - 0s - loss: 0.1165 - accuracy: 0.9514 - 73ms/epoch - 7ms/step\n",
      "Test loss: [0.11648522317409515, 0.9514285922050476]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step\n",
      "[[167   7]\n",
      " [ 10 166]]\n",
      "Confusion matrix, without normalization\n",
      "[[167   7]\n",
      " [ 10 166]]\n",
      "accuracy:  0.9514285714285714\n",
      "f1_score:  0.9512893982808023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "# f1 score\n",
    "precision_fp = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall_fp = cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "f1_score_fp = 2 * precision_fp * recall_fp / (precision_fp + recall_fp)\n",
    "print('f1_score: ', f1_score_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpbyzthrx8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpbyzthrx8\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139152"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if \"LSTM\" in model_name:\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "model_tflite = converter.convert()\n",
    "# save the model\n",
    "open('./saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(model_tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ConvLSTM model\n",
    "if \"LSTM\" in model_name:\n",
    "    def representative_data_gen():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_q_model = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    print('input: ', input_type)\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "    print('output: ', output_type)\n",
    "    # Save the quantized model to disk\n",
    "    open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_3 (Quantize  (None, 50, 6)                3         ['input_2[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_1 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_3[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 48, 64)            1347      ['quant_reshape_1[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_1 (Qua  (None, 1, 24, 64)            1         ['quant_conv2d_17[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_19[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_20[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_21[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_18[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_23[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_25[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_22[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_27 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_27[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_28 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_28[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_29 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_29[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_26[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_31 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_20[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_31[0][0]']     \n",
      " 21 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_21 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_21\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_32 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_21[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_32[0][0]']     \n",
      " 22 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_22 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_22\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_33 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_22[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_33[0][0]']     \n",
      " 23 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_30 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_20[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_7 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_23\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_30[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_23 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_7[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_1   (None, 1, 12, 64)            3         ['quant_re_lu_23[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_1 (QuantizeW  (None, 768)                  1         ['quant_average_pooling2d_1[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_1 (QuantizeWra  (None, 2)                    1543      ['quant_flatten_1[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34087 (133.15 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 2277 (8.89 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model = tfmot.quantization.keras.quantize_model(model)\n",
    "q_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmp7n97tq1c\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmp7n97tq1c\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68848"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_q'+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_q_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_q_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (48601, 2)\n",
      "y_val.shape:  (12151, 2)\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "# Define the checkpoint\n",
    "checkpoint_qat_path = './checkpoints/'+model_name+'_qat'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "checkpoint_qat = ModelCheckpoint(checkpoint_qat_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "757/760 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9332\n",
      "Epoch 1: val_loss improved from inf to 0.15400, saving model to ./checkpoints\\TinyFallNet_6axis_qat_Rescaled.keras\n",
      "760/760 [==============================] - 23s 22ms/step - loss: 0.3324 - accuracy: 0.9331 - val_loss: 0.1540 - val_accuracy: 0.9533 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "760/760 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9539\n",
      "Epoch 2: val_loss improved from 0.15400 to 0.12280, saving model to ./checkpoints\\TinyFallNet_6axis_qat_Rescaled.keras\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.2264 - accuracy: 0.9539 - val_loss: 0.1228 - val_accuracy: 0.9666 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.2479 - accuracy: 0.9513\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.12280\n",
      "760/760 [==============================] - 18s 24ms/step - loss: 0.2479 - accuracy: 0.9513 - val_loss: 0.1789 - val_accuracy: 0.9511 - lr: 5.0000e-04\n",
      "Epoch 3: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, checkpoint_qat],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpzfl32gwa\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpzfl32gwa\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69072"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.load_weights(checkpoint_qat_path)\n",
    "\n",
    "q_model.save('./saved_models/'+model_name+'_qat'+('_Rescaled' if train_with_int else '')+'.keras')\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_dynR.tflite', \"wb\").write(tflite_q_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  tflite_q_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite', \"wb\").write(tflite_q_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "tflite_q_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_qat_FullInt'+('_Rescaled' if train_with_int else '')+'.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis\n",
      "input:  {'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([ 1, 50,  6]), 'shape_signature': array([-1, 50,  6]), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 73, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[165   9]\n",
      " [ 13 163]]\n",
      "Confusion matrix, without normalization\n",
      "[[165   9]\n",
      " [ 13 163]]\n",
      "f1_score:  0.9367816091954023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_qat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "#accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "#print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 6)             1         ['input_2[0][0]']             \n",
      " e_1 (PruneLowMagnitude)                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            2370      ['prune_low_magnitude_reshape_\n",
      " _17 (PruneLowMagnitude)                                            1[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 24, 64)            1         ['prune_low_magnitude_conv2d_1\n",
      " oling2d_1 (PruneLowMagnitu                                         7[0][0]']                     \n",
      " de)                                                                                              \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _19 (PruneLowMagnitude)                                            ing2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_12 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 12 (PruneLowMagnitude)                                             rmalization_12[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_12\n",
      " _20 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_13 (PruneLow                                         0[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 13 (PruneLowMagnitude)                                             rmalization_13[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_13\n",
      " _21 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_14 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _18 (PruneLowMagnitude)                                            ing2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_4   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_14[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    8[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_4[0]\n",
      " 14 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_14\n",
      " _23 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_15 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 15 (PruneLowMagnitude)                                             rmalization_15[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_15\n",
      " _24 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_16 (PruneLow                                         4[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 16 (PruneLowMagnitude)                                             rmalization_16[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_16\n",
      " _25 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_17 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_14\n",
      " _22 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_5   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_17[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    2[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_5[0]\n",
      " 17 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_17\n",
      " _27 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_18 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 18 (PruneLowMagnitude)                                             rmalization_18[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_18\n",
      " _28 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_19 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 19 (PruneLowMagnitude)                                             rmalization_19[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_19\n",
      " _29 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_20 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_17\n",
      " _26 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_6   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_20[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    6[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_6[0]\n",
      " 20 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            2066      ['prune_low_magnitude_re_lu_20\n",
      " _31 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_3\n",
      " normalization_21 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 21 (PruneLowMagnitude)                                             rmalization_21[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 16)            1554      ['prune_low_magnitude_re_lu_21\n",
      " _32 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 16)            65        ['prune_low_magnitude_conv2d_3\n",
      " normalization_22 (PruneLow                                         2[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 22 (PruneLowMagnitude)                                             rmalization_22[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            2114      ['prune_low_magnitude_re_lu_22\n",
      " _33 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 24, 64)            257       ['prune_low_magnitude_conv2d_3\n",
      " normalization_23 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 24, 64)            8258      ['prune_low_magnitude_re_lu_20\n",
      " _30 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_7   (None, 1, 24, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_23[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_3\n",
      "                                                                    0[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 24, 64)            1         ['prune_low_magnitude_add_7[0]\n",
      " 23 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 12, 64)            1         ['prune_low_magnitude_re_lu_23\n",
      " e_pooling2d_1 (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 768)                  1         ['prune_low_magnitude_average_\n",
      " n_1 (PruneLowMagnitude)                                            pooling2d_1[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense_  (None, 2)                    3076      ['prune_low_magnitude_flatten_\n",
      " 1 (PruneLowMagnitude)                                              1[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62982 (246.22 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 31172 (121.96 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "ups = pruning_callbacks.UpdatePruningStep()\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()\n",
    "\n",
    "checkpoint_prune_path = './checkpoints/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "# Define the checkpoint\n",
    "checkpoint_prune = ModelCheckpoint(checkpoint_prune_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "759/760 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.9412\n",
      "Epoch 1: val_loss improved from inf to 0.19317, saving model to ./checkpoints\\TinyFallNet_6axis_pruned_unstructured_Rescaled.keras\n",
      "760/760 [==============================] - 23s 16ms/step - loss: 0.3535 - accuracy: 0.9412 - val_loss: 0.1932 - val_accuracy: 0.9621 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "757/760 [============================>.] - ETA: 0s - loss: 0.2752 - accuracy: 0.9476\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.19317\n",
      "760/760 [==============================] - 13s 17ms/step - loss: 0.2747 - accuracy: 0.9476 - val_loss: 0.1960 - val_accuracy: 0.9444 - lr: 5.0000e-04\n",
      "Epoch 2: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eff4ed6410>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups, checkpoint_prune],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.30540844798088074\n",
      "Pruned model accuracy:  0.8942857384681702\n",
      "Full-precision model accuracy:  0.9514285714285714\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "pruned_model_unstructured.load_weights(checkpoint_prune_path)\n",
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmput8dccyv\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmput8dccyv\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "pruned_model_unstructured.save('./saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))\n",
    "\n",
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  120140\n",
      "Size of the full-precision model:  120074\n",
      "The achieved compression ratio is 1.00x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "unstructured_pruned_model_path = './saved_models/'+model_name+'_pruned_unstructured'+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name+('_Rescaled' if train_with_int else '')+'.tflite'\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(unstructured_pruned_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(unstructured_pruned_model_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TinyFallNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_4 (Quantize  (None, 50, 6)                3         ['input_2[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_1 (QuantizeW  (None, 1, 50, 6)             1         ['quantize_layer_4[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 48, 64)            1347      ['quant_reshape_1[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_1 (Qua  (None, 1, 24, 64)            1         ['quant_conv2d_17[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_19[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_20[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_21[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_18[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_23[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_25[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_22[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_27 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_27[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_28 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_28[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_29 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_29[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_26[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_31 (QuantizeW  (None, 1, 24, 16)            1073      ['quant_re_lu_20[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_31[0][0]']     \n",
      " 21 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_21 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_21\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_32 (QuantizeW  (None, 1, 24, 16)            817       ['quant_re_lu_21[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 16)            65        ['quant_conv2d_32[0][0]']     \n",
      " 22 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_22 (QuantizeWr  (None, 1, 24, 16)            3         ['quant_batch_normalization_22\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_33 (QuantizeW  (None, 1, 24, 64)            1217      ['quant_re_lu_22[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 24, 64)            259       ['quant_conv2d_33[0][0]']     \n",
      " 23 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_30 (QuantizeW  (None, 1, 24, 64)            4291      ['quant_re_lu_20[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_7 (QuantizeWrapp  (None, 1, 24, 64)            1         ['quant_batch_normalization_23\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_30[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_23 (QuantizeWr  (None, 1, 24, 64)            3         ['quant_add_7[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_1   (None, 1, 12, 64)            3         ['quant_re_lu_23[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_1 (QuantizeW  (None, 768)                  1         ['quant_average_pooling2d_1[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_1 (QuantizeWra  (None, 2)                    1543      ['quant_flatten_1[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34087 (133.15 KB)\n",
      "Trainable params: 31810 (124.26 KB)\n",
      "Non-trainable params: 2277 (8.89 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()\n",
    "\n",
    "checkpoint_pqat_path = './checkpoints/'+model_name+'_pqat'+('_Rescaled' if train_with_int else '')+'.keras'\n",
    "\n",
    "checkpoint_pqat = ModelCheckpoint(checkpoint_pqat_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (48601, 50, 6)\n",
      "y_train.shape:  (48601, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.9224\n",
      "Epoch 1: val_loss improved from inf to 0.11166, saving model to ./checkpoints\\TinyFallNet_6axis_pqat_Rescaled.keras\n",
      "760/760 [==============================] - 24s 23ms/step - loss: 0.4028 - accuracy: 0.9225 - val_loss: 0.1117 - val_accuracy: 0.9654 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "758/760 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9559\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.11166\n",
      "760/760 [==============================] - 16s 21ms/step - loss: 0.2250 - accuracy: 0.9559 - val_loss: 0.1513 - val_accuracy: 0.9587 - lr: 5.0000e-04\n",
      "Epoch 2: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1efff85d7e0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "if train_with_int:\n",
    "    assert X_train.dtype == np.int8\n",
    "    \n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups, checkpoint_pqat],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpjy9eubyz\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\10744\\AppData\\Local\\Temp\\tmpjy9eubyz\\assets\n",
      "g:\\python\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69072"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "pruned_qat_model.load_weights(checkpoint_pqat_path)\n",
    "\n",
    "pruned_qat_model.save('./saved_models/'+model_name+'_pqat'+('_Rescaled' if train_with_int else '')+'.keras')  # The file needs to end with the .keras extension\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "if not train_with_int:\n",
    "  # Dynamic Range Quantization\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_dynR.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "  # Full Integer Quantization(float input)\n",
    "  converter.representative_dataset = representative_data_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.float32\n",
    "  converter.inference_output_type = tf.int8\n",
    "  pruned_qat_tflite_model = converter.convert()\n",
    "  open('./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "\n",
    "# Full Integer Quantization(int input)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8 # Convert output to int8\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "open('./saved_models/'+model_name+'_pqat_FullInt'+('_Rescaled' if train_with_int else '') +'.tflite', \"wb\").write(pruned_qat_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name:  TinyFallNet_6axis\n",
      "input:  {'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([ 1, 50,  6]), 'shape_signature': array([-1, 50,  6]), 'dtype': <class 'numpy.int8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 73, 'shape': array([1, 2]), 'shape_signature': array([-1,  2]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "Evaluated on  300 .\n",
      "[[165   9]\n",
      " [ 11 165]]\n",
      "Confusion matrix, without normalization\n",
      "[[165   9]\n",
      " [ 11 165]]\n",
      "f1_score:  0.9428571428571428\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHpCAYAAAChumdzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQFklEQVR4nO3dd1gU59oG8HsWpIgsTWVBEVAUsdcY7EYi9pqokUSwG1vsJQkKWLAXjCUmHhWPHks02BJ7FAsaQbEHsRsRjCIgGIrsfH/4sckKJCy7sDvu/fOa63LfmZ15xsPRO2+ZEURRFEFERERkwGT6LoCIiIjo3zCwEBERkcFjYCEiIiKDx8BCREREBo+BhYiIiAweAwsREREZPAYWIiIiMngMLERERGTwGFiIiIjI4DGwEElcfHw8OnToABsbGwiCgIiICJ2e//79+xAEARs3btTpeaWsbdu2aNu2rb7LIDIqDCxEOnDnzh2MGDECVatWhYWFBeRyOVq0aIEVK1bgzz//LNFr+/v74+rVq5g7dy42b96MJk2alOj1SlNAQAAEQYBcLi/wzzE+Ph6CIEAQBCxevFjj8yckJCAoKAixsbE6qJaISpKpvgsgkroDBw7g448/hrm5OQYOHIg6deogOzsbp0+fxpQpU3D9+nWsW7euRK79559/IioqCl999RXGjBlTItdwdXXFn3/+iTJlypTI+f+NqakpXr16hX379qFv375q+7Zs2QILCwtkZmYW69wJCQkIDg6Gm5sbGjRoUOTvHT58uFjXI6LiY2Ah0sK9e/fQv39/uLq64vjx43ByclLtGz16NG7fvo0DBw6U2PX/+OMPAICtrW2JXUMQBFhYWJTY+f+Nubk5WrRogf/973/5AsvWrVvRpUsX7Nq1q1RqefXqFcqWLQszM7NSuR4R/YVDQkRaWLhwIdLT07F+/Xq1sJLHw8MDX3zxherz69evMXv2bFSrVg3m5uZwc3PDl19+iaysLLXvubm5oWvXrjh9+jTee+89WFhYoGrVqggPD1cdExQUBFdXVwDAlClTIAgC3NzcALwZSsn7/d8FBQVBEAS1tiNHjqBly5awtbVFuXLl4OnpiS+//FK1v7A5LMePH0erVq1gZWUFW1tb9OjRAzdv3izwerdv30ZAQABsbW1hY2ODQYMG4dWrV4X/wb5lwIAB+Pnnn5GSkqJqu3DhAuLj4zFgwIB8xycnJ2Py5MmoW7cuypUrB7lcjk6dOuHy5cuqY06cOIGmTZsCAAYNGqQaWsq7z7Zt26JOnTqIiYlB69atUbZsWdWfy9tzWPz9/WFhYZHv/n19fWFnZ4eEhIQi3ysRFYyBhUgL+/btQ9WqVdG8efMiHT906FDMnDkTjRo1wrJly9CmTRuEhoaif//++Y69ffs2PvroI3z44YdYsmQJ7OzsEBAQgOvXrwMAevfujWXLlgEAPvnkE2zevBnLly/XqP7r16+ja9euyMrKQkhICJYsWYLu3bvjzJkz//i9o0ePwtfXF0+fPkVQUBAmTpyIs2fPokWLFrh//36+4/v27YuXL18iNDQUffv2xcaNGxEcHFzkOnv37g1BELB7925V29atW1GzZk00atQo3/F3795FREQEunbtiqVLl2LKlCm4evUq2rRpowoPXl5eCAkJAQAMHz4cmzdvxubNm9G6dWvVeZ4/f45OnTqhQYMGWL58Odq1a1dgfStWrECFChXg7++P3NxcAMC3336Lw4cPY+XKlXB2di7yvRJRIUQiKpbU1FQRgNijR48iHR8bGysCEIcOHarWPnnyZBGAePz4cVWbq6urCECMjIxUtT19+lQ0NzcXJ02apGq7d++eCEBctGiR2jn9/f1FV1fXfDXMmjVL/Pv/7ZctWyYCEP/4449C6867xoYNG1RtDRo0ECtWrCg+f/5c1Xb58mVRJpOJAwcOzHe9wYMHq52zV69eooODQ6HX/Pt9WFlZiaIoih999JHYvn17URRFMTc3V1QoFGJwcHCBfwaZmZlibm5uvvswNzcXQ0JCVG0XLlzId2952rRpIwIQ165dW+C+Nm3aqLUdOnRIBCDOmTNHvHv3rliuXDmxZ8+e/3qPRFQ07GEhKqa0tDQAgLW1dZGO/+mnnwAAEydOVGufNGkSAOSb61KrVi20atVK9blChQrw9PTE3bt3i13z2/LmvuzZswdKpbJI33ny5AliY2MREBAAe3t7VXu9evXw4Ycfqu7z70aOHKn2uVWrVnj+/Lnqz7AoBgwYgBMnTiAxMRHHjx9HYmJigcNBwJt5LzLZm7/ecnNz8fz5c9Vw18WLF4t8TXNzcwwaNKhIx3bo0AEjRoxASEgIevfuDQsLC3z77bdFvhYR/TMGFqJiksvlAICXL18W6fgHDx5AJpPBw8NDrV2hUMDW1hYPHjxQa69SpUq+c9jZ2eHFixfFrDi/fv36oUWLFhg6dCgcHR3Rv39/7Nix4x/DS16dnp6e+fZ5eXnh2bNnyMjIUGt/+17s7OwAQKN76dy5M6ytrbF9+3Zs2bIFTZs2zfdnmUepVGLZsmWoXr06zM3NUb58eVSoUAFXrlxBampqka9ZqVIljSbYLl68GPb29oiNjUVYWBgqVqxY5O8S0T9jYCEqJrlcDmdnZ1y7dk2j77096bUwJiYmBbaLoljsa+TNr8hjaWmJyMhIHD16FJ999hmuXLmCfv364cMPP8x3rDa0uZc85ubm6N27NzZt2oQff/yx0N4VAJg3bx4mTpyI1q1b47///S8OHTqEI0eOoHbt2kXuSQLe/Plo4tKlS3j69CkA4OrVqxp9l4j+GQMLkRa6du2KO3fuICoq6l+PdXV1hVKpRHx8vFp7UlISUlJSVCt+dMHOzk5tRU2et3txAEAmk6F9+/ZYunQpbty4gblz5+L48eP45ZdfCjx3Xp1xcXH59v32228oX748rKystLuBQgwYMACXLl3Cy5cvC5yonOeHH35Au3btsH79evTv3x8dOnSAj49Pvj+ToobHosjIyMCgQYNQq1YtDB8+HAsXLsSFCxd0dn4iY8fAQqSFqVOnwsrKCkOHDkVSUlK+/Xfu3MGKFSsAvBnSAJBvJc/SpUsBAF26dNFZXdWqVUNqaiquXLmianvy5Al+/PFHteOSk5PzfTfvAWpvL7XO4+TkhAYNGmDTpk1qAeDatWs4fPiw6j5LQrt27TB79mx88803UCgUhR5nYmKSr/dm586dePz4sVpbXrAqKNxpatq0aXj48CE2bdqEpUuXws3NDf7+/oX+ORKRZvjgOCItVKtWDVu3bkW/fv3g5eWl9qTbs2fPYufOnQgICAAA1K9fH/7+/li3bh1SUlLQpk0b/Prrr9i0aRN69uxZ6JLZ4ujfvz+mTZuGXr16Ydy4cXj16hXWrFmDGjVqqE06DQkJQWRkJLp06QJXV1c8ffoUq1evRuXKldGyZctCz79o0SJ06tQJ3t7eGDJkCP7880+sXLkSNjY2CAoK0tl9vE0mk+Hrr7/+1+O6du2KkJAQDBo0CM2bN8fVq1exZcsWVK1aVe24atWqwdbWFmvXroW1tTWsrKzQrFkzuLu7a1TX8ePHsXr1asyaNUu1zHrDhg1o27YtAgMDsXDhQo3OR0QF0PMqJaJ3wq1bt8Rhw4aJbm5uopmZmWhtbS22aNFCXLlypZiZmak6LicnRwwODhbd3d3FMmXKiC4uLuKMGTPUjhHFN8uau3Tpku86by+nLWxZsyiK4uHDh8U6deqIZmZmoqenp/jf//4337LmY8eOiT169BCdnZ1FMzMz0dnZWfzkk0/EW7du5bvG20t/jx49KrZo0UK0tLQU5XK52K1bN/HGjRtqx+Rd7+1l0xs2bBABiPfu3Sv0z1QU1Zc1F6awZc2TJk0SnZycREtLS7FFixZiVFRUgcuR9+zZI9aqVUs0NTVVu882bdqItWvXLvCafz9PWlqa6OrqKjZq1EjMyclRO27ChAmiTCYTo6Ki/vEeiOjfCaKowaw3IiIiIj3gHBYiIiIyeAwsREREZPAYWIiIiMjgMbAQERGRwWNgISIiIoPHwEJEREQGjw+OKyVKpRIJCQmwtrbW6ePAiYiodImiiJcvX8LZ2Vn1VvCSlpmZiezsbJ2cy8zMDBYWFjo5V2liYCklCQkJcHFx0XcZRESkI48ePULlypVL/DqZmZmwtHYAXr/SyfkUCgXu3bsnudDCwFJKrK2tAQBmtfwhmBT9dfVEUvPwxGJ9l0BUol6mpcHD3UX193pJy87OBl6/gnntQYC2/37kZiPx+gZkZ2czsFDB8oaBBBMzBhZ6p8nlcn2XQFQqSn14Xwf/fkj50fYMLERERFIgANA2JEl4CiUDCxERkRQIsjebtueQKOlWTkREREaDPSxERERSIAg6GBKS7pgQAwsREZEUcEiIiIiIDF5eD4u2mwYiIyPRrVs3ODs7QxAERERE5Dvm5s2b6N69O2xsbGBlZYWmTZvi4cOHqv2ZmZkYPXo0HBwcUK5cOfTp0wdJSUka3z4DCxERERUoIyMD9evXx6pVqwrcf+fOHbRs2RI1a9bEiRMncOXKFQQGBqo942XChAnYt28fdu7ciZMnTyIhIQG9e/fWuBYOCREREUmCDoaENOyn6NSpEzp16lTo/q+++gqdO3fGwoULVW3VqlVT/T41NRXr16/H1q1b8cEHHwAANmzYAC8vL5w7dw7vv/9+CVVORERE+qHDIaG0tDS1LSsrS+NylEolDhw4gBo1asDX1xcVK1ZEs2bN1IaNYmJikJOTAx8fH1VbzZo1UaVKFURFRWl0PQYWIiIiI+Pi4gIbGxvVFhoaqvE5nj59ivT0dMyfPx8dO3bE4cOH0atXL/Tu3RsnT54EACQmJsLMzAy2trZq33V0dERiYqJG1+OQEBERkRTocJXQo0eP1F6jYW5urvGplEolAKBHjx6YMGECAKBBgwY4e/Ys1q5dizZt2mhX61vYw0JERCQFOhwSksvlaltxAkv58uVhamqKWrVqqbV7eXmpVgkpFApkZ2cjJSVF7ZikpCQoFAqNrsfAQkRERBozMzND06ZNERcXp9Z+69YtuLq6AgAaN26MMmXK4NixY6r9cXFxePjwIby9vTW6HoeEiIiIpEAPD45LT0/H7du3VZ/v3buH2NhY2Nvbo0qVKpgyZQr69euH1q1bo127djh48CD27duHEydOAABsbGwwZMgQTJw4Efb29pDL5Rg7diy8vb01WiEEMLAQERFJgx4ezR8dHY127dqpPk+cOBEA4O/vj40bN6JXr15Yu3YtQkNDMW7cOHh6emLXrl1o2bKl6jvLli2DTCZDnz59kJWVBV9fX6xevVrz0kVRFDX+FmksLS0NNjY2MK87DIKJmb7LISoxLy58o+8SiEpUWloaHB1skJqaqjZxtSSvZ2NjA/P3p0Iw1Xyuyd+Jr7OQdW5hqdWuS+xhISIikgIjf5cQAwsREZEUCIIOAot039Ys3ahFRERERoM9LERERFIgE95s2p5DohhYiIiIpIBzWIiIiMjg6WFZsyGRbtQiIiIio8EeFiIiIingkBAREREZPA4JERERERk29rAQERFJAYeEiIiIyOBxSIiIiIjIsLGHhYiISAo4JEREREQGj0NCRERERIaNPSxERESSoIMhIQn3UzCwEBERSYGRDwkxsBAREUmBIOhg0q10A4t0+4aIiIjIaLCHhYiISAq4rJmIiIgMnpHPYZFu1CIiIiKjwR4WIiIiKeCQEBERERk8DgkRERERGTb2sBAREUkBh4SIiIjI4HFIiIiIiMiwsYeFiIhIAgRBgGDEPSwMLERERBJg7IGFQ0JERERk8NjDQkREJAXC/2/ankOiGFiIiIgkwNiHhBhYiIiIJMDYAwvnsBAREZHBY2AhIiKSgLweFm03TURGRqJbt25wdnaGIAiIiIgo9NiRI0dCEAQsX75crT05ORl+fn6Qy+WwtbXFkCFDkJ6ervH9M7AQERFJgD4CS0ZGBurXr49Vq1b943E//vgjzp07B2dn53z7/Pz8cP36dRw5cgT79+9HZGQkhg8frlEdAOewEBERUSE6deqETp06/eMxjx8/xtixY3Ho0CF06dJFbd/Nmzdx8OBBXLhwAU2aNAEArFy5Ep07d8bixYsLDDiFYQ8LERGRFAg62gCkpaWpbVlZWcUqSalU4rPPPsOUKVNQu3btfPujoqJga2urCisA4OPjA5lMhvPnz2t0LQYWIiIiCdDlkJCLiwtsbGxUW2hoaLFqWrBgAUxNTTFu3LgC9ycmJqJixYpqbaamprC3t0diYqJG1+KQEBERkZF59OgR5HK56rO5ubnG54iJicGKFStw8eJF7ZdbFwF7WIiIiCRAEHTRy/LmXHK5XG0rTmA5deoUnj59iipVqsDU1BSmpqZ48OABJk2aBDc3NwCAQqHA06dP1b73+vVrJCcnQ6FQaHQ99rAQERFJgAAdPDhOh8/m/+yzz+Dj46PW5uvri88++wyDBg0CAHh7eyMlJQUxMTFo3LgxAOD48eNQKpVo1qyZRtdjYCEiIqICpaen4/bt26rP9+7dQ2xsLOzt7VGlShU4ODioHV+mTBkoFAp4enoCALy8vNCxY0cMGzYMa9euRU5ODsaMGYP+/ftrtEIIYGAhIiKSBH08mj86Ohrt2rVTfZ44cSIAwN/fHxs3bizSObZs2YIxY8agffv2kMlk6NOnD8LCwjSqA2BgISIikgY9vK25bdu2EEWxyMffv38/X5u9vT22bt2q2YULwEm3REREZPDYw0JERCQFOhgSEiX8tmYGFiIiIgnQxRyW0nheSklhYCEiIpIAYw8snMNCREREBo89LERERFKgh1VChoSBhYiISAI4JERERERk4NjDQkREJAHG3sPCwEJERCQBxh5YOCREREREBo89LERERBJg7D0sDCxERERSYOTLmjkkRERERAaPPSxEREQSwCEhIiIiMngMLERERGTwjD2wcA4LSUqLRtXww/IRuHt4Lv689A26ta2X7xhPd0fsXD4CiZGL8OzsEpz+7xS4KOxU+w999wX+vPSN2hb2Vf/SvA0irb18+RKTJ45HjWqusLO2RNtWzRF94YK+yyIqMexhIUmxsjTH1VuPEb4nCtuXDs+3371yeRz7z0RsijiLOWsOIC0jE7WqOSEzK0ftuPW7zmD2mv2qz68yc94+FZFB+3zEUNy4fg3/2bgZTk7O+N/W/6JLRx9cvHIDlSpV0nd5VBKMfJUQAwtJyuEzN3D4zI1C9weP6YZDp6/jqxV7VG33fn+W77g/M7OR9PxlidRIVNL+/PNPROzehZ2796Blq9YAgK9nBuGn/fvw3bdrEBQyR88VUkngkBDRO0IQBHRsWRvxD59i76rReHAsFJHhkwscNurXuQkeHZ+P6J1fImRsd1halNFDxUTF8/r1a+Tm5sLCwkKt3cLSEmfPnNZTVUQli4GF3hkV7cvB2soCkwd9iCNnb6Db599g7y+XsW3JULRs7KE6bvvP0Rj8VTg6Dg/D4v8cxoAuTbFhjr8eKyfSjLW1NZq9743QubORkJCA3Nxc/G/Lf3H+XBQSE5/ouzwqIXk9LNpuUsUhIQ0EBAQgJSUFERERAIC2bduiQYMGWL58uV7rojdksjf5e/+Jq1i55RcAwJVbj9GsflUM+6glTsfcBgD8Z/cZ1Xeu307Ak2dpOLhuHNwrly9w+IjIEP1n42aMGDYY1VwrwcTEBA0aNkLffp/g0qUYfZdGJUSADoaEJDyJRa89LAEBARAEAfPnz1drj4iI0Ph/FDc3tyIFBzc3t3xps3LlyhpdiwzTsxfpyMnJxc276v+FGXc3UW2V0NsuXL0PAKjmUqEkyyPSqarVquHI8ZN4lpKO+HuPcDrqV+S8zoG7e1V9l0ZUIvQ+JGRhYYEFCxbgxYsXpXbNkJAQPHnyRLVdunSp1K5NJSfndS5ibjxADVdHtfbqrhXx8EnhP1/1Pd8E1sRnqSVaH1FJsLKygpOTE168eIGjhw+ha7ce+i6JSoixDwnpPbD4+PhAoVAgNDT0H4/btWsXateuDXNzc7i5uWHJkiWqfW3btsWDBw8wYcKEIv0PYm1tDYVCodoqVKiA3NxcDBkyBO7u7rC0tISnpydWrFihk3sk3bGyNEO9GpVQr8abZZtulRxQr0YlVQ/Ksk1H8ZFvIwzq1RxVXcpjZL/W6Ny6DtbtiATwZtnz9GEd0dDLBVWc7NGlTV18P/sznIqJx7X4BL3dF5Gmjhw+hMOHDuL+vXs4dvQIOvq0Qw3PmhgYMEjfpVFJEXS0SZTe57CYmJhg3rx5GDBgAMaNG1fg8ExMTAz69u2LoKAg9OvXD2fPnsWoUaPg4OCAgIAA7N69G/Xr18fw4cMxbNiwYtWhVCpRuXJl7Ny5Ew4ODjh79iyGDx8OJycn9O3bV+PzZWVlISsrS/U5LS2tWHWRuka1XHH4+y9UnxdO7gMA2Lz3HIbP+i/2/nIFY+duw5TBHbBk6ke49eApPpnyPc7G3gUA5OS8xgfNPDFmQDtYWZrh96QXiDgWi/nfH9LL/RAVV2pqKmZ+PQOPf/8d9vb26NGrD4Jnz0WZMlzxRu8mvQcWAOjVqxcaNGiAWbNmYf369fn2L126FO3bt0dgYCAAoEaNGrhx4wYWLVqEgIAA2Nvbw8TERNVz8m+mTZuGr7/+WvV53rx5GDduHIKDg1Vt7u7uiIqKwo4dO4oVWEJDQ9XOR7pxKiYelg3H/OMx4XvOIXzPuQL3/Z6Ugg5D2XNG0vfRx33x0cea/91E0sXnsBiIBQsWYNOmTbh582a+fTdv3kSLFi3U2lq0aIH4+Hjk5uZqfK0pU6YgNjZWtQ0cOBAAsGrVKjRu3BgVKlRAuXLlsG7dOjx8+LBY9zNjxgykpqaqtkePHhXrPERERADnsBhEDwsAtG7dGr6+vpgxYwYCAgJK9Frly5eHh4eHWtu2bdswefJkLFmyBN7e3rC2tsaiRYtw/vz5Yl3D3Nwc5ubmuiiXiIjI6BlMYAGA+fPno0GDBvD09FRr9/LywpkzZ9Tazpw5gxo1asDExAQAYGZmVqzelr+fr3nz5hg1apSq7c6dO8U+HxERkS4JwptN23NIlcEMCQFA3bp14efnh7CwMLX2SZMm4dixY5g9ezZu3bqFTZs24ZtvvsHkyZNVx7i5uSEyMhKPHz/Gs2eaP/yrevXqiI6OxqFDh3Dr1i0EBgbiAt98SkREBuJNYNF2SEjfd1F8BhVYgDfPSFEqlWptjRo1wo4dO7Bt2zbUqVMHM2fOREhIiNrQUUhICO7fv49q1aqhQgXNHwA2YsQI9O7dG/369UOzZs3w/Plztd4WIiIivRL+6mUp7iblZc2CKIqivoswBmlpabCxsYF53WEQTMz0XQ5RiXlx4Rt9l0BUotLS0uDoYIPU1FTI5fJSuZ6NjQ2qjvsBJuZWWp0rNysDd8M+KrXadcmg5rAQERFRwYx9WTMDCxERkQRw0i0RERGRgWNgISIikgCZTNDJponIyEh069YNzs7OEAQBERERqn05OTmYNm0a6tatCysrKzg7O2PgwIFISFB/L1tycjL8/Pwgl8tha2uLIUOGID09XfP71/gbREREVOq0XSFUnCGljIwM1K9fH6tWrcq379WrV7h48SICAwNx8eJF7N69G3FxcejevbvacX5+frh+/TqOHDmC/fv3IzIyEsOHD9f4/jmHhYiIiArUqVMndOrUqcB9NjY2OHLkiFrbN998g/feew8PHz5ElSpVcPPmTRw8eBAXLlxAkyZNAAArV65E586dsXjxYjg7Oxe5FvawEBERSYAu3yWUlpamtmVlZemkxtTUVAiCAFtbWwBAVFQUbG1tVWEFAHx8fCCTyTR+9Q0DCxERkQTockjIxcUFNjY2qi00NFTr+jIzMzFt2jR88sknqme8JCYmomLFimrHmZqawt7eHomJiRqdn0NCRERERubRo0dqD47T9mW9OTk56Nu3L0RRxJo1a7Qtr0AMLERERBKgywfHyeVynT3pNi+sPHjwAMePH1c7r0KhwNOnT9WOf/36NZKTk6FQKDS6DoeEiIiIJECXc1h0JS+sxMfH4+jRo3BwcFDb7+3tjZSUFMTExKjajh8/DqVSiWbNmml0LfawEBERSYA+nnSbnp6O27dvqz7fu3cPsbGxsLe3h5OTEz766CNcvHgR+/fvR25urmpeir29PczMzODl5YWOHTti2LBhWLt2LXJycjBmzBj0799foxVCAAMLERERFSI6Ohrt2rVTfZ44cSIAwN/fH0FBQdi7dy8AoEGDBmrf++WXX9C2bVsAwJYtWzBmzBi0b98eMpkMffr0QVhYmMa1MLAQERFJgAAdzGGBZt9v27YtRFEsdP8/7ctjb2+PrVu3anTdgjCwEBERSQBffkhERERk4NjDQkREJAG6XNYsRQwsREREEsAhISIiIiIDxx4WIiIiCeCQEBERERk8DgkRERERGTj2sBAREUkAh4SIiIjI8OlgSEjDB90aFA4JERERkcFjDwsREZEEcEiIiIiIDJ6xrxJiYCEiIpIAY+9h4RwWIiIiMnjsYSEiIpIADgkRERGRweOQEBEREZGBYw8LERGRBBh7DwsDCxERkQQY+xwWDgkRERGRwWMPCxERkQRwSIiIiIgMHoeEiIiIiAwce1iIiIgkgENCREREZPAE6GBISCeV6AcDCxERkQTIBAEyLROLtt/XJ85hISIiIoPHHhYiIiIJMPZVQgwsREREEmDsk245JEREREQGjz0sREREEiAT3mzankOqGFiIiIikQNDBkI6EAwuHhIiIiMjgsYeFiIhIArhKiIiIiAye8P+/tD2HVHFIiIiIiAweAwsREZEE5K0S0nbTRGRkJLp16wZnZ2cIgoCIiAi1/aIoYubMmXBycoKlpSV8fHwQHx+vdkxycjL8/Pwgl8tha2uLIUOGID09XfP71/gbREREVOryHhyn7aaJjIwM1K9fH6tWrSpw/8KFCxEWFoa1a9fi/PnzsLKygq+vLzIzM1XH+Pn54fr16zhy5Aj279+PyMhIDB8+XOP75xwWIiIiKlCnTp3QqVOnAveJoojly5fj66+/Ro8ePQAA4eHhcHR0REREBPr374+bN2/i4MGDuHDhApo0aQIAWLlyJTp37ozFixfD2dm5yLUUKbDs3bu3yCfs3r17kY8lIiKiotHlKqG0tDS1dnNzc5ibm2t0rnv37iExMRE+Pj6qNhsbGzRr1gxRUVHo378/oqKiYGtrqworAODj4wOZTIbz58+jV69eRb5ekQJLz549i3QyQRCQm5tb5IsTERFR0cgEATItE0ve911cXNTaZ82ahaCgII3OlZiYCABwdHRUa3d0dFTtS0xMRMWKFdX2m5qawt7eXnVMURUpsCiVSo1OSkRERLqlyx6WR48eQS6Xq9o17V3RB60m3f59Ug0RERFJg1wuV9uKE1gUCgUAICkpSa09KSlJtU+hUODp06dq+1+/fo3k5GTVMUWlcWDJzc3F7NmzUalSJZQrVw53794FAAQGBmL9+vWano6IiIiKQB+rhP6Ju7s7FAoFjh07pmpLS0vD+fPn4e3tDQDw9vZGSkoKYmJiVMccP34cSqUSzZo10+h6GgeWuXPnYuPGjVi4cCHMzMxU7XXq1MH333+v6emIiIioCPKGhLTdNJGeno7Y2FjExsYCeDPRNjY2Fg8fPoQgCBg/fjzmzJmDvXv34urVqxg4cCCcnZ1Vc1+9vLzQsWNHDBs2DL/++ivOnDmDMWPGoH///hqtEAKKEVjCw8Oxbt06+Pn5wcTERNVev359/Pbbb5qejoiIiAxUdHQ0GjZsiIYNGwIAJk6ciIYNG2LmzJkAgKlTp2Ls2LEYPnw4mjZtivT0dBw8eBAWFhaqc2zZsgU1a9ZE+/bt0blzZ7Rs2RLr1q3TuBaNn8Py+PFjeHh45GtXKpXIycnRuAAiIiL6d7pcJVRUbdu2hSiKhe4XBAEhISEICQkp9Bh7e3ts3bpVo+sWROMellq1auHUqVP52n/44QdVAiMiIiLdEnS0SZXGPSwzZ86Ev78/Hj9+DKVSid27dyMuLg7h4eHYv39/SdRIRERERk7jHpYePXpg3759OHr0KKysrDBz5kzcvHkT+/btw4cfflgSNRIRERk9Q1slVNqK9S6hVq1a4ciRI7quhYiIiApRnLctF3QOqSr2yw+jo6Nx8+ZNAG/mtTRu3FhnRRERERH9ncaB5ffff8cnn3yCM2fOwNbWFgCQkpKC5s2bY9u2bahcubKuayQiIjJ6uhjSkfKQkMZzWIYOHYqcnBzcvHkTycnJSE5Oxs2bN6FUKjF06NCSqJGIiIhQug+NMzQa97CcPHkSZ8+ehaenp6rN09MTK1euRKtWrXRaHBERERFQjMDi4uJS4APicnNzNX7MLhERERUNh4Q0tGjRIowdOxbR0dGqtujoaHzxxRdYvHixTosjIiKiN/JWCWm7SVWReljs7OzUUllGRgaaNWsGU9M3X3/9+jVMTU0xePBg1QuPiIiISHeMvYelSIFl+fLlJVwGERERUeGKFFj8/f1Lug4iIiL6B7p4F5B0+1e0eHAcAGRmZiI7O1utTS6Xa1UQERER5aePtzUbEo0n3WZkZGDMmDGoWLEirKysYGdnp7YRERER6ZrGgWXq1Kk4fvw41qxZA3Nzc3z//fcIDg6Gs7MzwsPDS6JGIiIio6ftQ+Ok/vA4jYeE9u3bh/DwcLRt2xaDBg1Cq1at4OHhAVdXV2zZsgV+fn4lUScREZFRM/ZVQhr3sCQnJ6Nq1aoA3sxXSU5OBgC0bNkSkZGRuq2OiIiICMUILFWrVsW9e/cAADVr1sSOHTsAvOl5yXsZIhEREemWsQ8JaRxYBg0ahMuXLwMApk+fjlWrVsHCwgITJkzAlClTdF4gERER/bVKSNtNqjSewzJhwgTV7318fPDbb78hJiYGHh4eqFevnk6LIyIiIgK0fA4LALi6usLV1VUXtRAREVEhdDGkI+EOlqIFlrCwsCKfcNy4ccUuhoiIiApm7KuEihRYli1bVqSTCYLAwPIv7h1fxKcB0zvNrukYfZdAVKLE3Ox/P6gEyFCMiacFnEOqihRY8lYFEREREemD1nNYiIiIqORxSIiIiIgMniAAMiOedCvl4SwiIiIyEuxhISIikgCZDnpYtP2+PjGwEBERSYCxz2Ep1pDQqVOn8Omnn8Lb2xuPHz8GAGzevBmnT5/WaXFEREREQDECy65du+Dr6wtLS0tcunQJWVlZAIDU1FTMmzdP5wUSERHRX0NC2m5SpXFgmTNnDtauXYvvvvsOZcqUUbW3aNECFy9e1GlxRERE9Abf1qyhuLg4tG7dOl+7jY0NUlJSdFETERERkRqNA4tCocDt27fztZ8+fRpVq1bVSVFERESkTiYIOtmkSuPAMmzYMHzxxRc4f/48BEFAQkICtmzZgsmTJ+Pzzz8viRqJiIiMnkxHm1RpXPv06dMxYMAAtG/fHunp6WjdujWGDh2KESNGYOzYsSVRIxEREelBbm4uAgMD4e7uDktLS1SrVg2zZ8+GKIqqY0RRxMyZM+Hk5ARLS0v4+PggPj5e57Vo/BwWQRDw1VdfYcqUKbh9+zbS09NRq1YtlCtXTufFERER0Ru6mDSr6fcXLFiANWvWYNOmTahduzaio6MxaNAg2NjYYNy4cQCAhQsXIiwsDJs2bYK7uzsCAwPh6+uLGzduwMLCQruC/6bYD44zMzNDrVq1dFYIERERFU4G7eegyKDZ98+ePYsePXqgS5cuAAA3Nzf873//w6+//grgTe/K8uXL8fXXX6NHjx4AgPDwcDg6OiIiIgL9+/fXqt6/0ziwtGvX7h+flHf8+HGtCiIiIqL8dNnDkpaWptZubm4Oc3PzfMc3b94c69atw61bt1CjRg1cvnwZp0+fxtKlSwEA9+7dQ2JiInx8fFTfsbGxQbNmzRAVFaXfwNKgQQO1zzk5OYiNjcW1a9fg7++vq7qIiIiohLi4uKh9njVrFoKCgvIdN336dKSlpaFmzZowMTFBbm4u5s6dCz8/PwBAYmIiAMDR0VHte46Ojqp9uqJxYFm2bFmB7UFBQUhPT9e6ICIiIspPly8/fPToEeRyuaq9oN4VANixYwe2bNmCrVu3onbt2oiNjcX48ePh7Oxc6p0UOnv54aeffor33nsPixcv1tUpiYiI6P8JArSew5L3dblcrhZYCjNlyhRMnz5dNbRTt25dPHjwAKGhofD394dCoQAAJCUlwcnJSfW9pKSkfCMy2tLZkuyoqCidzgYmIiIi/Xr16hVkMvWoYGJiAqVSCQBwd3eHQqHAsWPHVPvT0tJw/vx5eHt767QWjXtYevfurfZZFEU8efIE0dHRCAwM1FlhRERE9Bd9LGvu1q0b5s6diypVqqB27dq4dOkSli5disGDB///+QSMHz8ec+bMQfXq1VXLmp2dndGzZ0/tin2LxoHFxsZG7bNMJoOnpydCQkLQoUMHnRVGREREf9HlHJaiWrlyJQIDAzFq1Cg8ffoUzs7OGDFiBGbOnKk6ZurUqcjIyMDw4cORkpKCli1b4uDBgzofdRHEvz+u7l/k5ubizJkzqFu3Luzs7HRayLsuLS0NNjY2SPgjpUjjhkRSVb4Zn3hN7zYxNxtZV79Dampqqfx9nvfvx9d7LsLCylqrc2VmvMScHo1KrXZd0mgOi4mJCTp06MC3MhMREZUyQUe/pErjSbd16tTB3bt3S6IWIiIiKkTekJC2m1RpHFjmzJmDyZMnY//+/Xjy5AnS0tLUNiIiIiJdK/Kk25CQEEyaNAmdO3cGAHTv3l3tEf2iKEIQBOTm5uq+SiIiIiOnj0m3hqTIgSU4OBgjR47EL7/8UpL1EBERUQEEQfjHd/kV9RxSVeTAkreYqE2bNiVWDBERERXM2HtYNJrDIuVkRkRERNKl0YPjatSo8a+hJTk5WauCiIiIKD99POnWkGgUWIKDg/M96ZaIiIhKnkwQtH75obbf1yeNAkv//v1RsWLFkqqFiIiIqEBFDiycv0JERKQ/xj7pVuNVQkRERKQHOpjDIuEn8xc9sCiVypKsg4iIiKhQGs1hISIiIv2QQYBMyy4Sbb+vTwwsREREEmDsy5o1fvkhERERUWljDwsREZEEcJUQERERGTxjf3Ach4SIiIjI4LGHhYiISAKMfdItAwsREZEEyKCDISEuayYiIqKSZOw9LJzDQkRERAaPPSxEREQSIIP2vQxS7qVgYCEiIpIAQRAgaDmmo+339UnKYYuIiIiMBHtYiIiIJED4/03bc0gVAwsREZEE8Em3RERERAaOPSxEREQSId3+Ee0xsBAREUkAHxxHREREZODYw0JERCQBxv4cFgYWIiIiCeCTbomIiMjgGXsPi5TDFhERERkJ9rAQERFJgLE/6ZY9LERERBKQNySk7aapx48f49NPP4WDgwMsLS1Rt25dREdHq/aLooiZM2fCyckJlpaW8PHxQXx8vC5vHQADCxERERXixYsXaNGiBcqUKYOff/4ZN27cwJIlS2BnZ6c6ZuHChQgLC8PatWtx/vx5WFlZwdfXF5mZmTqthUNCREREEqCPVUILFiyAi4sLNmzYoGpzd3dX/V4URSxfvhxff/01evToAQAIDw+Ho6MjIiIi0L9/fy0r/gt7WIiIiCRAl0NCaWlpaltWVlaB19y7dy+aNGmCjz/+GBUrVkTDhg3x3Xffqfbfu3cPiYmJ8PHxUbXZ2NigWbNmiIqK0un9M7AQEREZGRcXF9jY2Ki20NDQAo+7e/cu1qxZg+rVq+PQoUP4/PPPMW7cOGzatAkAkJiYCABwdHRU+56jo6Nqn65wSIiIiEgCdLlK6NGjR5DL5ap2c3PzAo9XKpVo0qQJ5s2bBwBo2LAhrl27hrVr18Lf31/LajTDHhYiIiIJyHv5obYbAMjlcrWtsMDi5OSEWrVqqbV5eXnh4cOHAACFQgEASEpKUjsmKSlJtU9XGFiIiIioQC1atEBcXJxa261bt+Dq6grgzQRchUKBY8eOqfanpaXh/Pnz8Pb21mktHBIiIiKSABkEyLQcFNL0+xMmTEDz5s0xb9489O3bF7/++ivWrVuHdevWAXgzEXj8+PGYM2cOqlevDnd3dwQGBsLZ2Rk9e/bUqta3MbAQERFJwN+HdLQ5hyaaNm2KH3/8ETNmzEBISAjc3d2xfPly+Pn5qY6ZOnUqMjIyMHz4cKSkpKBly5Y4ePAgLCwstCv2LQwsREREVKiuXbuia9euhe4XBAEhISEICQkp0ToYWIiIiCRA+P9f2p5DqhhYiIiIJEAfQ0KGhIGFiIhIAgQdTLqVcg8LlzUTERGRwWMPCxERkQRwSIiIiIgMnrEHFg4JERERkcFjDwsREZEEcFkzERERGTyZ8GbT9hxSxSEhIiIiMnjsYSEiIpIADgkRERGRweMqISKJO30qEh/36g4Pt0ooZy7Dvj0Ravv3ROxG986+qOJUHuXMZbhyOVYvdRIVVYtG1fDD8hG4e3gu/rz0Dbq1rZfvGE93R+xcPgKJkYvw7OwSnP7vFLgo7FT7D333Bf689I3aFvZV/9K8DSKdYg8LSd6rjAzUqVcPnwUMwoC+fQrc792iBXp/9DHGfD5cDxUSacbK0hxXbz1G+J4obF+a/2fWvXJ5HPvPRGyKOIs5aw4gLSMTtao5ITMrR+249bvOYPaa/arPrzJz3j4VSYgA7Yd0JNzBwsBC0tehYyd06Nip0P2f+H0GAHhw/34pVUSkncNnbuDwmRuF7g8e0w2HTl/HVyv2qNru/f4s33F/ZmYj6fnLEqmRSh9XCRERkWQIgoCOLWsj/uFT7F01Gg+OhSIyfHKBw0b9OjfBo+PzEb3zS4SM7Q5LizJ6qJh0RdDRL6liYCmijRs3wtbWVvU5KCgIDRo00Fs9RGScKtqXg7WVBSYP+hBHzt5At8+/wd5fLmPbkqFo2dhDddz2n6Mx+KtwdBwehsX/OYwBXZpiwxx/PVZOpB2jGxIKCAjApk2b8rXHx8fDw8OjgG8QERkOmezNf2fuP3EVK7f8AgC4cusxmtWvimEftcTpmNsAgP/sPqP6zvXbCXjyLA0H142De+XyBQ4fkeHjKiEj1LFjRzx58kRtc3d313dZRET/6tmLdOTk5OLm3Sdq7XF3E9VWCb3twtX7AIBqLhVKsjwqQYKONqkyysBibm4OhUKhtq1YsQJ169aFlZUVXFxcMGrUKKSnp+u7VCIiNTmvcxFz4wFquDqqtVd3rYiHT14U+r36npUBAInPUku0PqKSYnRDQoWRyWQICwuDu7s77t69i1GjRmHq1KlYvXp1sc6XlZWFrKws1ee0tDRdlUpvSU9Px907t1WfH9y/hyuXY2FnZw+XKlWQnJyM3x89xJOEBADArVtxAABHRwUcFQq91Ez0T6wszdR6QtwqOaBejUp4kfYKjxJfYNmmo9i8YDBOX7yNk9G30KF5LXRuXQe+w1YAeLPsuV+nJjh0+jqep2Sgbo1KWDipN07FxONafIK+bou0JIMAmZZjOjIJ97EYZWDZv38/ypUrp/rcqVMn7Ny5U/XZzc0Nc+bMwciRI4sdWEJDQxEcHKx1rfTvLsZEo3OHD1Sfp0+dBADw+8wf336/AT/t34uRwwar9gd8+gkAYMbXM/FVYFCp1kpUFI1queLw91+oPi+c/Ob5Qpv3nsPwWf/F3l+uYOzcbZgyuAOWTP0Itx48xSdTvsfZ2LsAgJyc1/igmSfGDGgHK0sz/J70AhHHYjH/+0N6uR/SDV0M6Ug3rhhpYGnXrh3WrFmj+mxlZYWjR48iNDQUv/32G9LS0vD69WtkZmbi1atXKFu2rMbXmDFjBiZOnKj6nJaWBhcXF53UT+pat2mL9Cxlofs/HRiATwcGlF5BRFo6FRMPy4Zj/vGY8D3nEL7nXIH7fk9KQYehK0qiNCK9Mco5LFZWVvDw8FBtWVlZ6Nq1K+rVq4ddu3YhJiYGq1atAgBkZ2cX6xrm5uaQy+VqGxERUbEZ+axbo+xheVtMTAyUSiWWLFmiWjK4Y8cOPVdFRET0F2N/W7NR9rC8zcPDAzk5OVi5ciXu3r2LzZs3Y+3atfoui4iIiP4fAwuA+vXrY+nSpViwYAHq1KmDLVu2IDQ0VN9lERER/UX46+Fxxd0k3MECQRRFUd9FGIO0tDTY2Ngg4Y8Uzmehd1r5ZmP1XQJRiRJzs5F19TukpqaWyt/nef9+HI99iHLW2l0v/WUaPmhQpdRq1yX2sBAREZHB46RbIiIiKTDyB7EwsBAREUmAsa8SYmAhIiKSAL6tmYiIiMjAsYeFiIhIAox8CgsDCxERkSQYeWLhkBAREREZPAYWIiIiCRB09Ku45s+fD0EQMH78eFVbZmYmRo8eDQcHB5QrVw59+vRBUlKSDu42PwYWIiIiCdD2sfzarDK6cOECvv32W9SrV0+tfcKECdi3bx927tyJkydPIiEhAb1799bB3ebHwEJERESFSk9Ph5+fH7777jvY2dmp2lNTU7F+/XosXboUH3zwARo3bowNGzbg7NmzOHfunM7rYGAhIiKSAEFHG/Dm/UR/37Kysgq97ujRo9GlSxf4+PiotcfExCAnJ0etvWbNmqhSpQqioqJ0cMfqGFiIiIikQIeJxcXFBTY2NqotNDS0wEtu27YNFy9eLHB/YmIizMzMYGtrq9bu6OiIxMRELW82Py5rJiIiMjKPHj1Se1uzubl5gcd88cUXOHLkCCwsLEqzvAKxh4WIiEgCdLlKSC6Xq20FBZaYmBg8ffoUjRo1gqmpKUxNTXHy5EmEhYXB1NQUjo6OyM7ORkpKitr3kpKSoFAodH7/7GEhIiKSgNJ+l1D79u1x9epVtbZBgwahZs2amDZtGlxcXFCmTBkcO3YMffr0AQDExcXh4cOH8Pb21q7QAjCwEBERUT7W1taoU6eOWpuVlRUcHBxU7UOGDMHEiRNhb28PuVyOsWPHwtvbG++//77O62FgISIikgBDfDL/smXLIJPJ0KdPH2RlZcHX1xerV6/W8VXeYGAhIiKSAgNILCdOnFD7bGFhgVWrVmHVqlXanbgIGFiIiIgkQNtH6+edQ6q4SoiIiIgMHntYiIiIJKC0VwkZGgYWIiIiCTCAKSx6xSEhIiIiMnjsYSEiIpICI+9iYWAhIiKSAK4SIiIiIjJw7GEhIiKSAK4SIiIiIoNn5FNYOCREREREho89LERERFJg5F0sDCxEREQSYOyrhBhYiIiIpEAHk24lnFc4h4WIiIgMH3tYiIiIJMDIp7AwsBAREUmCkScWDgkRERGRwWMPCxERkQRwlRAREREZPGN/ND+HhIiIiMjgsYeFiIhIAox8zi0DCxERkSQYeWLhkBAREREZPPawEBERSQBXCREREZHBE6CDVUI6qUQ/OCREREREBo89LERERBJg5HNuGViIiIikwNgfHMfAQkREJAnG3cfCOSxERERk8NjDQkREJAEcEiIiIiKDZ9wDQhwSIiIiIglgDwsREZEEcEiIiIiIDJ6xP5qfQ0JERERk8BhYiIiIpEDQ0aaB0NBQNG3aFNbW1qhYsSJ69uyJuLg4tWMyMzMxevRoODg4oFy5cujTpw+SkpKKf5+FYGAhIiKSAD3kFZw8eRKjR4/GuXPncOTIEeTk5KBDhw7IyMhQHTNhwgTs27cPO3fuxMmTJ5GQkIDevXtrda8F4RwWIiIiKtDBgwfVPm/cuBEVK1ZETEwMWrdujdTUVKxfvx5bt27FBx98AADYsGEDvLy8cO7cObz//vs6q4U9LERERBKQt0pI2w0A0tLS1LasrKwi1ZCamgoAsLe3BwDExMQgJycHPj4+qmNq1qyJKlWqICoqSqf3z8BCREQkAYKOfgGAi4sLbGxsVFtoaOi/Xl+pVGL8+PFo0aIF6tSpAwBITEyEmZkZbG1t1Y51dHREYmKiTu+fQ0JERERSoMNH3T569AhyuVzVbG5u/q9fHT16NK5du4bTp09rWUTxMLAQEREZGblcrhZY/s2YMWOwf/9+REZGonLlyqp2hUKB7OxspKSkqPWyJCUlQaFQ6LJkDgkRERFJgT5WCYmiiDFjxuDHH3/E8ePH4e7urra/cePGKFOmDI4dO6Zqi4uLw8OHD+Ht7a35Tf4D9rAQERFJgD4ezT969Ghs3boVe/bsgbW1tWpeio2NDSwtLWFjY4MhQ4Zg4sSJsLe3h1wux9ixY+Ht7a3TFUIAAwsREREVYs2aNQCAtm3bqrVv2LABAQEBAIBly5ZBJpOhT58+yMrKgq+vL1avXq3zWhhYiIiIJEH7dwlpOigkiuK/HmNhYYFVq1Zh1apVxS2qSBhYiIiIJMDY39bMSbdERERk8BhYiIiIyOBxSIiIiEgCOCREREREZODYw0JERCQBgg5WCWm/ykh/GFiIiIgkgENCRERERAaOPSxEREQSoMOXNUsSAwsREZEUGHliYWAhIiKSAGOfdMs5LERERGTw2MNCREQkAca+SoiBhYiISAKMfAoLh4SIiIjI8LGHhYiISAqMvIuFgYWIiEgCuEqIiIiIyMCxh6WUiKIIAHj5Mk3PlRCVLDE3W98lEJWovJ/xvL/XS8vLl2lar/KR8r9BDCyl5OXLlwAAz6pV9FwJERHpwsuXL2FjY1Pi1zEzM4NCoUB1dxednE+hUMDMzEwn5ypNgljaEdFIKZVKJCQkwNraGoKUF8JLSFpaGlxcXPDo0SPI5XJ9l0NUIvhzXvpEUcTLly/h7OwMmax0ZlZkZmYiO1s3vZdmZmawsLDQyblKE3tYSolMJkPlypX1XYZRksvl/Iuc3nn8OS9dpdGz8ncWFhaSDBm6xEm3REREZPAYWIiIiMjgMbDQO8vc3ByzZs2Cubm5vkshKjH8OSdjwUm3REREZPDYw0JEREQGj4GFiIiIDB4DCxERERk8BhYiIiIyeAwsRP/v9u3b+i6BiIgKwcBCBGDLli3w9/fHvn379F0KkVaUSqW+SyAqEQwsRADc3d1hYmKCdevWYf/+/fouh0hjP/30E4A3rwFhaKF3EQMLGbWDBw8iOTkZzZs3x5IlS5CRkYHVq1cztJCkREdHY+TIkRg8eDAAhhZ6NzGwkNGKiorChAkTMGPGDKSkpKBp06aYP38+MjMzGVpIUqpWrYqJEyfi8uXLGDp0KACGFnr3MLCQ0WratCk+/fRT3LhxA19++SVevHiB9957j6GFJGPFihU4ffo07O3tERAQAH9/f0RHRzO00DuJgYWMklKphKmpKaZNm4YuXbrg0qVL+OqrrxhaSDKePXuGn3/+Gd27d8evv/4KW1tbDBw4EIMHD2ZooXcSAwsZJZlMhtzcXJiammLy5Mno3r17vtCyYMECZGZmYt26ddi9e7e+SyZSU758eSxZsgS+vr7o1q0bzp8/z9BC7zQGFjJaJiYmAABTU1NMmTIF3bp1UwstTZs2xcKFC/H7779j27ZtSE9P13PFRG/kvbO2du3aCAwMRJs2bdC9e3eGFnqn8W3NZFREUYQgCLh27Rri4uJgY2MDV1dXVK9eHTk5OVi4cCH279+Phg0bYt68ebC1tcXFixfh4OAAV1dXfZdPpKJUKiGTvflvzmvXriEkJAQnT57E3r170axZM6SkpCA8PBzh4eGoVq0atm/frueKibTDwELvvLyQ8vr1a5iammL37t0YO3YsHBwcoFQq4ezsjGnTpqF9+/aq0HLw4EG4ubnhm2++gY2Njb5vgUgl7+f5bVeuXMGcOXPyhZZvv/0WBw4cwPbt2+Hk5KSHiol0g4GF3ll5/wWakpICW1tbAMAvv/yCvn37Ijg4GKNGjcLOnTsxePBguLi4YNGiRejSpQtycnIQFBSECxcuIDw8HAqFQr83QvT/8sLK6dOnVU9l9vLyQkBAAADg6tWrmD17Nk6ePIl9+/bhvffeQ2pqKpRKJezs7PRYOZH2GFjonZQXVmJjY/HBBx/g2LFjqFmzJsaNGwc7OzssXLgQjx8/RsuWLVG/fn3k5uYiPj4eq1evxgcffIDXr18jNTUVDg4O+r4VMmJ5P8cZGRmwsrICAOzevRvDhg1D69atYW1tjT179mDChAkICgoC8Ca0hIaGYseOHTh//jwaN26sxzsg0iGR6B2Tm5sriqIoxsbGilZWVuL06dNV+65cuSKeOnVKfPHihdiwYUNx6NChoiiK4vbt20VTU1PR0dFRPHDggF7qJvq7vJ/j6OhosVq1auIff/whXrhwQXRxcRHXrFkjiqIo3rp1S7SxsREFQRDHjh2r+u7FixfFgIAAMS4uTi+1E5UEU30HJiJdyvsv0qtXr8Lb2xuTJ09GSEiIan/VqlVhZWWF/fv3w9zcHLNmzQIAODs7o3Xr1qhfvz5q1qypr/KJAPz1c3z58mW0a9cOgwcPRvny5bFv3z707dsXI0eOxKNHj9ChQwf07dsXTZs2xYgRI2BnZ4fg4GA0bNgQ3377LczMzPR9K0Q6w8BC7xSZTIYHDx7A29sbPXr0UAsrS5cuRVpaGoKCgvDq1SvcuHEDCQkJqFy5Mn766SdUrVoVs2bN4iRb0qu8sHLlyhU0b94c48ePx9y5cwEAgwYNwsmTJ1W/b9euHdatW4fff/8dzs7OmD17Nl69eoVFixYxrNA7h4GF3jmiKMLOzg5ZWVk4deoUWrVqhcWLFyMwMBAHDhwA8GaiYsuWLfHxxx/Dzc0NMTExiIqKYlghvZPJZHj06BHat2+Prl27qsIKAKxZswb3799H5cqV8fz5cwQHBwMAypYtiw8//BA+Pj5o0qSJvkonKlF8cBy9U5RKJdzc3HD06FHcunULy5cvx8iRIxEaGoqffvoJH3zwAQCgbt26mDp1KsaOHYumTZsiOjoadevW1XP1RG/k5ubC3d0dmZmZOHPmDAAgNDQU06dPR5cuXWBhYYHr16/j7NmzePXqFRYvXoyrV6+iU6dO8PT01HP1RCWDq4TonZPXpf7bb7+hX79+uHr1KhYvXoyJEycCgOp5LESGLD4+HuPGjYOZmRkcHR2xZ88ebN68GR06dAAALF68GFOnToWHhweSk5Nx5MgRNGzYUM9VE5UcBhZ6J+WFljt37qBnz55wc3PD1KlT0apVK7X9QOEP4iLSt1u3bmHMmDE4ffo0Zs+ejUmTJqn2ZWdn49q1a3j06BEaNWoEFxcXPVZKVPIYWEjy8t6PkveulLwg8veelo8++giurq6YMWMGWrZsqc9yiTRy584djBo1CiYmJvjyyy9VP79//1knMgb8aSfJyQsomZmZAN4Elfj4eNXv8+QFmJo1a+KHH37A48ePMX36dERFRZV+0UTFVK1aNXzzzTcQRRFz5sxRzWlhWCFjw594khyZTIa7d+9i/PjxePz4MX744Qd4eXnh+vXrBR6bF1q2bNkCpVKJypUr66FqouKrXr06wsLCUKZMGUyePBnnzp3Td0lEpY5DQiRJkZGR6NmzJ+rXr4+oqCisW7cOAwcOLHQ+Sm5uLkxMTJCTk4MyZcrooWIi7f32228IDAzEkiVLUKVKFX2XQ1SqGFhIcvJCyYIFCzBjxgy8//77CA8Ph4eHh9r+f/oukVRlZ2fzoXBklDgkRJKTm5sLALCwsMDMmTORlJSEoKAgXLp0CQAgCAL+nsPz5rzk7SOSMoYVMlbsYSHJyOsdefs5KocPH8aIESPQvHlzTJ06FfXr1wcAREVFwdvbW1/lEhGRDjGwkCTkhZVjx47hxx9/xIsXL1CrVi0MGzYMFStWxOHDhzFy5Ei0aNEC/fv3x8WLFzFr1iwkJiaiQoUK7FkhIpI4BhaSjIiICHzyySf49NNP8eDBA7x48QJ//PEHIiMjUaVKFRw7dgyTJ0+GUqlEWloafvjhBzRu3FjfZRMRkQ4wsJBBenty7LNnz/Dhhx9iwIABmDJlCgDg2rVrmDRpEuLj4/Hrr7+ifPnyuH//PtLS0lChQgU4OTnpq3wiItIxTrolg5KXn1+9egXgrwmz6enpePLkCRo0aKA61svLCwsXLoSdnR22bdsGAHBzc0O9evUYVoiI3jEMLGRQBEHA06dP4ebmhh07dqie5qlQKODi4oKTJ0+qjjUxMUG9evVgamqKuLg4fZVMRESlgIGFDI5MJkP37t3x2WefYc+ePaq2Zs2a4fjx49i9e7fqWEEQUKlSJdja2kIURXCEk4jo3cQ5LKR3BT3M7enTp5g7dy5WrlyJXbt2oVevXnj+/Dn8/PyQmpqKZs2aoUWLFoiMjER4eDjOnz+PmjVr6ukOiIiopDGwkF7lvXE2IyMDubm5kMvlqn1PnjzBvHnzsGrVKuzcuRN9+vTB8+fPMX/+fJw5cwbPnj2DQqFAWFiY2twWIiJ69zCwkN7Fx8ejb9++KFeuHIYNGwaFQoEOHToAALKysjBp0iSsXr0a27dvx8cff4zXr19DEAQkJyejbNmysLKy0vMdEBFRSTP990OISo5SqcTGjRtx+fJlWFhYICUlBa9evYK9vT3ee+89DB48GIMGDYKDgwP69esHuVwOX19fAECFChX0XD0REZUW9rCQ3iUmJmLBggW4c+cOPDw8MHr0aGzZsgWnTp3ClStXYG9vj6pVqyImJgZPnz7FiRMn0Lp1a32XTUREpYg9LKR3CoUCU6ZMwbx583D69GlUr14dM2fOBACcP38eCQkJWLduHSpWrIinT5+ifPnyeq6YiIhKG3tYyGDkTbI9f/48evbsiS+//FK1LycnB0qlEqmpqahYsaIeqyQiIn1gYCGDkpiYiLlz5+LChQvo2bMnpk+fDgD53tBMRETGhYGFDE5eaLl06RLat2+P4OBgfZdERER6xifdksFRKBT46quvUL16dZw9exbPnz/Xd0lERKRn7GEhg5WUlAQAcHR01HMlRESkbwwsREREZPA4JEREREQGj4GFiIiIDB4DCxERERk8BhYiIiIyeAwsREREZPAYWIiIiMjgMbAQERGRwWNgITIyAQEB6Nmzp+pz27ZtMX78+FKv48SJExAEASkpKYUeIwgCIiIiinzOoKAgNGjQQKu67t+/D0EQEBsbq9V5iEi3GFiIDEBAQAAEQYAgCDAzM4OHhwdCQkLw+vXrEr/27t27MXv27CIdW5SQQURUEvj6WyID0bFjR2zYsAFZWVn46aefMHr0aJQpUwYzZszId2x2djbMzMx0cl17e3udnIeIqCSxh4XIQJibm0OhUMDV1RWff/45fHx8sHfvXgB/DePMnTsXzs7O8PT0BAA8evQIffv2ha2tLezt7dGjRw/cv39fdc7c3FxMnDgRtra2cHBwwNSpU/H22zjeHhLKysrCtGnT4OLiAnNzc3h4eGD9+vW4f/8+2rVrBwCws7ODIAgICAgAACiVSoSGhsLd3R2WlpaoX78+fvjhB7Xr/PTTT6hRowYsLS3Rrl07tTqLatq0aahRowbKli2LqlWrIjAwEDk5OfmO+/bbb+Hi4oKyZcuib9++SE1NVdv//fffw8vLCxYWFqhZsyZWr16tcS1EVLoYWIgMlKWlJbKzs1Wfjx07hri4OBw5cgT79+9HTk4OfH19YW1tjVOnTuHMmTMoV64cOnbsqPrekiVLsHHjRvznP//B6dOnkZycjB9//PEfrztw4ED873//Q1hYGG7evIlvv/0W5cqVg4uLC3bt2gUAiIuLw5MnT7BixQoAQGhoKMLDw7F27Vpcv34dEyZMwKeffoqTJ08CeBOsevfujW7duiE2NhZDhw7F9OnTNf4zsba2xsaNG3Hjxg2sWLEC3333HZYtW6Z2zO3bt7Fjxw7s27cPBw8exKVLlzBq1CjV/i1btmDmzJmYO3cubt68iXnz5iEwMBCbNm3SuB4iKkUiEemdv7+/2KNHD1EURVGpVIpHjhwRzc3NxcmTJ6v2Ozo6illZWarvbN68WfT09BSVSqWqLSsrS7S0tBQPHTokiqIoOjk5iQsXLlTtz8nJEStXrqy6liiKYps2bcQvvvhCFEVRjIuLEwGIR44cKbDOX375RQQgvnjxQtWWmZkpli1bVjx79qzasUOGDBE/+eQTURRFccaMGWKtWrXU9k+bNi3fud4GQPzxxx8L3b9o0SKxcePGqs+zZs0STUxMxN9//13V9vPPP4symUx88uSJKIqiWK1aNXHr1q1q55k9e7bo7e0tiqIo3rt3TwQgXrp0qdDrElHp4xwWIgOxf/9+lCtXDjk5OVAqlRgwYACCgoJU++vWras2b+Xy5cu4ffs2rK2t1c6TmZmJO3fuIDU1FU+ePEGzZs1U+0xNTdGkSZN8w0J5YmNjYWJigjZt2hS57tu3b+PVq1f48MMP1dqzs7PRsGFDAMDNmzfV6gAAb2/vIl8jz/bt2xEWFoY7d+4gPT0dr1+/hlwuVzumSpUqqFSpktp1lEol4uLiYG1tjTt37mDIkCEYNmyY6pjXr1/DxsZG43qIqPQwsBAZiHbt2mHNmjUwMzODs7MzTE3V/+9pZWWl9jk9PR2NGzfGli1b8p2rQoUKxarB0tJS4++kp6cDAA4cOKAWFIA383J0JSoqCn5+fggODoavry9sbGywbds2LFmyRONav/vuu3wBysTERGe1EpHuMbAQGQgrKyt4eHgU+fhGjRph+/btqFixYr5ehjxOTk44f/48WrduDeBNT0JMTAwaNWpU4PF169aFUqnEyZMn4ePjk29/Xg9Pbm6uqq1WrVowNzfHw4cPC+2Z8fLyUk0gznPu3Ll/v8m/OXv2LFxdXfHVV1+p2h48eJDvuIcPHyIhIQHOzs6q68hkMnh6esLR0RHOzs64e/cu/Pz8NLo+EekXJ90SSZSfnx/Kly+PHj164NSpU7h37x5OnDiBcePG4ffffwcAfPHFF5g/fz4iIiLw22+/YdSoUf/4DBU3Nzf4+/tj8ODBiIiIUJ1zx44dAABXV1cIgoD9+/fjjz/+QHp6OqytrTF58mRMmDABmzZtwp07d3Dx4kWsXLlSNZF15MiRiI+Px5QpUxAXF4etW7di48aNGt1v9erV8fDhQ2zbtg137txBWFhYgROILSws4O/vj8uXL+PUqVMYN24c+vbtC4VCAQAIDg5GaGgowsLCcOvWLVy9ehUbNmzA0qVLNaqHiEoXAwuRRJUtWxaRkZGoUqUKevfuDS8vLwwZMgSZmZmqHpdJkybhs88+g7+/P7y9vWFtbY1evXr943nXrFmDjz76CKNGjULNmjUxbNgwZGRkAAAqVaqE4OBgTJ8+HY6OjhgzZgwAYPbs2QgMDERoaCi8vLzQsWNHHDhwAO7u7gDezCvZtWsXIiIiUL9+faxduxbz5s3T6H67d++OCRMmYMyYMWjQoAHOnj2LwMDAfMd5eHigd+/e6Ny5Mzp06IB69eqpLVseOnQovv/+e2zYsAF169ZFmzZtsHHjRlWtRGSYBLGw2XdEREREBoI9LERERGTwGFiIiIjI4DGwEBERkcFjYCEiIiKDx8BCREREBo+BhYiIiAweAwsREREZPAYWIiIiMngMLERERGTwGFiIiIjI4DGwEBERkcH7P2s8fiiQWuupAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "if train_with_int:\n",
    "    print('model name: ', model_name)\n",
    "    # Load the model into an interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_Rescaled.tflite')\n",
    "    X_test_qat = X_test.astype('int8')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.int8 and y_test_qat.dtype == np.int8\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path='./saved_models/'+model_name+'_pqat_FullInt_FPInput.tflite')\n",
    "    X_test_qat = X_test.astype('float32')\n",
    "    y_test_qat = y_test.astype('int8')\n",
    "    assert X_test_qat.dtype == np.float32 and y_test_qat.dtype == np.int8\n",
    "\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_qat):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_qat, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned QAT model:  42582\n",
      "Size of the QAT model:  42508\n",
      "Size of the full-precision model:  120074\n",
      "The achieved compression ratio is 2.82x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "pqat_model_path = './saved_models/'+model_name+'_pqat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "qat_model_path = './saved_models/'+model_name+'_qat_FullInt_'+('Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "full_prec_model_path = './saved_models/'+model_name +('_Rescaled' if train_with_int else 'FPInput')+'.tflite'\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size(pqat_model_path))\n",
    "print('Size of the QAT model: ', get_gzipped_model_size(qat_model_path))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size(full_prec_model_path))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size(full_prec_model_path) / get_gzipped_model_size(pqat_model_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "#from torch import nn\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import train, test, plot_confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import flatten\n",
    "from tensorflow.keras.layers import Flatten, Softmax\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mac\n",
    "sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "#sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "#sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.4\n",
    "num_window_fall_data = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabel = label.astype(np.int8)\\ndata = data.reshape(data.shape[0], 50, 9)\\n# normalize the data\\n# Initialize the normalized data array\\nnormalized_data = np.zeros_like(data, dtype=float)\\n\\n# Normalize each channel using Min-Max scaling to [0, 1]\\nfor i in range(data.shape[2]):\\n    min_val = data[:, :, i].min()\\n    max_val = data[:, :, i].max()\\n    normalized_data[:, :, i] = (data[:, :, i] - min_val) / (max_val - min_val)\\n\\nprint(normalized_data.shape)\\nprint(normalized_data[0][0])\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "label = label.astype(np.int8)\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "# normalize the data\n",
    "# Initialize the normalized data array\n",
    "normalized_data = np.zeros_like(data, dtype=float)\n",
    "\n",
    "# Normalize each channel using Min-Max scaling to [0, 1]\n",
    "for i in range(data.shape[2]):\n",
    "    min_val = data[:, :, i].min()\n",
    "    max_val = data[:, :, i].max()\n",
    "    normalized_data[:, :, i] = (data[:, :, i] - min_val) / (max_val - min_val)\n",
    "\n",
    "print(normalized_data.shape)\n",
    "print(normalized_data[0][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert the normalized data to uint8\\nquantized_data = np.round(normalized_data * 255).astype(np.uint8)\\n\\nprint(quantized_data[0][0])\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Convert the normalized data to uint8\n",
    "quantized_data = np.round(normalized_data * 255).astype(np.uint8)\n",
    "\n",
    "print(quantized_data[0][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nB_size = (label == 0).sum()\\nA_size = (label == 1).sum()\\nprint(\\'B_size: \\', B_size)\\t\\nprint(\\'A_size: \\', A_size)\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(quantized_data, label, test_size=0.2, random_state=42)\\n\\n# Further split the training data into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\\n\\n#print(np.unique(y_train)) # [0 1]\\n#y_train = y_train.astype(np.int64)\\n#y_test = y_test.astype(np.int64)\\n\\n\\n# select the test data that is not zero\\nX_test_true = X_test[y_test != 0]\\ny_test_true = y_test[y_test != 0]\\n# length of the test data\\ntest_len = X_test_true.shape[0]\\nX_test_false = X_test[y_test == 0]\\ny_test_false = y_test[y_test == 0]\\n# X_test.shape:  (17, 50, 9)\\n# randomly len number of test data that is zero\\nindex = np.random.choice(X_test_false.shape[0], test_len, replace=False)\\n# X_test.shape:  (17, 50, 9)\\n# randomly len number of test data that is zero\\n#index = np.random.choice(X_test_false.shape[0], len, replace=False)\\n\\n\\nX_test_false = X_test[index]\\ny_test_false = y_test[index]\\n\\n# concatenate the true and false test data\\nX_test = np.concatenate((X_test_true, X_test_false), axis=0)\\ny_test = np.concatenate((y_test_true, y_test_false), axis=0)\\n#X_test = X_test[y_test != 0]\\n#y_test = y_test[y_test != 0]\\nprint(X_test.shape)\\n\\n\\n\\ndevice = (\\n     \"cuda\"\\n     if torch.cuda.is_available()\\n     else \"cpu\"\\n )\\n#device = \"cpu\"\\nprint(f\"Using {device} device\")\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(quantized_data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "#y_train = y_train.astype(np.int64)\n",
    "#y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "device = (\n",
    "     \"cuda\"\n",
    "     if torch.cuda.is_available()\n",
    "     else \"cpu\"\n",
    " )\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [-1.07000000e-01 -3.40000000e-02  1.01200000e+00  3.43774800e-01\n",
      "  1.14591600e-01  0.00000000e+00  1.77725842e+02 -6.10200270e+00\n",
      " -2.71295613e+01]\n",
      "B_size:  25020\n",
      "A_size:  547\n",
      "data:  [-1.07000000e-01 -3.40000000e-02  1.01200000e+00  3.43774800e-01\n",
      "  1.14591600e-01  0.00000000e+00  1.77725842e+02 -6.10200270e+00\n",
      " -2.71295613e+01]\n",
      "(216, 50, 9)\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "label = label.astype(np.int64)\n",
    "# one-hot encoding\n",
    "#label = to_categorical(label, num_classes=2)\n",
    "# transpose the data to (batch_size, sequence_length, in_channels)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "# normalize the data\n",
    "# Initialize a new scaling object for normalizing input data\n",
    "# Z-score normalization\n",
    "\n",
    "print('data: ', data[0][0])\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "# transpose the data to (batch_size, in_channels, sequence_length)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "print('data: ', data[0][0])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "device = (\n",
    "     \"cuda\"\n",
    "     if torch.cuda.is_available()\n",
    "     else \"cpu\"\n",
    " )\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(50, 9))\n",
    "x = layers.Reshape((1, 50, 9))(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.MaxPooling2D(pool_size=(1, 2))(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.AveragePooling2D(pool_size=(1, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "ResNet24 = keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "\n",
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 1, 50, 9)             0         ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 1, 48, 64)            1792      ['reshape_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 1, 46, 64)            12352     ['conv2d_81[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 23, 64)            0         ['conv2d_82[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 1, 23, 16)            1040      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 1, 23, 16)            64        ['conv2d_84[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_63 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_63[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 1, 23, 16)            64        ['conv2d_85[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_64 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_64[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 1, 23, 64)            256       ['conv2d_86[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 1, 23, 64)            4160      ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " add_21 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_65[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_83[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_65 (ReLU)             (None, 1, 23, 64)            0         ['add_21[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_65[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_66 (Ba  (None, 1, 23, 16)            64        ['conv2d_88[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_66 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_66[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_67 (Ba  (None, 1, 23, 16)            64        ['conv2d_89[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_67 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_67[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_68 (Ba  (None, 1, 23, 64)            256       ['conv2d_90[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_65[0][0]']            \n",
      "                                                                                                  \n",
      " add_22 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_68[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_87[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_68 (ReLU)             (None, 1, 23, 64)            0         ['add_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_68[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_69 (Ba  (None, 1, 23, 16)            64        ['conv2d_91[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_69 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_69[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_70 (Ba  (None, 1, 23, 16)            64        ['conv2d_92[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_70 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_70[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_71 (Ba  (None, 1, 23, 64)            256       ['conv2d_93[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_23 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_71[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_68[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_71 (ReLU)             (None, 1, 23, 64)            0         ['add_23[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_71[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_72 (Ba  (None, 1, 23, 16)            64        ['conv2d_95[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_72 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_72[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_73 (Ba  (None, 1, 23, 16)            64        ['conv2d_96[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_73 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_73[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_74 (Ba  (None, 1, 23, 64)            256       ['conv2d_97[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_71[0][0]']            \n",
      "                                                                                                  \n",
      " add_24 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_74[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_94[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_74 (ReLU)             (None, 1, 23, 64)            0         ['add_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_74[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_75 (Ba  (None, 1, 23, 16)            64        ['conv2d_98[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_75 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_75[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_76 (Ba  (None, 1, 23, 16)            64        ['conv2d_99[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_76 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_76[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_77 (Ba  (None, 1, 23, 64)            256       ['conv2d_100[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_25 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_77[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_74[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_77 (ReLU)             (None, 1, 23, 64)            0         ['add_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_77[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_78 (Ba  (None, 1, 23, 16)            64        ['conv2d_102[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_78 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_78[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_78[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_79 (Ba  (None, 1, 23, 16)            64        ['conv2d_103[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_79 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_79[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_80 (Ba  (None, 1, 23, 64)            256       ['conv2d_104[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 1, 23, 64)            4160      ['re_lu_77[0][0]']            \n",
      "                                                                                                  \n",
      " add_26 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_80[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_101[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_80 (ReLU)             (None, 1, 23, 64)            0         ['add_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_80[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_81 (Ba  (None, 1, 23, 16)            64        ['conv2d_105[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_81 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_81[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_81[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_82 (Ba  (None, 1, 23, 16)            64        ['conv2d_106[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_82 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_82[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_82[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_83 (Ba  (None, 1, 23, 64)            256       ['conv2d_107[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_83[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_80[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_83 (ReLU)             (None, 1, 23, 64)            0         ['add_27[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (Avera  (None, 1, 11, 64)            0         ['re_lu_83[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 704)                  0         ['average_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 2)                    1410      ['flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55266 (215.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 1344 (5.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: \\n\")\n",
    "ResNet24.build(input_shape=(None, 50, 9))\n",
    "ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16362, 2)\n",
      "y_val.shape:  (4091, 2)\n",
      "X_train.shape:  (16362, 50, 9)\n",
      "y_train.shape:  (16362, 2)\n",
      "Epoch 1/50\n",
      "256/256 [==============================] - 9s 23ms/step - loss: 1.4780 - accuracy: 0.8450 - val_loss: 0.3963 - val_accuracy: 0.8968 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.8205 - accuracy: 0.8643 - val_loss: 0.1434 - val_accuracy: 0.9550 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.6366 - accuracy: 0.8797 - val_loss: 0.4919 - val_accuracy: 0.8240 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.6881 - accuracy: 0.8759 - val_loss: 0.4134 - val_accuracy: 0.8856 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.5659 - accuracy: 0.8950 - val_loss: 0.3123 - val_accuracy: 0.8817 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.5619 - accuracy: 0.8909 - val_loss: 0.4887 - val_accuracy: 0.8543 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "254/256 [============================>.] - ETA: 0s - loss: 0.5309 - accuracy: 0.8946\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.5319 - accuracy: 0.8945 - val_loss: 0.2741 - val_accuracy: 0.9044 - lr: 5.0000e-04\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "\n",
    "ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = ResNet24.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (216, 50, 9)\n",
      "7/7 - 0s - loss: 0.2593 - accuracy: 0.9120 - 32ms/epoch - 5ms/step\n",
      "Test loss: [0.25931084156036377, 0.9120370149612427]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = ResNet24.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2def79100>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step\n",
      "[[ 97   8]\n",
      " [ 11 100]]\n",
      "Confusion matrix, without normalization\n",
      "[[ 97   8]\n",
      " [ 11 100]]\n",
      "accuracy:  0.9120370370370371\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ResNet24.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp0gg2u2kp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp0gg2u2kp/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "2023-12-08 01:51:50.775316: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 01:51:50.775458: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 01:51:50.777933: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp0gg2u2kp\n",
      "2023-12-08 01:51:50.788376: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 01:51:50.788393: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp0gg2u2kp\n",
      "2023-12-08 01:51:50.822463: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 01:51:51.248440: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp0gg2u2kp\n",
      "2023-12-08 01:51:51.347982: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 570057 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231988"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet24.save('saved_models/ResNet24.keras')  # The file needs to end with the .keras extension\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ResNet24)\n",
    "ResNet24_tflite = converter.convert()\n",
    "# save the model\n",
    "open(\"saved_models/ResNet24.tflite\", \"wb\").write(ResNet24_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_4 (Quantize  (None, 50, 9)                3         ['input_4[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_3 (QuantizeW  (None, 1, 50, 9)             1         ['quantize_layer_4[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_81 (QuantizeW  (None, 1, 48, 64)            1923      ['quant_reshape_3[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_82 (QuantizeW  (None, 1, 46, 64)            12483     ['quant_conv2d_81[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_3 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_82[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_84 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_84[0][0]']     \n",
      " 63 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_63 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_63\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_85 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_63[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_85[0][0]']     \n",
      " 64 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_64 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_64\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_86 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_64[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_86[0][0]']     \n",
      " 65 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_83 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_21 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_65\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_83[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_65 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_21[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_88 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_65[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_88[0][0]']     \n",
      " 66 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_66 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_66\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_89 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_66[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_89[0][0]']     \n",
      " 67 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_67 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_67\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_90 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_67[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_90[0][0]']     \n",
      " 68 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_87 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_65[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_22 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_68\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_87[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_68 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_22[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_91 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_91[0][0]']     \n",
      " 69 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_69 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_69\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_92 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_69[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_92[0][0]']     \n",
      " 70 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_70 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_70\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_93 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_70[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_93[0][0]']     \n",
      " 71 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_23 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_71\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_68[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_71 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_23[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_95 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_95[0][0]']     \n",
      " 72 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_72 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_72\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_96 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_72[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_96[0][0]']     \n",
      " 73 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_73 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_73\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_97 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_73[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_97[0][0]']     \n",
      " 74 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_94 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_24 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_74\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_94[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_74 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_24[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_98 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_98[0][0]']     \n",
      " 75 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_75 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_75\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_99 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_75[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_99[0][0]']     \n",
      " 76 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_76 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_76\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_100 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_76[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_100[0][0]']    \n",
      " 77 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_25 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_77\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_74[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_77 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_25[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_102 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_77[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_102[0][0]']    \n",
      " 78 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_78 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_78\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_103 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_78[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_103[0][0]']    \n",
      " 79 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_79 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_79\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_104 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_79[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_104[0][0]']    \n",
      " 80 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_101 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_77[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_26 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_80\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_101[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_80 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_26[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_105 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_105[0][0]']    \n",
      " 81 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_81 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_81\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_106 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_81[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_106[0][0]']    \n",
      " 82 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_82 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_82\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_107 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_82[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_107[0][0]']    \n",
      " 83 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_27 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_83\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_80[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_83 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_27[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_3   (None, 1, 11, 64)            3         ['quant_re_lu_83[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_3 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_3[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_3 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_3[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_ResNet24 = tfmot.quantization.keras.quantize_model(ResNet24)\n",
    "q_ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "256/256 [==============================] - 13s 29ms/step - loss: 0.9134 - accuracy: 0.7950 - val_loss: 0.3259 - val_accuracy: 0.8533 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.7345 - accuracy: 0.8355 - val_loss: 0.6014 - val_accuracy: 0.7697 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.6371 - accuracy: 0.8651 - val_loss: 0.2597 - val_accuracy: 0.9113 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.5623 - accuracy: 0.8798 - val_loss: 0.3060 - val_accuracy: 0.8814 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.5235 - accuracy: 0.8839 - val_loss: 0.2986 - val_accuracy: 0.8942 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.4174 - accuracy: 0.9061 - val_loss: 0.1939 - val_accuracy: 0.9259 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.4978 - accuracy: 0.8956 - val_loss: 0.1653 - val_accuracy: 0.9367 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 8s 29ms/step - loss: 0.4894 - accuracy: 0.8905 - val_loss: 0.3022 - val_accuracy: 0.8954 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 0.4302 - accuracy: 0.9045 - val_loss: 0.2175 - val_accuracy: 0.9242 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.3973 - accuracy: 0.9138 - val_loss: 0.4946 - val_accuracy: 0.8399 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.4284 - accuracy: 0.9077 - val_loss: 0.1557 - val_accuracy: 0.9494 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 8s 32ms/step - loss: 0.3672 - accuracy: 0.9222 - val_loss: 0.3285 - val_accuracy: 0.9252 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.4927 - accuracy: 0.9003 - val_loss: 0.4127 - val_accuracy: 0.8678 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.4123 - accuracy: 0.9108 - val_loss: 0.4238 - val_accuracy: 0.8286 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.4334 - accuracy: 0.9153 - val_loss: 0.3092 - val_accuracy: 0.9103 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.4447 - accuracy: 0.9159\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "256/256 [==============================] - 8s 31ms/step - loss: 0.4453 - accuracy: 0.9158 - val_loss: 0.4773 - val_accuracy: 0.9025 - lr: 5.0000e-04\n",
      "Epoch 16: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_ResNet24.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ResNet24.save('q_ResNet24.keras')  # The file needs to end with the .keras extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef representative_data_gen():\\n  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\\n    yield [input_value]\\n\\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\nconverter.representative_dataset = representative_data_gen\\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\nconverter._experimental_lower_tensor_list_ops = False\\nconverter.inference_input_type = tf.int8\\nconverter.inference_output_type = tf.int8\\n\\ntflite_model_quant_int8 = converter.convert()\\n\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model_quant_int8 = converter.convert()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi8i_fp7f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi8i_fp7f/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-08 01:42:57.716011: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 01:42:57.716031: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 01:42:57.716270: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi8i_fp7f\n",
      "2023-12-08 01:42:57.726514: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 01:42:57.726533: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi8i_fp7f\n",
      "2023-12-08 01:42:57.753023: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 01:42:58.178929: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpi8i_fp7f\n",
      "2023-12-08 01:42:58.296072: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 579810 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "\n",
    "# Convert the model\n",
    "tflite_model_quant_int8_qat = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105616"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"saved_models/ResNet24_quant_int8_qat.tflite\", \"wb\").write(tflite_model_quant_int8_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'name': 'serving_default_input_4:0', 'index': 0, 'shape': array([ 1, 50,  9], dtype=int32), 'shape_signature': array([-1, 50,  9], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 106, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n"
     ]
    }
   ],
   "source": [
    "# test the quantized model\n",
    "X_test_int8 = X_test.astype('float32')\n",
    "y_test_int8 = y_test.astype('int8')\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= tflite_model_quant_int8_qat)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_int8, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model size with 8-bit quantization: 103 KB\n",
      "TFLite Model size without quantization: 226 KB\n",
      "\n",
      "Reduction in model size by a factor of 2.196523\n",
      "accuracy:  0.9120370370370371\n",
      "Confusion matrix, without normalization\n",
      "[[ 97   8]\n",
      " [ 11 100]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_in_kb = os.path.getsize('saved_models/ResNet24.tflite') / 1024\n",
    "# ResNet24_tflite tflite_quant_in_kb\n",
    "tflite_quant_in_kb = os.path.getsize('saved_models/ResNet24_quant_int8_qat.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (tflite_in_kb / tflite_quant_in_kb))\n",
    "\n",
    "accuracy = (predictions == gt).mean()\n",
    "print('accuracy: ', accuracy)\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of full precision model:  0.9120370370370371\n",
      "accuracy of quantized model:  0.9120370370370371\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of full precision model: ', accuracy_fp)\n",
    "print('accuracy of quantized model: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning + QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 9)             1         ['input_4[0][0]']             \n",
      " e_3 (PruneLowMagnitude)                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            3522      ['prune_low_magnitude_reshape_\n",
      " _81 (PruneLowMagnitude)                                            3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 46, 64)            24642     ['prune_low_magnitude_conv2d_8\n",
      " _82 (PruneLowMagnitude)                                            1[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 23, 64)            1         ['prune_low_magnitude_conv2d_8\n",
      " oling2d_3 (PruneLowMagnitu                                         2[0][0]']                     \n",
      " de)                                                                                              \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _84 (PruneLowMagnitude)                                            ing2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_63 (PruneLow                                         4[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 63 (PruneLowMagnitude)                                             rmalization_63[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_63\n",
      " _85 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_64 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 64 (PruneLowMagnitude)                                             rmalization_64[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_64\n",
      " _86 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_8\n",
      " normalization_65 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _83 (PruneLowMagnitude)                                            ing2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_21  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_65[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_8\n",
      "                                                                    3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_21[0\n",
      " 65 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_65\n",
      " _88 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_66 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 66 (PruneLowMagnitude)                                             rmalization_66[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_66\n",
      " _89 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_67 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 67 (PruneLowMagnitude)                                             rmalization_67[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_67\n",
      " _90 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_68 (PruneLow                                         0[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_65\n",
      " _87 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_22  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_68[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_8\n",
      "                                                                    7[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_22[0\n",
      " 68 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_68\n",
      " _91 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_69 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 69 (PruneLowMagnitude)                                             rmalization_69[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_69\n",
      " _92 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_70 (PruneLow                                         2[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 70 (PruneLowMagnitude)                                             rmalization_70[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_70\n",
      " _93 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_71 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_23  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_71[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_68\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_23[0\n",
      " 71 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_71\n",
      " _95 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_72 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 72 (PruneLowMagnitude)                                             rmalization_72[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_72\n",
      " _96 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_73 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 73 (PruneLowMagnitude)                                             rmalization_73[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_73\n",
      " _97 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_74 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_71\n",
      " _94 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_24  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_74[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_9\n",
      "                                                                    4[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_24[0\n",
      " 74 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_74\n",
      " _98 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_75 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 75 (PruneLowMagnitude)                                             rmalization_75[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_75\n",
      " _99 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_9\n",
      " normalization_76 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 76 (PruneLowMagnitude)                                             rmalization_76[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_76\n",
      " _100 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_77 (PruneLow                                         00[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_25  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_77[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_74\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_25[0\n",
      " 77 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_77\n",
      " _102 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_78 (PruneLow                                         02[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 78 (PruneLowMagnitude)                                             rmalization_78[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_78\n",
      " _103 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_79 (PruneLow                                         03[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 79 (PruneLowMagnitude)                                             rmalization_79[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_79\n",
      " _104 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_80 (PruneLow                                         04[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_77\n",
      " _101 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_26  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_80[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    01[0][0]']                    \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_26[0\n",
      " 80 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_80\n",
      " _105 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_81 (PruneLow                                         05[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 81 (PruneLowMagnitude)                                             rmalization_81[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_81\n",
      " _106 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_82 (PruneLow                                         06[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 82 (PruneLowMagnitude)                                             rmalization_82[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_82\n",
      " _107 (PruneLowMagnitude)                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_83 (PruneLow                                         07[0][0]']                    \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_27  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      "  (PruneLowMagnitude)                                               rmalization_83[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_80\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_27[0\n",
      " 83 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 11, 64)            1         ['prune_low_magnitude_re_lu_83\n",
      " e_pooling2d_3 (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 704)                  1         ['prune_low_magnitude_average_\n",
      " n_3 (PruneLowMagnitude)                                            pooling2d_3[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense_  (None, 2)                    2820      ['prune_low_magnitude_flatten_\n",
      " 3 (PruneLowMagnitude)                                              3[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 106895 (417.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 52973 (207.24 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "\n",
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(ResNet24, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "256/256 [==============================] - 14s 24ms/step - loss: 0.4525 - accuracy: 0.9170 - val_loss: 0.2169 - val_accuracy: 0.9235 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.3272 - accuracy: 0.9293 - val_loss: 0.3437 - val_accuracy: 0.8770 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 6s 22ms/step - loss: 0.3194 - accuracy: 0.9350 - val_loss: 0.6370 - val_accuracy: 0.8062 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.4674 - accuracy: 0.9157 - val_loss: 0.1649 - val_accuracy: 0.9474 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.3849 - accuracy: 0.9232 - val_loss: 0.2501 - val_accuracy: 0.9174 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 6s 25ms/step - loss: 0.3591 - accuracy: 0.9268 - val_loss: 0.2852 - val_accuracy: 0.9032 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 0.2551 - accuracy: 0.9433 - val_loss: 0.1966 - val_accuracy: 0.9448 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.2186 - accuracy: 0.9484 - val_loss: 0.1242 - val_accuracy: 0.9553 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 0.2042 - accuracy: 0.9534 - val_loss: 0.1291 - val_accuracy: 0.9562 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1740 - accuracy: 0.9608 - val_loss: 0.1120 - val_accuracy: 0.9582 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1568 - accuracy: 0.9628 - val_loss: 0.2468 - val_accuracy: 0.9152 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1452 - accuracy: 0.9639 - val_loss: 0.0913 - val_accuracy: 0.9655 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1509 - accuracy: 0.9669 - val_loss: 0.1154 - val_accuracy: 0.9582 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1489 - accuracy: 0.9660 - val_loss: 0.1927 - val_accuracy: 0.9345 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 6s 24ms/step - loss: 0.1556 - accuracy: 0.9636 - val_loss: 0.0741 - val_accuracy: 0.9734 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1688 - accuracy: 0.9615 - val_loss: 0.1686 - val_accuracy: 0.9518 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1731 - accuracy: 0.9628 - val_loss: 0.2115 - val_accuracy: 0.9333 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1854 - accuracy: 0.9608 - val_loss: 0.1344 - val_accuracy: 0.9587 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1581 - accuracy: 0.9649 - val_loss: 0.0856 - val_accuracy: 0.9716 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.1602 - accuracy: 0.9621\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "256/256 [==============================] - 6s 23ms/step - loss: 0.1600 - accuracy: 0.9621 - val_loss: 0.1676 - val_accuracy: 0.9482 - lr: 5.0000e-04\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a5c04fa0>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, pruning_callbacks.UpdatePruningStep()],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.2559881806373596\n",
      "Pruned model accuracy:  0.9351851940155029\n",
      "Full-precision model accuracy:  0.9120370370370371\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# save the model\n",
    "pruned_model_unstructured.save('saved_models/ResNet24_pruned_unstructured.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp77f5cbj3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp77f5cbj3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: /Users/liuxinqing/Documents/Fall_Detection/saved_models/ResNet24_pruned_unstructured.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 02:17:15.845266: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 02:17:15.845433: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 02:17:15.846747: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp77f5cbj3\n",
      "2023-12-08 02:17:15.853834: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 02:17:15.853846: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp77f5cbj3\n",
      "2023-12-08 02:17:15.872969: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 02:17:16.035216: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmp77f5cbj3\n",
      "2023-12-08 02:17:16.089268: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 242533 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = 'saved_models/ResNet24_pruned_unstructured.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to actually compress the models via gzip and measure the zipped size.\n",
    "import tempfile\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "    # It returns the size of the gzipped model in bytes.\n",
    "    import os\n",
    "    import zipfile\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  128005\n",
      "Size of the full-precision model:  201853\n",
      "The achieved compression ratio is 1.00x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(pruned_tflite_file_unstructured))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('saved_models/ResNet24.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('saved_models/ResNet24_pruned_unstructured.tflite') / get_gzipped_model_size(pruned_tflite_file_unstructured)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_6 (Quantize  (None, 50, 9)                3         ['input_4[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_3 (QuantizeW  (None, 1, 50, 9)             1         ['quantize_layer_6[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_81 (QuantizeW  (None, 1, 48, 64)            1923      ['quant_reshape_3[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_82 (QuantizeW  (None, 1, 46, 64)            12483     ['quant_conv2d_81[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_3 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_82[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_84 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_84[0][0]']     \n",
      " 63 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_63 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_63\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_85 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_63[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_85[0][0]']     \n",
      " 64 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_64 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_64\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_86 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_64[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_86[0][0]']     \n",
      " 65 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_83 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_3[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_21 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_65\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_83[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_65 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_21[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_88 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_65[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_88[0][0]']     \n",
      " 66 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_66 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_66\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_89 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_66[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_89[0][0]']     \n",
      " 67 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_67 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_67\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_90 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_67[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_90[0][0]']     \n",
      " 68 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_87 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_65[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_22 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_68\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_87[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_68 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_22[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_91 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_68[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_91[0][0]']     \n",
      " 69 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_69 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_69\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_92 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_69[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_92[0][0]']     \n",
      " 70 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_70 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_70\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_93 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_70[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_93[0][0]']     \n",
      " 71 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_23 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_71\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_68[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_71 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_23[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_95 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_95[0][0]']     \n",
      " 72 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_72 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_72\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_96 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_72[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_96[0][0]']     \n",
      " 73 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_73 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_73\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_97 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_73[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_97[0][0]']     \n",
      " 74 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_94 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_71[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_24 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_74\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_94[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_74 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_24[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_98 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_74[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_98[0][0]']     \n",
      " 75 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_75 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_75\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_99 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_75[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_99[0][0]']     \n",
      " 76 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_76 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_76\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_100 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_76[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_100[0][0]']    \n",
      " 77 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_25 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_77\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_74[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_77 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_25[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_102 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_77[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_102[0][0]']    \n",
      " 78 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_78 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_78\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_103 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_78[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_103[0][0]']    \n",
      " 79 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_79 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_79\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_104 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_79[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_104[0][0]']    \n",
      " 80 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_101 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_77[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_26 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_80\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_101[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_80 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_26[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_105 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_80[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_105[0][0]']    \n",
      " 81 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_81 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_81\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_106 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_81[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_106[0][0]']    \n",
      " 82 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_82 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_82\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_107 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_82[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_107[0][0]']    \n",
      " 83 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_27 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_83\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_80[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_83 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_27[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_3   (None, 1, 11, 64)            3         ['quant_re_lu_83[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_3 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_3[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_3 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_3[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (16362, 50, 9)\n",
      "y_train.shape:  (16362, 2)\n",
      "64\n",
      "Epoch 1/50\n",
      "256/256 [==============================] - 11s 29ms/step - loss: 1.0197 - accuracy: 0.8261 - val_loss: 0.1878 - val_accuracy: 0.9186 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.5408 - accuracy: 0.8902 - val_loss: 0.2299 - val_accuracy: 0.9181 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.4254 - accuracy: 0.9095 - val_loss: 0.1578 - val_accuracy: 0.9440 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.3731 - accuracy: 0.9229 - val_loss: 0.2064 - val_accuracy: 0.9291 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.2628 - accuracy: 0.9400 - val_loss: 0.1617 - val_accuracy: 0.9477 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 8s 29ms/step - loss: 0.2337 - accuracy: 0.9487 - val_loss: 0.1670 - val_accuracy: 0.9467 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 0.1769 - accuracy: 0.9587 - val_loss: 0.1206 - val_accuracy: 0.9646 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.1635 - accuracy: 0.9633 - val_loss: 0.1414 - val_accuracy: 0.9555 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 0.1471 - accuracy: 0.9667 - val_loss: 0.1074 - val_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 0.1400 - accuracy: 0.9697 - val_loss: 0.1096 - val_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.1160 - accuracy: 0.9720 - val_loss: 0.0929 - val_accuracy: 0.9675 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 8s 30ms/step - loss: 0.1012 - accuracy: 0.9768 - val_loss: 0.1536 - val_accuracy: 0.9545 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.0915 - accuracy: 0.9798 - val_loss: 0.0814 - val_accuracy: 0.9760 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.1152 - accuracy: 0.9756 - val_loss: 0.1585 - val_accuracy: 0.9553 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 0.1629 - accuracy: 0.9658 - val_loss: 0.1127 - val_accuracy: 0.9638 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 7s 28ms/step - loss: 0.1404 - accuracy: 0.9716 - val_loss: 0.1267 - val_accuracy: 0.9602 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.1423 - accuracy: 0.9718 - val_loss: 0.1029 - val_accuracy: 0.9707 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9784\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "256/256 [==============================] - 7s 29ms/step - loss: 0.1070 - accuracy: 0.9784 - val_loss: 0.1076 - val_accuracy: 0.9677 - lr: 5.0000e-04\n",
      "Epoch 18: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x39eaa5dc0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "print(batch_size)\n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, pruning_callbacks.UpdatePruningStep()],\n",
    "            class_weight=class_weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned QAT model loss:  0.22265245020389557\n",
      "Pruned QAT model accuracy:  0.9351851940155029\n",
      "Full-precision model accuracy:  0.9120370370370371\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_loss, pruned_qat_acc = pruned_qat_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned QAT model loss: ', pruned_qat_loss)\n",
    "print('Pruned QAT model accuracy: ', pruned_qat_acc)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_qat_tflite_file = 'models/pruned_qat_model.tflite'\n",
    "\n",
    "with open(pruned_qat_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_qat_tflite_model)\n",
    "\n",
    "print('Saved pruned QAT TFLite model to:', os.path.abspath(pruned_qat_tflite_file))\n",
    "\n",
    "# write TFLite model to a C source (or header) file\n",
    "#c_model_name = 'pruned_qat_fmnist'\n",
    "\n",
    "#with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "#    file.write(hex_to_c_array(pruned_qat_tflite_model, c_model_name))\n",
    "\n",
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size(pruned_qat_tflite_file))\n",
    "print('Size of th QAT model: ', get_gzipped_model_size( 'models/fmnist_qat_int8.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('models/fmnist_model_f32.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('models/fmnist_model_f32.tflite') / get_gzipped_model_size(pruned_qat_tflite_file)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

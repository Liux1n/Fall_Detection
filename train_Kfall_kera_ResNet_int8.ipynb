{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "#from torch import nn\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import train, test, plot_confusion_matrix, get_gzipped_model_size\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import flatten\n",
    "from tensorflow.keras.layers import Flatten, Softmax\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mac\n",
    "sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "#sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "#sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.4\n",
    "num_window_fall_data = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_size:  25020\n",
      "A_size:  591\n",
      "(226, 50, 9)\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "label = label.astype(np.int64)\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "device = (\n",
    "     \"cuda\"\n",
    "     if torch.cuda.is_available()\n",
    "     else \"cpu\"\n",
    " )\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(50, 9))\n",
    "x = layers.Reshape((1, 50, 9))(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.MaxPooling2D(pool_size=(1, 2))(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.AveragePooling2D(pool_size=(1, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "ResNet24 = keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "\n",
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 50, 9)             0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 1, 48, 64)            1792      ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 1, 46, 64)            12352     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 1, 23, 64)            0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 1, 23, 16)            1040      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1, 23, 16)            64        ['conv2d_3[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 1, 23, 16)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 1, 23, 16)            784       ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1, 23, 16)            64        ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 1, 23, 64)            1088      ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1, 23, 64)            256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 1, 23, 64)            4160      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1, 23, 64)            0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'conv2d_2[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 1, 23, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 1, 23, 16)            1040      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1, 23, 16)            64        ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 1, 23, 16)            784       ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1, 23, 16)            64        ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 1, 23, 64)            1088      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1, 23, 64)            256       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 1, 23, 64)            4160      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'conv2d_6[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 1, 23, 64)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1, 23, 16)            64        ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1, 23, 16)            64        ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1, 23, 64)            256       ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    , 're_lu_5[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 1, 23, 64)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1, 23, 16)            64        ['conv2d_14[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1, 23, 16)            64        ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1, 23, 64)            256       ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 1, 23, 64)            0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 1, 23, 16)            64        ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 1, 23, 16)            64        ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 1, 23, 64)            256       ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_14[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 1, 23, 64)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 1, 23, 16)            64        ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 1, 23, 16)            64        ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 1, 23, 64)            256       ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 1, 23, 64)            0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 1, 23, 16)            64        ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1, 23, 16)            64        ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_19[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 1, 23, 64)            256       ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)             (None, 1, 23, 64)            0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 1, 11, 64)            0         ['re_lu_20[0][0]']            \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 704)                  0         ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    1410      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55266 (215.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 1344 (5.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: \\n\")\n",
    "ResNet24.build(input_shape=(None, 50, 9))\n",
    "ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16390, 2)\n",
      "y_val.shape:  (4098, 2)\n",
      "X_train.shape:  (16390, 50, 9)\n",
      "y_train.shape:  (16390, 2)\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 9s 24ms/step - loss: 1.4014 - accuracy: 0.8120 - val_loss: 0.3970 - val_accuracy: 0.9053 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.7543 - accuracy: 0.8674 - val_loss: 0.1939 - val_accuracy: 0.9429 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.7467 - accuracy: 0.8730 - val_loss: 0.2654 - val_accuracy: 0.9192 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.5077 - accuracy: 0.9011 - val_loss: 0.3981 - val_accuracy: 0.8736 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.6026 - accuracy: 0.8907 - val_loss: 0.3155 - val_accuracy: 0.8982 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.5158 - accuracy: 0.9012 - val_loss: 0.2176 - val_accuracy: 0.9283 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.5840 - accuracy: 0.8915 - val_loss: 0.1606 - val_accuracy: 0.9527 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.4289 - accuracy: 0.9148 - val_loss: 0.4358 - val_accuracy: 0.8953 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.4352 - accuracy: 0.9113 - val_loss: 0.3438 - val_accuracy: 0.8987 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.4076 - accuracy: 0.9155 - val_loss: 0.2997 - val_accuracy: 0.9122 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.3652 - accuracy: 0.9206 - val_loss: 0.5511 - val_accuracy: 0.8458 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.9223\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.3995 - accuracy: 0.9223 - val_loss: 0.1981 - val_accuracy: 0.9395 - lr: 5.0000e-04\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "\n",
    "ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = ResNet24.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (226, 50, 9)\n",
      "8/8 - 0s - loss: 0.3696 - accuracy: 0.8982 - 61ms/epoch - 8ms/step\n",
      "Test loss: [0.36956554651260376, 0.8982300758361816]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = ResNet24.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2e0b11cd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "[[ 99  10]\n",
      " [ 13 104]]\n",
      "Confusion matrix, without normalization\n",
      "[[ 99  10]\n",
      " [ 13 104]]\n",
      "accuracy:  0.8982300884955752\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAHpCAYAAABDZnwKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABI00lEQVR4nO3deVxU9f7H8fcAsohsooIkAi655JZlhrtpmldT0zLLClzrqpn7UtcNF9Lc0lyyum7XfqZllto1Tcs9U3PP3E1T0VIBl0CE8/vDy9SEFsjAzHFezx7n8WjOOXPOZ7hcfff5fr9nLIZhGAIAAHBSbo4uAAAA4K8QVgAAgFMjrAAAAKdGWAEAAE6NsAIAAJwaYQUAADg1wgoAAHBqhBUAAODUCCsAAMCpEVYAkzty5IiaNGmigIAAWSwWLVu2zK7XP3nypCwWi+bOnWvX65pZgwYN1KBBA0eXAbgMwgpgB8eOHdPLL7+sUqVKydvbW/7+/qpdu7befvtt/fbbb3l675iYGO3bt09jxozRggUL9PDDD+fp/fJTbGysLBaL/P39b/tzPHLkiCwWiywWiyZMmJDj6589e1YjRozQ7t277VAtgLzi4egCALNbuXKlnnnmGXl5eemll15SpUqVdOPGDW3atEkDBgzQgQMHNHv27Dy592+//aatW7fqjTfeUM+ePfPkHhEREfrtt99UoECBPLn+3/Hw8ND169e1fPlytWvXzubYwoUL5e3trZSUlLu69tmzZzVy5EhFRkaqWrVq2X7f6tWr7+p+AO4OYQXIhRMnTqh9+/aKiIjQunXrVLx4ceuxHj166OjRo1q5cmWe3f+XX36RJAUGBubZPSwWi7y9vfPs+n/Hy8tLtWvX1v/93/9lCSsffvihmjdvrk8++SRfarl+/boKFiwoT0/PfLkfgFsYBgJyYfz48bp69ao++OADm6CSqUyZMnrttdesr2/evKlRo0apdOnS8vLyUmRkpF5//XWlpqbavC8yMlItWrTQpk2b9Mgjj8jb21ulSpXS/PnzreeMGDFCERERkqQBAwbIYrEoMjJS0q3hk8x//6MRI0bIYrHY7FuzZo3q1KmjwMBAFSpUSOXKldPrr79uPX6nOSvr1q1T3bp15evrq8DAQLVq1UoHDx687f2OHj2q2NhYBQYGKiAgQB07dtT169fv/IP9k+eff17//e9/lZiYaN23fft2HTlyRM8//3yW8y9duqT+/furcuXKKlSokPz9/dWsWTPt2bPHes4333yjGjVqSJI6duxoHU7K/JwNGjRQpUqVtHPnTtWrV08FCxa0/lz+PGclJiZG3t7eWT5/06ZNFRQUpLNnz2b7swLIirAC5MLy5ctVqlQp1apVK1vnd+nSRcOGDVP16tU1efJk1a9fX/Hx8Wrfvn2Wc48ePaqnn35ajz/+uCZOnKigoCDFxsbqwIEDkqQ2bdpo8uTJkqTnnntOCxYs0JQpU3JU/4EDB9SiRQulpqYqLi5OEydOVMuWLbV58+a/fN9XX32lpk2b6sKFCxoxYoT69u2rLVu2qHbt2jp58mSW89u1a6crV64oPj5e7dq109y5czVy5Mhs19mmTRtZLBYtXbrUuu/DDz9U+fLlVb169SznHz9+XMuWLVOLFi00adIkDRgwQPv27VP9+vWtwaFChQqKi4uTJHXr1k0LFizQggULVK9ePet1Ll68qGbNmqlatWqaMmWKGjZseNv63n77bRUtWlQxMTFKT0+XJL377rtavXq1pk2bprCwsGx/VgC3YQC4K0lJSYYko1WrVtk6f/fu3YYko0uXLjb7+/fvb0gy1q1bZ90XERFhSDI2bNhg3XfhwgXDy8vL6Nevn3XfiRMnDEnGW2+9ZXPNmJgYIyIiIksNw4cPN/74f/vJkycbkoxffvnljnVn3mPOnDnWfdWqVTOKFStmXLx40bpvz549hpubm/HSSy9luV+nTp1srvnUU08ZwcHBd7znHz+Hr6+vYRiG8fTTTxuNGjUyDMMw0tPTjdDQUGPkyJG3/RmkpKQY6enpWT6Hl5eXERcXZ923ffv2LJ8tU/369Q1JxqxZs257rH79+jb7vvzyS0OSMXr0aOP48eNGoUKFjNatW//tZwTw9+isAHcpOTlZkuTn55et87/44gtJUt++fW329+vXT5KyzG2pWLGi6tata31dtGhRlStXTsePH7/rmv8sc67LZ599poyMjGy959y5c9q9e7diY2NVuHBh6/4qVaro8ccft37OP3rllVdsXtetW1cXL160/gyz4/nnn9c333yjhIQErVu3TgkJCbcdApJuzXNxc7v1x1t6erouXrxoHeL6/vvvs31PLy8vdezYMVvnNmnSRC+//LLi4uLUpk0beXt769133832vQDcGWEFuEv+/v6SpCtXrmTr/J9++klubm4qU6aMzf7Q0FAFBgbqp59+stlfsmTJLNcICgrS5cuX77LirJ599lnVrl1bXbp0UUhIiNq3b6/Fixf/ZXDJrLNcuXJZjlWoUEG//vqrrl27ZrP/z58lKChIknL0Wf7xj3/Iz89PH330kRYuXKgaNWpk+VlmysjI0OTJk1W2bFl5eXmpSJEiKlq0qPbu3aukpKRs3/O+++7L0WTaCRMmqHDhwtq9e7emTp2qYsWKZfu9AO6MsALcJX9/f4WFhWn//v05et+fJ7jeibu7+233G4Zx1/fInE+RycfHRxs2bNBXX32lF198UXv37tWzzz6rxx9/PMu5uZGbz5LJy8tLbdq00bx58/Tpp5/esasiSWPHjlXfvn1Vr149/ec//9GXX36pNWvW6IEHHsh2B0m69fPJiV27dunChQuSpH379uXovQDujLAC5EKLFi107Ngxbd269W/PjYiIUEZGho4cOWKz//z580pMTLSu7LGHoKAgm5Uzmf7cvZEkNzc3NWrUSJMmTdIPP/ygMWPGaN26dfr6669ve+3MOg8dOpTl2I8//qgiRYrI19c3dx/gDp5//nnt2rVLV65cue2k5Ewff/yxGjZsqA8++EDt27dXkyZN1Lhx4yw/k+wGx+y4du2aOnbsqIoVK6pbt24aP368tm/fbrfrA66MsALkwsCBA+Xr66suXbro/PnzWY4fO3ZMb7/9tqRbwxiSsqzYmTRpkiSpefPmdqurdOnSSkpK0t69e637zp07p08//dTmvEuXLmV5b+bD0f68nDpT8eLFVa1aNc2bN8/mL//9+/dr9erV1s+ZFxo2bKhRo0bpnXfeUWho6B3Pc3d3z9K1WbJkic6cOWOzLzNU3S7Y5dSgQYN06tQpzZs3T5MmTVJkZKRiYmLu+HMEkH08FA7IhdKlS+vDDz/Us88+qwoVKtg8wXbLli1asmSJYmNjJUlVq1ZVTEyMZs+ercTERNWvX1/fffed5s2bp9atW99xWezdaN++vQYNGqSnnnpKvXr10vXr1zVz5kzdf//9NhNM4+LitGHDBjVv3lwRERG6cOGCZsyYoRIlSqhOnTp3vP5bb72lZs2aKTo6Wp07d9Zvv/2madOmKSAgQCNGjLDb5/gzNzc3/etf//rb81q0aKG4uDh17NhRtWrV0r59+7Rw4UKVKlXK5rzSpUsrMDBQs2bNkp+fn3x9fVWzZk1FRUXlqK5169ZpxowZGj58uHUp9Zw5c9SgQQMNHTpU48ePz9H1APyJg1cjAfeEw4cPG127djUiIyMNT09Pw8/Pz6hdu7Yxbdo0IyUlxXpeWlqaMXLkSCMqKsooUKCAER4ebgwZMsTmHMO4tXS5efPmWe7z5yWzd1q6bBiGsXr1aqNSpUqGp6enUa5cOeM///lPlqXLa9euNVq1amWEhYUZnp6eRlhYmPHcc88Zhw8fznKPPy/v/eqrr4zatWsbPj4+hr+/v/Hkk08aP/zwg805mff789LoOXPmGJKMEydO3PFnahi2S5fv5E5Ll/v162cUL17c8PHxMWrXrm1s3br1tkuOP/vsM6NixYqGh4eHzeesX7++8cADD9z2nn+8TnJyshEREWFUr17dSEtLszmvT58+hpubm7F169a//AwA/prFMHIwww0AACCfMWcFAAA4NcIKAABwaoQVAADg1AgrAADAqRFWAACAUyOsAAAAp8ZD4fJJRkaGzp49Kz8/P7s+4hsAkL8Mw9CVK1cUFhZm/XbvvJaSkqIbN27Y5Vqenp7y9va2y7XyC2Eln5w9e1bh4eGOLgMAYCenT59WiRIl8vw+KSkp8vELlm5et8v1QkNDdeLECVMFFsJKPvHz85MkedboLYuHl4OrAfLOqRWvO7oEIE9dSU5Wmahw65/ree3GjRvSzevyeqCj5O6Zu4ul31DCgTm6ceMGYQVZZQ79WDy8CCu4p/n7+zu6BCBf5PuQvrunLLkMK2Z9ZD1hBQAAM7BIym1AMumUScIKAABmYHG7teX2GiZkzqoBAIDLoLMCAIAZWCx2GAYy5zgQYQUAADNw4WEgwgoAAGbgwp0Vc0YsAADgMuisAABgCnYYBjJpj4KwAgCAGTAMBAAAYGvDhg168sknFRYWJovFomXLltkcNwxDw4YNU/HixeXj46PGjRvryJEjNudcunRJHTp0kL+/vwIDA9W5c2ddvXo1R3UQVgAAMIPM1UC53XLg2rVrqlq1qqZPn37b4+PHj9fUqVM1a9Ysbdu2Tb6+vmratKlSUlKs53To0EEHDhzQmjVrtGLFCm3YsEHdunXLUR0MAwEAYAYOGAZq1qyZmjVrdttjhmFoypQp+te//qVWrVpJkubPn6+QkBAtW7ZM7du318GDB7Vq1Spt375dDz/8sCRp2rRp+sc//qEJEyYoLCwsW3XQWQEAwMUkJyfbbKmpqTm+xokTJ5SQkKDGjRtb9wUEBKhmzZraunWrJGnr1q0KDAy0BhVJaty4sdzc3LRt27Zs34uwAgCAGdhxGCg8PFwBAQHWLT4+PsflJCQkSJJCQkJs9oeEhFiPJSQkqFixYjbHPTw8VLhwYes52cEwEAAAZmDHYaDTp0/L39/futvLyyt3181jdFYAAHAx/v7+NtvdhJXQ0FBJ0vnz5232nz9/3nosNDRUFy5csDl+8+ZNXbp0yXpOdhBWAAAwAwesBvorUVFRCg0N1dq1a637kpOTtW3bNkVHR0uSoqOjlZiYqJ07d1rPWbdunTIyMlSzZs1s34thIAAAzMBiscMXGeZsGOnq1as6evSo9fWJEye0e/duFS5cWCVLllTv3r01evRolS1bVlFRURo6dKjCwsLUunVrSVKFChX0xBNPqGvXrpo1a5bS0tLUs2dPtW/fPtsrgSTCCgAAuIMdO3aoYcOG1td9+/aVJMXExGju3LkaOHCgrl27pm7duikxMVF16tTRqlWr5O3tbX3PwoUL1bNnTzVq1Ehubm5q27atpk6dmqM6LIZhGPb5SPgrycnJCggIkFf0IFk8nHsiE5Abl9eNdHQJQJ5KTk5WSHCAkpKSbCap5uX9AgIC5FXndVk8vP/+DX/BuJmi1E1j8612e6GzAgCAGdhjzokd56zkJ8IKAABmwBcZAgAAOCc6KwAAmAHDQAAAwKkxDAQAAOCc6KwAAGAGDAMBAACnxjAQAACAc6KzAgCAGTAMBAAAnBrDQAAAAM6JzgoAAKZgh2Egk/YoCCsAAJiBCw8DEVYAADADi8UOE2zNGVbM2Q8CAAAug84KAABmwNJlAADg1Fx4zoo5IxYAAHAZdFYAADADhoEAAIBTYxgIAADAOdFZAQDADBgGAgAATo1hIAAAAOdEZwUAABOwWCyyuGhnhbACAIAJuHJYYRgIAAA4NTorAACYgeV/W26vYUKEFQAATMCVh4EIKwAAmIArhxXmrAAAAKdGZwUAABNw5c4KYQUAABNw5bDCMBAAAHBqdFYAADADli4DAABnxjAQAACAk6KzAgCACVgsskNnxT615DfCCgAAJmCRHYaBTJpWGAYCAABOjc4KAAAm4MoTbAkrAACYgQsvXWYYCAAAODU6KwAAmIEdhoEMhoEAAEBescecldyvJnIMwgoAACbgymGFOSsAAMCp0VkBAMAMXHg1EGEFAAATYBgIAADASdFZAQDABFy5s0JYAQDABFw5rDAMBAAAnBqdFQAATMCVOyuEFQAAzMCFly4zDAQAAJwanRUAAEyAYSAAAODUCCsAAMCpuXJYYc4KTK+Qj6feevUJHVrcR5fW/Etfz+ish8qHWY8XC/LV7CGtdXxpP11c/YY+e+sFlS5R2IEVAzmzaeMGtW39pKJKhsmngEWff7bM5rhhGIobMUxR4cUV5OejfzRtrKNHjjimWCAPEFZgejMHtdJjD5dWpzFL9XDsDH21/ZhWTopRWBE/SdLiMc8pKixIz7z+f3q08yydOp+kLybFqKB3AQdXDmTPtWvXVLlKVU2ZOv22xydOGK8Z70zV1OmztGHzNvn6+urJ5k2VkpKSz5UiT1nstJkQYQWm5u3podb1KuiNmau1ec9POn7mksbM+UbHzlxS19Y1VKZEsGpWCleviSu088ezOnL6onpNXCFvLw+1a1TZ0eUD2dL0iWYaETdarVo/leWYYRiaPnWKBr3+Lz3ZspUqV6mi9+fM17mzZ7N0YGBumcNAud3MiLACU/Nwd5OHh7tSbty02Z+SmqZalUvKy9P91us/HDcMQzfS0lWrSsl8rRXICydPnFBCQoIee6yxdV9AQIBqPFJT277d6sDKAPshrMDUrv52Q9/uP6UhMfVVPNhPbm4WtX+8imo+EK7QYD8d+ulXnUpI1KhujRVYyFsFPNzV7/k6KlEsQKHBfo4uH8i1hIQESVKxkBCb/cVCQnT+fIIjSkIeobOCbImNjVXr1q2trxs0aKDevXs7rB7c0mn0UlksFh3/tL+SvhqqHk/X1OK1+5RhGLqZnqH2/1qkMuHBOvfFEF1a/YbqPRipVd8eVkaG4ejSASDbLLJDWDHppBWHhpXY2FhZLBa9+eabNvuXLVuW4/QXGRmpKVOmZOu8P/+PV6JEiRzdC87lxNnLatJrjoKbjFbZZyap7svvqYCHu06cvSxJ2nX4nB7tPEshzcYq6qkJajXgPwr2L6gT5y47uHIg90JDQyVJF86ft9l/4fx5hYSEOqIkwO4c3lnx9vbWuHHjdPly/v3FERcXp3Pnzlm3Xbt25du9kXeup6Qp4eJVBRbyVuMapbVi0482x5OvperXpOsqXaKwqpcLy3IcMKPIqCiFhobq66/XWvclJydr+3fbVPPRaAdWBntjGMiBGjdurNDQUMXHx//leZ988okeeOABeXl5KTIyUhMnTrQea9CggX766Sf16dMnW/9j+Pn5KTQ01LoVLVpU6enp6ty5s6KiouTj46Ny5crp7bfftstnRN5qXKO0Hn+kjCKKB+qxh0tp1duxOnzqV83/4lYIbdOgoupWi1Rk8SC1qFNOKye+pOWbftTa7cccXDmQPVevXtWe3bu1Z/duSbcm1e7ZvVunTp2SxWJRj169NW7saK1Y/rn279unzh1fUvGwMLVs1dqhdcPOHLB0OT09XUOHDrX+3Vi6dGmNGjVKhvH7MLphGBo2bJiKFy8uHx8fNW7cWEfs/Jwfhz/B1t3dXWPHjtXzzz+vXr163XZIZufOnWrXrp1GjBihZ599Vlu2bFH37t0VHBys2NhYLV26VFWrVlW3bt3UtWvXu6ojIyNDJUqU0JIlSxQcHKwtW7aoW7duKl68uNq1a5fj66Wmpio1NdX6Ojk5+a7qwt8LKOStuG6NdV9Rf1268ps+W/+Dhr+3VjfTMyRJocF+GtfzCRUL8lXCxata+OUexc9b7+Cqgez7fucONW3c0Pp60IC+kqQXXozRe/+eq379B+r6tWvq+c9uSkxMVK3adfT5ilXy9vZ2VMm4R4wbN04zZ87UvHnz9MADD2jHjh3q2LGjAgIC1KtXL0nS+PHjNXXqVM2bN09RUVEaOnSomjZtqh9++MFuv4MW44/xKJ/FxsYqMTFRy5YtU3R0tCpWrKgPPvhAy5Yt01NPPWVNbh06dNAvv/yi1atXW987cOBArVy5UgcOHJB0ay5K7969/3bCa2RkpM6dO6cCBX5/INjYsWOtP/Q/6tmzpxISEvTxxx9nqVe61dGpVq3abefKjBgxQiNHjsyy3yt6kCweXn9ZI2Bml9dl/b0H7iXJyckKCQ5QUlKS/P398+V+AQEBiui+RG5eBXN1rYzU6/ppxjPZrr1FixYKCQnRBx98YN3Xtm1b+fj46D//+Y8Mw1BYWJj69eun/v37S5KSkpIUEhKiuXPnqn379rmqN5PDh4EyjRs3TvPmzdPBgwezHDt48KBq165ts6927do6cuSI0tPTc3yvAQMGaPfu3dbtpZdekiRNnz5dDz30kIoWLapChQpp9uzZOnXq1F19niFDhigpKcm6nT59+q6uAwCAZN85K8nJyTbbH0cC/qhWrVpau3atDh8+LEnas2ePNm3apGbNmkmSTvzvOT+NG9s+56dmzZrautV+z/lx+DBQpnr16qlp06YaMmSIYmNj8/ReRYoUUZkyZWz2LVq0SP3799fEiRMVHR0tPz8/vfXWW9q2bdtd3cPLy0teXnRQAADOJzw83Ob18OHDNWLEiCznDR48WMnJySpfvrzc3d2Vnp6uMWPGqEOHDpJ+f85PyJ+e8xMSEmI9Zg9OE1Yk6c0331S1atVUrlw5m/0VKlTQ5s2bbfZt3rxZ999/v9zdbz2h1NPT8666LH+8Xq1atdS9e3frvmPHmIAJAHAOFsutLbfXkKTTp0/bDAPd6T+uFy9erIULF+rDDz/UAw88oN27d6t3794KCwtTTExM7orJAacZBpKkypUrq0OHDpo6darN/n79+mnt2rUaNWqUDh8+rHnz5umdd96xjo9Jt+aibNiwQWfOnNGvv/6a43uXLVtWO3bs0JdffqnDhw9r6NCh2r59e64/EwAA9nArrOR2GOjWtfz9/W22O4WVAQMGaPDgwWrfvr0qV66sF198UX369LGu4M18zs/5Pz3n5/z589Zj9uBUYUW69QyUjIwMm33Vq1fX4sWLtWjRIlWqVEnDhg1TXFyczXBRXFycTp48qdKlS6to0aI5vu/LL7+sNm3a6Nlnn1XNmjV18eJFmy4LAAAOZfm9u3K3W06XLl+/fl1ubrZRwd3d3fr3dNT/nvOzdq3tc362bdum6Gj7PefHoauBXEnmbG5WA+Fex2og3OsctRqoVK+P5e7lm6trpade0/GpT2e79tjYWH311Vd699139cADD2jXrl3q1q2bOnXqpHHjxkm6tUDmzTfftFm6vHfvXrsuXXaqOSsAAOD27PEE2py+f9q0aRo6dKi6d++uCxcuKCwsTC+//LKGDRtmPWfgwIG6du2aunW79ZyfOnXqaNUq+z7nh85KPqGzAldBZwX3Okd1Vsr0/sQunZWjU9rmW+324nRzVgAAAP6IYSAAAEzAzc0iN7fcDQMZuXy/oxBWAAAwAXs+Z8VsGAYCAABOjc4KAAAm4IjVQM6CsAIAgAkwDAQAAOCk6KwAAGACDAMBAACnRlgBAABOjTkrAAAATorOCgAAJmCRHYaBZM7WCmEFAAATYBgIAADASdFZAQDABFgNBAAAnBrDQAAAAE6KzgoAACbAMBAAAHBqDAMBAAA4KTorAACYAMNAAADAudlhGMikD7BlGAgAADg3OisAAJgAw0AAAMCpufJqIMIKAAAm4MqdFeasAAAAp0ZnBQAAE2AYCAAAODWGgQAAAJwUnRUAAEzAlTsrhBUAAEzAleesMAwEAACcGp0VAABMgGEgAADg1BgGAgAAcFJ0VgAAMAGGgQAAgFOzyA7DQHapJP8RVgAAMAE3i0VuuUwruX2/ozBnBQAAODU6KwAAmIArrwYirAAAYAKuPMGWYSAAAODU6KwAAGACbpZbW26vYUaEFQAAzMBih2Eck4YVhoEAAIBTo7MCAIAJsBoIAAA4Ncv//sntNcyIYSAAAODU6KwAAGACrAYCAABOjYfCAQAAOKlsdVY+//zzbF+wZcuWd10MAAC4PVYD/Y3WrVtn62IWi0Xp6em5qQcAANyGm8Uit1ymjdy+31GyFVYyMjLyug4AAPAXXLmzkqs5KykpKfaqAwAA4LZyHFbS09M1atQo3XfffSpUqJCOHz8uSRo6dKg++OADuxcIAAB+Xw2U282MchxWxowZo7lz52r8+PHy9PS07q9UqZLef/99uxYHAABuyRwGyu1mRjkOK/Pnz9fs2bPVoUMHubu7W/dXrVpVP/74o12LAwAAyPFD4c6cOaMyZcpk2Z+RkaG0tDS7FAUAAGy58mqgHHdWKlasqI0bN2bZ//HHH+vBBx+0S1EAAMCWxU6bGeW4szJs2DDFxMTozJkzysjI0NKlS3Xo0CHNnz9fK1asyIsaAQCAC8txZ6VVq1Zavny5vvrqK/n6+mrYsGE6ePCgli9frscffzwvagQAwOW58mqgu/oiw7p162rNmjX2rgUAANwB37p8F3bs2KGDBw9KujWP5aGHHrJbUQAAAJlyHFZ+/vlnPffcc9q8ebMCAwMlSYmJiapVq5YWLVqkEiVK2LtGAABcnj2Gccw6DJTjOStdunRRWlqaDh48qEuXLunSpUs6ePCgMjIy1KVLl7yoEQAAyDUfCCfdRWdl/fr12rJli8qVK2fdV65cOU2bNk1169a1a3EAAAA57qyEh4ff9uFv6enpCgsLs0tRAADAlqNWA505c0YvvPCCgoOD5ePjo8qVK2vHjh3W44ZhaNiwYSpevLh8fHzUuHFjHTlyxJ4fPedh5a233tKrr75qU+iOHTv02muvacKECXYtDgAA3JK5Gii3W05cvnxZtWvXVoECBfTf//5XP/zwgyZOnKigoCDrOePHj9fUqVM1a9Ysbdu2Tb6+vmratKlSUlLs9tmzNQwUFBRkk8auXbummjVrysPj1ttv3rwpDw8PderUSa1bt7ZbcQAA4BZHTLAdN26cwsPDNWfOHOu+qKgo678bhqEpU6boX//6l1q1aiXp1ncIhoSEaNmyZWrfvn2u6s2UrbAyZcoUu9wMAAA4XnJyss1rLy8veXl5ZTnv888/V9OmTfXMM89o/fr1uu+++9S9e3d17dpVknTixAklJCSocePG1vcEBASoZs2a2rp1a/6GlZiYGLvcDAAA3B17fLdP5vvDw8Nt9g8fPlwjRozIcv7x48c1c+ZM9e3bV6+//rq2b9+uXr16ydPTUzExMUpISJAkhYSE2LwvJCTEeswe7vqhcJKUkpKiGzdu2Ozz9/fPVUEAACAre37r8unTp23+vr5dV0WSMjIy9PDDD2vs2LGSpAcffFD79+/XrFmz8rWRkeMJtteuXVPPnj1VrFgx+fr6KigoyGYDAADOzd/f32a7U1gpXry4KlasaLOvQoUKOnXqlCQpNDRUknT+/Hmbc86fP289Zg85DisDBw7UunXrNHPmTHl5een999/XyJEjFRYWpvnz59utMAAA8LvcPhDubh4MV7t2bR06dMhm3+HDhxURESHp1mTb0NBQrV271no8OTlZ27ZtU3R0dK4/c6YcDwMtX75c8+fPV4MGDdSxY0fVrVtXZcqUUUREhBYuXKgOHTrYrTgAAHCLI1YD9enTR7Vq1dLYsWPVrl07fffdd5o9e7Zmz55tvV7v3r01evRolS1bVlFRURo6dKjCwsLsujo4x2Hl0qVLKlWqlKRbbaRLly5JkurUqaN//vOfdisMAAA4Vo0aNfTpp59qyJAhiouLU1RUlKZMmWLTmBg4cKCuXbumbt26KTExUXXq1NGqVavk7e1ttzpyHFZKlSqlEydOqGTJkipfvrwWL16sRx55RMuXL7d+sSEAALAve3y/z928v0WLFmrRosVfXNOiuLg4xcXF5aKyv5bjOSsdO3bUnj17JEmDBw/W9OnT5e3trT59+mjAgAF2LxAAAPy+Gii3mxnluLPSp08f6783btxYP/74o3bu3KkyZcqoSpUqdi0OAAAgV89ZkaSIiAjrrGAAAJA3HDUM5AyyFVamTp2a7Qv26tXrrosBAAC354jVQM4iW2Fl8uTJ2bqYxWIhrPyNI58O4im/uKcF1ejp6BKAPGWk3/j7k/KAm+5ioultrmFG2QorJ06cyOs6AAAAbivXc1YAAEDeYxgIAAA4NYtFcnPRCbZmHb4CAAAugs4KAAAm4GaHzkpu3+8ohBUAAEzAlees3NUw0MaNG/XCCy8oOjpaZ86ckSQtWLBAmzZtsmtxAAAAOQ4rn3zyiZo2bSofHx/t2rVLqampkqSkpCSNHTvW7gUCAIDfh4Fyu5lRjsPK6NGjNWvWLL333nsqUKCAdX/t2rX1/fff27U4AABwS+bj9nO7mVGOw8qhQ4dUr169LPsDAgKUmJhoj5oAAACschxWQkNDdfTo0Sz7N23apFKlStmlKAAAYMvNYrHLZkY5Ditdu3bVa6+9pm3btslisejs2bNauHCh+vfvr3/+8595USMAAC7PzU6bGeV46fLgwYOVkZGhRo0a6fr166pXr568vLzUv39/vfrqq3lRIwAAcGE5DisWi0VvvPGGBgwYoKNHj+rq1auqWLGiChUqlBf1AQAA2WeCrElHge7+oXCenp6qWLGiPWsBAAB34KbczzlxkznTSo7DSsOGDf/yCXjr1q3LVUEAACArOis5UK1aNZvXaWlp2r17t/bv36+YmBh71QUAACDpLsLK5MmTb7t/xIgRunr1aq4LAgAAWbnyFxnabRXTCy+8oH//+9/2uhwAAPgDiyX3z1ox6zCQ3cLK1q1b5e3tba/LAQAASLqLYaA2bdrYvDYMQ+fOndOOHTs0dOhQuxUGAAB+xwTbHAgICLB57ebmpnLlyikuLk5NmjSxW2EAAOB3rjxnJUdhJT09XR07dlTlypUVFBSUVzUBAABY5WjOiru7u5o0acK3KwMAkM8sdvrHjHI8wbZSpUo6fvx4XtQCAADuIHMYKLebGeU4rIwePVr9+/fXihUrdO7cOSUnJ9tsAAAA9pTtOStxcXHq16+f/vGPf0iSWrZsafPYfcMwZLFYlJ6ebv8qAQBwcUywzYaRI0fqlVde0ddff52X9QAAgNuwWCx/+d182b2GGWU7rBiGIUmqX79+nhUDAABuz5U7Kzmas2LWRAYAAMwrR89Zuf/++/82sFy6dClXBQEAgKx4gm02jRw5MssTbAEAQN7L/DLC3F7DjHIUVtq3b69ixYrlVS0AAABZZDusMF8FAADHceUJtjleDQQAABzADnNWTPq0/eyHlYyMjLysAwAA4LZyNGcFAAA4hpsscstlayS373cUwgoAACbgykuXc/xFhgAAAPmJzgoAACbAaiAAAODUXPmhcAwDAQAAp0ZnBQAAE3DlCbaEFQAATMBNdhgGYukyAADIK67cWWHOCgAAcGp0VgAAMAE35b7DYNYOBWEFAAATsFgssuRyHCe373cUs4YsAADgIuisAABgApb/bbm9hhkRVgAAMAGeYAsAAOCk6KwAAGAS5uyL5B5hBQAAE+ChcAAAAE6KzgoAACbgys9ZIawAAGACPMEWAAA4NVfurJg1ZAEAABdBZwUAABPgCbYAAMCpMQwEAADgpOisAABgAq68GsisdQMA4FIyh4Fyu92tN998UxaLRb1797buS0lJUY8ePRQcHKxChQqpbdu2On/+vB0+rS3CCgAA+Evbt2/Xu+++qypVqtjs79Onj5YvX64lS5Zo/fr1Onv2rNq0aWP3+xNWAAAwAYudtpy6evWqOnTooPfee09BQUHW/UlJSfrggw80adIkPfbYY3rooYc0Z84cbdmyRd9+++1df87bIawAAGACmV9kmNtNkpKTk2221NTUO963R48eat68uRo3bmyzf+fOnUpLS7PZX758eZUsWVJbt26162cnrAAA4GLCw8MVEBBg3eLj42973qJFi/T999/f9nhCQoI8PT0VGBhosz8kJEQJCQl2rZfVQAAAmICbLHLL5WPdMt9/+vRp+fv7W/d7eXllOff06dN67bXXtGbNGnl7e+fqvrlFZwUAABOw5zCQv7+/zXa7sLJz505duHBB1atXl4eHhzw8PLR+/XpNnTpVHh4eCgkJ0Y0bN5SYmGjzvvPnzys0NNSun53OCgAAyKJRo0bat2+fzb6OHTuqfPnyGjRokMLDw1WgQAGtXbtWbdu2lSQdOnRIp06dUnR0tF1rIawAAGAClv/9k9trZJefn58qVapks8/X11fBwcHW/Z07d1bfvn1VuHBh+fv769VXX1V0dLQeffTRXNX5Z4QVAABM4I/DOLm5hj1NnjxZbm5uatu2rVJTU9W0aVPNmDHDvjcRYQUAAFOw2GGCbW47M998843Na29vb02fPl3Tp0/P1XX/DhNsAQCAU6OzAgCACTjjMFB+IawAAGACrhxWGAYCAABOjc4KAAAmkN9Ll50JYQUAABNws9zacnsNM2IYCAAAODU6KwAAmADDQAAAwKmxGggwsc2bNujZtq1UvlS4Agt6aMXnn9kcjx89UjWqPaCwIv6KCCuiVs2baMd32xxULfD3alcvrY+nvKzjq8fot13v6MkGVbKcM/SfzXV89Rhd2jpJK2f1VOmSRW97Lc8CHvp20WD9tusdVbn/vrwuHcgThBWY3vVr11S5chW9NXnabY+XKXu/3pr0trZs361VX61XyZKRatOymX795Zd8rhTIHl8fL+07fEa94z+67fF+sY3V/bn66jV2keq9NEHXfruh5dN7yMsza7N8bO9WOvdLUl6XjHxg0e9DQXf/jzkxDATTe7xpMz3etNkdjz/z7HM2r8eMm6AF8/6tA/v3qn7DRnldHpBjqzf/oNWbf7jj8R7PN9S4977Uim/2SZK6DJ2vn76KV8uGVbXky53W85rUrqhGj1bQcwPe1xN1HsjzupG3WA0EuIgbN25o3r/fk39AgCpVrurocoAci7wvWMWLBmjdth+t+5Kvpmj7/pOqWSXSuq9YYT/NGPqcOg+dr+u/3XBApbC33HdVzNtbIaxk09y5cxUYGGh9PWLECFWrVs1h9SBnVn2xQvcVDVBIkK9mTHtby5avUnCRIo4uC8ix0CL+kqQLl67Y7L9w8YpCgv2tr2fHvaD3Pt6k7384la/1AXnB5cJKbGysLBZLlu3o0aOOLg15qG79htr47U6t/nqjGj3eVLEvPqdfLlxwdFlAnuj+XH35FfTWW/9e7ehSYEeZq4Fyu5mRy4UVSXriiSd07tw5my0qKsrRZSEP+fr6qlTpMqrxyKN6Z9Z78vDw0IJ5/3Z0WUCOJfyaLOnWMM8fFQv20/mLt441qHG/alaJUtK2Kbqy/W0d+Hy4JGnzwoF6L+7F/C0YdmOx02ZGLjnB1svLS6GhoTb7Jk2apDlz5uj48eMqXLiwnnzySY0fP16FChVyUJXISxkZGUpNTXV0GUCOnTxzUed+SVLDmuW09/AZSZKfr7dqVIrUe0s2SZL6jf9YI6avsL6neNEArZjZUy8OnqPt+046omwgV1wyrNyOm5ubpk6dqqioKB0/flzdu3fXwIEDNWPGjLu6Xmpqqs1fhsnJyfYqFX9y9epVHT/2+zDeTz+d0N49uxVUuLAKFw7WxHFj1azFkwoJLa5Lv/6q996dqXNnz6h1m6cdWDVwZ74+niod/vtzUyLvC1aV++/T5eTrOp1wWdM//FqDujyho6d+0ckzFzW8e3Od+yVJn3+9R5J0OuGyzfWuXr/1Z9Hx07/ozIXEfPscsC83WeSWy3EcN5P2VlwyrKxYscKmY9KsWTMtWbLE+joyMlKjR4/WK6+8ctdhJT4+XiNHjsx1rfh7u77foSefaGx9/cag/pKk5154SZOnztDhw4f0f88t0MWLv6pw4WA9+NDD+u+ab1ShIks54ZyqV4zQ6vdfs74e37+tJGnB59+q2/D/aOLcr1TQx0vv/Os5Bfr5aMvuY2rZY4ZSb9x0VMnIB/YYxjFnVHHRsNKwYUPNnDnT+trX11dfffWV4uPj9eOPPyo5OVk3b95USkqKrl+/roIFC+b4HkOGDFHfvn2tr5OTkxUeHm6X+mGrbr0GSrx+5z+k/7Po43ysBsi9jTuPyOfBnn95zqiZKzVq5spsXe/UuUt/ez3AmbnkBFtfX1+VKVPGuqWmpqpFixaqUqWKPvnkE+3cuVPTp0+XdOu5HHfDy8tL/v7+NhsAAHfNhWfYumRn5c927typjIwMTZw4UW5ut/Lb4sWLHVwVAAC/c+VvXXbJzsqflSlTRmlpaZo2bZqOHz+uBQsWaNasWY4uCwAAiLAiSapataomTZqkcePGqVKlSlq4cKHi4+MdXRYAAL+zxwPhzNlYkcUwDMPRRbiC5ORkBQQE6FTCJeav4J4WWuu1vz8JMDEj/YZS972npKSkfPnzPPPvj3W7T6mQX+7ud/VKsh6rVjLfarcXOisAAMCpMcEWAAAzcOEHrRBWAAAwAVdeDURYAQDABOzxrcl86zIAAEAeoLMCAIAJuPCUFcIKAACm4MJphWEgAADg1OisAABgAqwGAgAATo3VQAAAAE6KzgoAACbgwvNrCSsAAJiCC6cVhoEAAIBTo7MCAIAJsBoIAAA4NVYDAQAAOCk6KwAAmIALz68lrAAAYAounFYIKwAAmIArT7BlzgoAAHBqdFYAADABV14NRFgBAMAEXHjKCsNAAADAudFZAQDADFy4tUJYAQDABFgNBAAA4KTorAAAYAKsBgIAAE7NhaesMAwEAACcG50VAADMwIVbK4QVAABMwJVXAxFWAAAwAztMsDVpVmHOCgAAcG50VgAAMAEXnrJCWAEAwBRcOK0wDAQAAJwanRUAAEyA1UAAAMCpufLj9hkGAgAATo3OCgAAJuDC82sJKwAAmIILpxWGgQAAgFMjrAAAYAIWO/2TE/Hx8apRo4b8/PxUrFgxtW7dWocOHbI5JyUlRT169FBwcLAKFSqktm3b6vz58/b86IQVAADMwKLfVwTd9ZbDe65fv149evTQt99+qzVr1igtLU1NmjTRtWvXrOf06dNHy5cv15IlS7R+/XqdPXtWbdq0setnZ84KAAC4rVWrVtm8njt3rooVK6adO3eqXr16SkpK0gcffKAPP/xQjz32mCRpzpw5qlChgr799ls9+uijdqmDzgoAACZgsdMmScnJyTZbampqtmpISkqSJBUuXFiStHPnTqWlpalx48bWc8qXL6+SJUtq69atufm4NggrAACYQK6HgP7wULnw8HAFBARYt/j4+L+9f0ZGhnr37q3atWurUqVKkqSEhAR5enoqMDDQ5tyQkBAlJCTY7bMzDAQAgCnYb+3y6dOn5e/vb93r5eX1t+/s0aOH9u/fr02bNuWyhpwjrAAA4GL8/f1twsrf6dmzp1asWKENGzaoRIkS1v2hoaG6ceOGEhMTbbor58+fV2hoqN3qZRgIAAATsOcwUHYZhqGePXvq008/1bp16xQVFWVz/KGHHlKBAgW0du1a675Dhw7p1KlTio6OtsfHlkRnBQAAU3DEA2x79OihDz/8UJ999pn8/Pys81ACAgLk4+OjgIAAde7cWX379lXhwoXl7++vV199VdHR0XZbCSQRVgAAwB3MnDlTktSgQQOb/XPmzFFsbKwkafLkyXJzc1Pbtm2Vmpqqpk2basaMGXatg7ACAIAJ3M0wzu2ukROGYfztOd7e3po+fbqmT59+l1X9PcIKAAAmcDePy7/dNcyICbYAAMCp0VkBAMAMHDHD1kkQVgAAMAEXzioMAwEAAOdGZwUAABNwxGogZ0FYAQDABFx5NRBhBQAAM3DhSSvMWQEAAE6NzgoAACbgwo0VwgoAAGbgyhNsGQYCAABOjc4KAACmkPvVQGYdCCKsAABgAgwDAQAAOCnCCgAAcGoMAwEAYAIMAwEAADgpOisAAJgA3w0EAACcGsNAAAAATorOCgAAJsB3AwEAAOfmwmmFsAIAgAm48gRb5qwAAACnRmcFAAATcOXVQIQVAABMwIWnrDAMBAAAnBudFQAAzMCFWyuEFQAATIDVQAAAAE6Kzko+MQxDknTlSrKDKwHylpF+w9ElAHkq83c888/1/HLlSnKuV/OY9e8gwko+uXLliiTpgbKRji0EAGAXV65cUUBAQJ7fx9PTU6GhoSobFW6X64WGhsrT09Mu18ovFiO/o6GLysjI0NmzZ+Xn5yeLWRe6m0xycrLCw8N1+vRp+fv7O7ocIE/we57/DMPQlStXFBYWJje3/JlNkZKSohs37NO19PT0lLe3t12ulV/orOQTNzc3lShRwtFluCR/f3/+EMc9j9/z/JUfHZU/8vb2Nl3AsCcm2AIAAKdGWAEAAE6NsIJ7lpeXl4YPHy4vLy9HlwLkGX7P4QqYYAsAAJwanRUAAODUCCsAAMCpEVYAAIBTI6wAAACnRlgB/ufo0aOOLgEAcBuEFUDSwoULFRMTo+XLlzu6FCBXMjIyHF0CYHeEFUBSVFSU3N3dNXv2bK1YscLR5QA59sUXX0i69dUeBBbcawgrcGmrVq3SpUuXVKtWLU2cOFHXrl3TjBkzCCwwlR07duiVV15Rp06dJBFYcO8hrMBlbd26VX369NGQIUOUmJioGjVq6M0331RKSgqBBaZSqlQp9e3bV3v27FGXLl0kEVhwbyGswGXVqFFDL7zwgn744Qe9/vrrunz5sh555BECC0zj7bff1qZNm1S4cGHFxsYqJiZGO3bsILDgnkNYgUvKyMiQh4eHBg0apObNm2vXrl164403CCwwjV9//VX//e9/1bJlS3333XcKDAzUSy+9pE6dOhFYcM8hrMAlubm5KT09XR4eHurfv79atmyZJbCMGzdOKSkpmj17tpYuXerokgEbRYoU0cSJE9W0aVM9+eST2rZtG4EF9yzCClyWu7u7JMnDw0MDBgzQk08+aRNYatSoofHjx+vnn3/WokWLdPXqVQdXDNyS+f2zDzzwgIYOHar69eurZcuWBBbcs/jWZbgUwzBksVi0f/9+HTp0SAEBAYqIiFDZsmWVlpam8ePHa8WKFXrwwQc1duxYBQYG6vvvv1dwcLAiIiIcXT5glZGRITe3W/+9uX//fsXFxWn9+vX6/PPPVbNmTSUmJmr+/PmaP3++SpcurY8++sjBFQN3j7CCe15mQLl586Y8PDy0dOlSvfrqqwoODlZGRobCwsI0aNAgNWrUyBpYVq1apcjISL3zzjsKCAhw9EcArDJ/n/9s7969Gj16dJbA8u6772rlypX66KOPVLx4cQdUDOQeYQX3rMz/8kxMTFRgYKAk6euvv1a7du00cuRIde/eXUuWLFGnTp0UHh6ut956S82bN1daWppGjBih7du3a/78+QoNDXXsBwH+JzOobNq0yfq05QoVKig2NlaStG/fPo0aNUrr16/X8uXL9cgjjygpKUkZGRkKCgpyYOVA7hBWcE/KDCq7d+/WY489prVr16p8+fLq1auXgoKCNH78eJ05c0Z16tRR1apVlZ6eriNHjmjGjBl67LHHdPPmTSUlJSk4ONjRHwUuLPP3+Nq1a/L19ZUkLV26VF27dlW9evXk5+enzz77TH369NGIESMk3Qos8fHxWrx4sbZt26aHHnrIgZ8AsBMDuMekp6cbhmEYu3fvNnx9fY3Bgwdbj+3du9fYuHGjcfnyZePBBx80unTpYhiGYXz00UeGh4eHERISYqxcudIhdQN/lPl7vGPHDqN06dLGL7/8Ymzfvt0IDw83Zs6caRiGYRw+fNgICAgwLBaL8eqrr1rf+/333xuxsbHGoUOHHFI7YG8ejg5LgD1l/pfovn37FB0drf79+ysuLs56vFSpUvL19dWKFSvk5eWl4cOHS5LCwsJUr149Va1aVeXLl3dU+YCk33+P9+zZo4YNG6pTp04qUqSIli9frnbt2umVV17R6dOn1aRJE7Vr1041atTQyy+/rKCgII0cOVIPPvig3n33XXl6ejr6owB2QVjBPcXNzU0//fSToqOj1apVK5ugMmnSJCUnJ2vEiBG6fv26fvjhB509e1YlSpTQF198oVKlSmn48OFMqIVDZQaVvXv3qlatWurdu7fGjBkjSerYsaPWr19v/feGDRtq9uzZ+vnnnxUWFqZRo0bp+vXreuuttwgquKcQVnDPMQxDQUFBSk1N1caNG1W3bl1NmDBBQ4cO1cqVKyXdmpRYp04dPfPMM4qMjNTOnTu1detWggoczs3NTadPn1ajRo3UokULa1CRpJkzZ+rkyZMqUaKELl68qJEjR0qSChYsqMcff1yNGzfWww8/7KjSgTzDQ+FwT8nIyFBkZKS++uorHT58WFOmTNErr7yi+Ph4ffHFF3rsscckSZUrV9bAgQP16quvqkaNGtqxY4cqV67s4OqBW9LT0xUVFaWUlBRt3rxZkhQfH6/BgwerefPm8vb21oEDB7RlyxZdv35dEyZM0L59+9SsWTOVK1fOwdUD9sdqINxzMtvoP/74o5599lnt27dPEyZMUN++fSXJ+rwVwJkdOXJEvXr1kqenp0JCQvTZZ59pwYIFatKkiSRpwoQJGjhwoMqUKaNLly5pzZo1evDBBx1cNZA3CCu4J2UGlmPHjql169aKjIzUwIEDVbduXZvj0p0fsgU42uHDh9WzZ09t2rRJo0aNUr9+/azHbty4of379+v06dOqXr26wsPDHVgpkLcIKzC9zO87yfzuk8wQ8scOy9NPP62IiAgNGTJEderUcWS5QI4cO3ZM3bt3l7u7u15//XXr7+8ff9eBex2/6TCdzHCSkpIi6VZIOXLkiPXfM2WGl/Lly+vjjz/WmTNnNHjwYG3dujX/iwbuUunSpfXOO+/IMAyNHj3aOoeFoAJXwm87TMfNzU3Hjx9X7969debMGX388ceqUKGCDhw4cNtzMwPLwoULlZGRoRIlSjigauDulS1bVlOnTlWBAgXUv39/ffvtt44uCchXDAPBlDZs2KDWrVuratWq2rp1q2bPnq2XXnrpjvNP0tPT5e7urrS0NBUoUMABFQO59+OPP2ro0KGaOHGiSpYs6ehygHxDWIHpZAaScePGaciQIXr00Uc1f/58lSlTxub4X70XMKsbN27wwDe4HIaBYDrp6emSJG9vbw0bNkznz5/XiBEjtGvXLkmSxWLRHzN45hyXzGOAmRFU4IrorMA0Mrsif35OyurVq/Xyyy+rVq1aGjhwoKpWrSpJ2rp1q6Kjox1VLgDATggrMIXMoLJ27Vp9+umnunz5sipWrKiuXbuqWLFiWr16tV555RXVrl1b7du31/fff6/hw4crISFBRYsWpaMCACZGWIFpLFu2TM8995xeeOEF/fTTT7p8+bJ++eUXbdiwQSVLltTatWvVv39/ZWRkKDk5WR9//LEeeughR5cNAMglwgqc0p8nwv766696/PHH9fzzz2vAgAGSpP3796tfv346cuSIvvvuOxUpUkQnT55UcnKyihYtquLFizuqfACAHTHBFk4lMztfv35d0u+TY69evapz586pWrVq1nMrVKig8ePHKygoSIsWLZIkRUZGqkqVKgQVALiHEFbgVCwWiy5cuKDIyEgtXrzY+pTO0NBQhYeHa/369dZz3d3dVaVKFXl4eOjQoUOOKhkAkMcIK3A6bm5uatmypV588UV99tln1n01a9bUunXrtHTpUuu5FotF9913nwIDA2UYhhjVBIB7D3NW4HC3e1DbhQsXNGbMGE2bNk2ffPKJnnrqKV28eFEdOnRQUlKSatasqdq1a2vDhg2aP3++tm3bpvLlyzvoEwAA8hJhBQ6V+c2x165dU3p6uvz9/a3Hzp07p7Fjx2r69OlasmSJ2rZtq4sXL+rNN9/U5s2b9euvvyo0NFRTp061mcsCALi3EFbgcEeOHFG7du1UqFAhde3aVaGhoWrSpIkkKTU1Vf369dOMGTP00Ucf6ZlnntHNmzdlsVh06dIlFSxYUL6+vg7+BACAvOTx96cAeScjI0Nz587Vnj175O3trcTERF2/fl2FCxfWI488ok6dOqljx44KDg7Ws88+K39/fzVt2lSSVLRoUQdXDwDID3RW4HAJCQkaN26cjh07pjJlyqhHjx5auHChNm7cqL1796pw4cIqVaqUdu7cqQsXLuibb75RvXr1HF02ACCf0FmBw4WGhmrAgAEaO3asNm3apLJly2rYsGGSpG3btuns2bOaPXu2ihUrpgsXLqhIkSIOrhgAkJ/orMBpZE6o3bZtm1q3bq3XX3/deiwtLU0ZGRlKSkpSsWLFHFglACC/EVbgVBISEjRmzBht375drVu31uDBgyUpyzctAwBcB2EFTiczsOzatUuNGjXSyJEjHV0SAMCBeIItnE5oaKjeeOMNlS1bVlu2bNHFixcdXRIAwIHorMBpnT9/XpIUEhLi4EoAAI5EWAEAAE6NYSAAAODUCCsAAMCpEVYAAIBTI6wAAACnRlgBAABOjbACAACcGmEFAAA4NcIK4GJiY2PVunVr6+sGDRqod+/e+V7HN998I4vFosTExDueY7FYtGzZsmxfc8SIEapWrVqu6jp58qQsFot2796dq+sAsB/CCuAEYmNjZbFYZLFY5OnpqTJlyiguLk43b97M83svXbpUo0aNyta52QkYAGBvfI0t4CSeeOIJzZkzR6mpqfriiy/Uo0cPFShQQEOGDMly7o0bN+Tp6WmX+xYuXNgu1wGAvEJnBXASXl5eCg0NVUREhP75z3+qcePG+vzzzyX9PnQzZswYhYWFqVy5cpKk06dPq127dgoMDFThwoXVqlUrnTx50nrN9PR09e3bV4GBgQoODtbAgQP152/Y+PMwUGpqqgYNGqTw8HB5eXmpTJky+uCDD3Ty5Ek1bNhQkhQUFCSLxaLY2FhJUkZGhuLj4xUVFSUfHx9VrVpVH3/8sc19vvjiC91///3y8fFRw4YNberMrkGDBun+++9XwYIFVapUKQ0dOlRpaWlZznv33XcVHh6uggULql27dkpKSrI5/v7776tChQry9vZW+fLlNWPGjBzXAiD/EFYAJ+Xj46MbN25YX69du1aHDh3SmjVrtGLFCqWlpalp06by8/PTxo0btXnzZhUqVEhPPPGE9X0TJ07U3Llz9e9//1ubNm3SpUuX9Omnn/7lfV966SX93//9n6ZOnaqDBw/q3XffVaFChRQeHq5PPvlEknTo0CGdO3dOb7/9tiQpPj5e8+fP16xZs3TgwAH16dNHL7zwgtavXy/pVqhq06aNnnzySe3evVtdunTR4MGDc/wz8fPz09y5c/XDDz/o7bff1nvvvafJkyfbnHP06FEtXrxYy5cv16pVq7Rr1y51797denzhwoUaNmyYxowZo4MHD2rs2LEaOnSo5s2bl+N6AOQTA4DDxcTEGK1atTIMwzAyMjKMNWvWGF5eXkb//v2tx0NCQozU1FTrexYsWGCUK1fOyMjIsO5LTU01fHx8jC+//NIwDMMoXry4MX78eOvxtLQ0o0SJEtZ7GYZh1K9f33jttdcMwzCMQ4cOGZKMNWvW3LbOr7/+2pBkXL582bovJSXFKFiwoLFlyxabczt37mw899xzhmEYxpAhQ4yKFSvaHB80aFCWa/2ZJOPTTz+94/G33nrLeOihh6yvhw8fbri7uxs///yzdd9///tfw83NzTh37pxhGIZRunRp48MPP7S5zqhRo4zo6GjDMAzjxIkThiRj165dd7wvgPzFnBXASaxYsUKFChVSWlqaMjIy9Pzzz2vEiBHW45UrV7aZp7Jnzx4dPXpUfn5+NtdJSUnRsWPHlJSUpHPnzqlmzZrWYx4eHnr44YezDAVl2r17t9zd3VW/fv1s13306FFdv35djz/+uM3+Gzdu6MEHH5QkHTx40KYOSYqOjs72PTJ99NFHmjp1qo4dO6arV6/q5s2b8vf3tzmnZMmSuu+++2zuk5GRoUOHDsnPz0/Hjh1T586d1bVrV+s5N2/eVEBAQI7rAZA/CCuAk2jYsKFmzpwpT09PhYWFycPD9v+evr6+Nq+vXr2qhx56SAsXLsxyraJFi95VDT4+Pjl+z9WrVyVJK1eutAkJ0q15OPaydetWdejQQSNHjlTTpk0VEBCgRYsWaeLEiTmu9b333ssSntzd3e1WKwD7IqwATsLX11dlypTJ9vnVq1fXRx99pGLFimXpLmQqXry4tm3bpnr16km61UHYuXOnqlevftvzK1eurIyMDK1fv16NGzfOcjyzs5Oenm7dV7FiRXl5eenUqVN37MhUqFDBOlk407fffvv3H/IPtmzZooiICL3xxhvWfT/99FOW806dOqWzZ88qLCzMeh83NzeVK1dOISEhCgsL0/Hjx9WhQ4cc3R+A4zDBFjCpDh06qEiRImrVqpU2btyoEydO6JtvvlGvXr30888/S5Jee+01vfnmm1q2bJl+/PFHde/e/S+fkRIZGamYmBh16tRJy5Yts15z8eLFkqSIiAhZLBatWLFCv/zyi65evSo/Pz/1799fffr00bx583Ts2DF9//33mjZtmnXS6iuvvKIjR45owIABOnTokD788EPNnTs3R5+3bNmyOnXqlBYtWqRjx45p6tSpt50s7O3trZiYGO3Zs0cbN25Ur1691K5dO4WGhkqSRo4cqfj4eE2dOlWHDx/Wvn37NGfOHE2aNClH9QDIP4QVwKQKFiyoDRs2qGTJkmrTpo0qVKigzp07KyUlxdpp6devn1588UXFxMQoOjpafn5+euqpp/7yujNnztTTTz+t7t27q3z58uratauuXbsmSbrvvvs0cuRIDR48WCEhIerZs6ckadSoURo6dKji4+NVoUIFPfHEE1q5cqWioqIk3ZpH8sknn2jZsmWqWrWqZs2apbFjx+bo87Zs2VJ9+vRRz549Va1aNW3ZskVDhw7Ncl6ZMmXUpk0b/eMf/1CTJk1UpUoVm6XJXbp00fvvv685c+aocuXKql+/vubOnWutFYDzsRh3mmkHAADgBOisAAAAp0ZYAQAATo2wAgAAnBphBQAAODXCCgAAcGqEFQAA4NQIKwAAwKkRVgAAgFMjrAAAAKdGWAEAAE6NsAIAAJza/wMQgxRL8pPx3QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ResNet24.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpm5o1d8ni/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpm5o1d8ni/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "2023-12-08 02:52:23.402835: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 02:52:23.402851: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 02:52:23.403507: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpm5o1d8ni\n",
      "2023-12-08 02:52:23.412432: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 02:52:23.412447: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpm5o1d8ni\n",
      "2023-12-08 02:52:23.429931: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2023-12-08 02:52:23.439233: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 02:52:23.759127: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpm5o1d8ni\n",
      "2023-12-08 02:52:23.852509: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 449002 microseconds.\n",
      "2023-12-08 02:52:23.949666: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231764"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResNet24.save('saved_models/ResNet24.keras')  # The file needs to end with the .keras extension\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ResNet24)\n",
    "ResNet24_tflite = converter.convert()\n",
    "# save the model\n",
    "open(\"saved_models/ResNet24.tflite\", \"wb\").write(ResNet24_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLa  (None, 50, 9)                3         ['input_1[0][0]']             \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " quant_reshape (QuantizeWra  (None, 1, 50, 9)             1         ['quantize_layer[0][0]']      \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d (QuantizeWrap  (None, 1, 48, 64)            1923      ['quant_reshape[0][0]']       \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 46, 64)            12483     ['quant_conv2d[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 23, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 23, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 23, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 23, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_10[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_12[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_14[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_16[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_13[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_11[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_17[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_18[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_19[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_21[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_22[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_23[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_20[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_25[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_26[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 11, 64)            3         ['quant_re_lu_20[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 704)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1415      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_ResNet24 = tfmot.quantization.keras.quantize_model(ResNet24)\n",
    "q_ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 12s 34ms/step - loss: 0.8733 - accuracy: 0.8202 - val_loss: 0.1523 - val_accuracy: 0.9536 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 8s 32ms/step - loss: 0.7457 - accuracy: 0.8483 - val_loss: 0.3700 - val_accuracy: 0.8665 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 8s 32ms/step - loss: 0.6108 - accuracy: 0.8633 - val_loss: 0.5915 - val_accuracy: 0.7821 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 9s 33ms/step - loss: 0.4585 - accuracy: 0.8926 - val_loss: 0.4015 - val_accuracy: 0.8497 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 9s 34ms/step - loss: 0.3682 - accuracy: 0.9110 - val_loss: 0.2633 - val_accuracy: 0.8978 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.9173\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 9s 33ms/step - loss: 0.3680 - accuracy: 0.9173 - val_loss: 0.2385 - val_accuracy: 0.9212 - lr: 5.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_ResNet24.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ResNet24.save('saved_models/q_ResNet24.keras')  # The file needs to end with the .keras extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpj1wo808_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpj1wo808_/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-08 02:53:26.700633: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 02:53:26.700649: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 02:53:26.701139: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpj1wo808_\n",
      "2023-12-08 02:53:26.710480: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 02:53:26.710501: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpj1wo808_\n",
      "2023-12-08 02:53:26.737496: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 02:53:27.051361: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpj1wo808_\n",
      "2023-12-08 02:53:27.139859: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 438718 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "\n",
    "# Convert the model\n",
    "tflite_model_quant_int8_qat = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105456"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"saved_models/ResNet24_quant_int8_qat.tflite\", \"wb\").write(tflite_model_quant_int8_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 50,  9], dtype=int32), 'shape_signature': array([-1, 50,  9], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 106, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#67 is a dynamic-sized tensor).\n"
     ]
    }
   ],
   "source": [
    "# test the quantized model\n",
    "X_test_int8 = X_test.astype('float32')\n",
    "y_test_int8 = y_test.astype('int8')\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= tflite_model_quant_int8_qat)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_int8, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model size with 8-bit quantization: 102 KB\n",
      "TFLite Model size without quantization: 226 KB\n",
      "\n",
      "Reduction in model size by a factor of 2.197732\n",
      "accuracy:  0.8938053097345132\n",
      "Confusion matrix, without normalization\n",
      "[[ 98  11]\n",
      " [ 13 104]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_in_kb = os.path.getsize('saved_models/ResNet24.tflite') / 1024\n",
    "# ResNet24_tflite tflite_quant_in_kb\n",
    "tflite_quant_in_kb = os.path.getsize('saved_models/ResNet24_quant_int8_qat.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (tflite_in_kb / tflite_quant_in_kb))\n",
    "\n",
    "accuracy = (predictions == gt).mean()\n",
    "print('accuracy: ', accuracy)\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of full precision model:  0.8982300884955752\n",
      "accuracy of quantized model:  0.8938053097345132\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of full precision model: ', accuracy_fp)\n",
    "print('accuracy of quantized model: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning + QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 9)             1         ['input_1[0][0]']             \n",
      " e (PruneLowMagnitude)                                                                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            3522      ['prune_low_magnitude_reshape[\n",
      "  (PruneLowMagnitude)                                               0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 46, 64)            24642     ['prune_low_magnitude_conv2d[0\n",
      " _1 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 23, 64)            1         ['prune_low_magnitude_conv2d_1\n",
      " oling2d (PruneLowMagnitude                                         [0][0]']                      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _3 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_3\n",
      " normalization (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu   (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu[0]\n",
      " _4 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_4\n",
      " normalization_1 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 1 (PruneLowMagnitude)                                              rmalization_1[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_1[\n",
      " _5 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_5\n",
      " normalization_2 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _2 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add (P  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " runeLowMagnitude)                                                  rmalization_2[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add[0][0\n",
      " 2 (PruneLowMagnitude)                                              ]']                           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_2[\n",
      " _7 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_7\n",
      " normalization_3 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 3 (PruneLowMagnitude)                                              rmalization_3[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_3[\n",
      " _8 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_4 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 4 (PruneLowMagnitude)                                              rmalization_4[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_4[\n",
      " _9 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_5 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_2[\n",
      " _6 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_1   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_5[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_6\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_1[0]\n",
      " 5 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_5[\n",
      " _10 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_6 (PruneLowM                                         0[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 6 (PruneLowMagnitude)                                              rmalization_6[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_6[\n",
      " _11 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_7 (PruneLowM                                         1[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 7 (PruneLowMagnitude)                                              rmalization_7[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_7[\n",
      " _12 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_8 (PruneLowM                                         2[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_2   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_8[0][0]',         \n",
      "                                                                     'prune_low_magnitude_re_lu_5[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_2[0]\n",
      " 8 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_8[\n",
      " _14 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_9 (PruneLowM                                         4[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 9 (PruneLowMagnitude)                                              rmalization_9[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_9[\n",
      " _15 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_10 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 10 (PruneLowMagnitude)                                             rmalization_10[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_10\n",
      " _16 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_11 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_8[\n",
      " _13 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_3   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_11[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_3[0]\n",
      " 11 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_11\n",
      " _17 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_12 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 12 (PruneLowMagnitude)                                             rmalization_12[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_12\n",
      " _18 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_13 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 13 (PruneLowMagnitude)                                             rmalization_13[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_13\n",
      " _19 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_14 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_4   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_14[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_11\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_4[0]\n",
      " 14 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_14\n",
      " _21 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_15 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 15 (PruneLowMagnitude)                                             rmalization_15[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_15\n",
      " _22 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_16 (PruneLow                                         2[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 16 (PruneLowMagnitude)                                             rmalization_16[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_16\n",
      " _23 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_17 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_14\n",
      " _20 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_5   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_17[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    0[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_5[0]\n",
      " 17 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_17\n",
      " _24 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_18 (PruneLow                                         4[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 18 (PruneLowMagnitude)                                             rmalization_18[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_18\n",
      " _25 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_19 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 19 (PruneLowMagnitude)                                             rmalization_19[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_19\n",
      " _26 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_20 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_6   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_20[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_17\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_6[0]\n",
      " 20 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 11, 64)            1         ['prune_low_magnitude_re_lu_20\n",
      " e_pooling2d (PruneLowMagni                                         [0][0]']                      \n",
      " tude)                                                                                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 704)                  1         ['prune_low_magnitude_average_\n",
      " n (PruneLowMagnitude)                                              pooling2d[0][0]']             \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense   (None, 2)                    2820      ['prune_low_magnitude_flatten[\n",
      " (PruneLowMagnitude)                                                0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 106895 (417.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 52973 (207.24 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(ResNet24, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 14s 25ms/step - loss: 0.3951 - accuracy: 0.9181 - val_loss: 0.3763 - val_accuracy: 0.8853 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.4837 - accuracy: 0.9077 - val_loss: 0.2190 - val_accuracy: 0.9400 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.4931 - accuracy: 0.9082 - val_loss: 0.3588 - val_accuracy: 0.8780 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.4182 - accuracy: 0.9210 - val_loss: 0.1729 - val_accuracy: 0.9395 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.3668 - accuracy: 0.9254 - val_loss: 0.2840 - val_accuracy: 0.9080 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.2843 - accuracy: 0.9395 - val_loss: 0.1451 - val_accuracy: 0.9468 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.3319 - accuracy: 0.9330 - val_loss: 0.2805 - val_accuracy: 0.9273 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.4501 - accuracy: 0.9201 - val_loss: 0.3590 - val_accuracy: 0.8641 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.3296 - accuracy: 0.9284 - val_loss: 0.1897 - val_accuracy: 0.9341 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.2332 - accuracy: 0.9475 - val_loss: 0.1294 - val_accuracy: 0.9544 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.2054 - accuracy: 0.9525 - val_loss: 0.1263 - val_accuracy: 0.9580 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.3357 - accuracy: 0.9265 - val_loss: 0.1961 - val_accuracy: 0.9319 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 6s 23ms/step - loss: 0.1959 - accuracy: 0.9547 - val_loss: 0.0946 - val_accuracy: 0.9685 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 6s 23ms/step - loss: 0.1776 - accuracy: 0.9594 - val_loss: 0.1323 - val_accuracy: 0.9585 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.1680 - accuracy: 0.9631 - val_loss: 0.1067 - val_accuracy: 0.9666 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.1388 - accuracy: 0.9683 - val_loss: 0.1177 - val_accuracy: 0.9634 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.1799 - accuracy: 0.9591 - val_loss: 0.1128 - val_accuracy: 0.9649 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.1248 - accuracy: 0.9706 - val_loss: 0.0840 - val_accuracy: 0.9729 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1437 - accuracy: 0.9702 - val_loss: 0.2139 - val_accuracy: 0.9378 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 7s 28ms/step - loss: 0.1776 - accuracy: 0.9622 - val_loss: 0.1534 - val_accuracy: 0.9627 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.2104 - accuracy: 0.9609 - val_loss: 0.0832 - val_accuracy: 0.9714 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1326 - accuracy: 0.9705 - val_loss: 0.1415 - val_accuracy: 0.9561 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1004 - accuracy: 0.9764 - val_loss: 0.0793 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1098 - accuracy: 0.9757 - val_loss: 0.0824 - val_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.1599 - accuracy: 0.9697 - val_loss: 0.1081 - val_accuracy: 0.9712 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1161 - accuracy: 0.9756 - val_loss: 0.0792 - val_accuracy: 0.9761 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1506 - accuracy: 0.9683 - val_loss: 0.1306 - val_accuracy: 0.9597 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9650\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1618 - accuracy: 0.9649 - val_loss: 0.0833 - val_accuracy: 0.9705 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.1268 - accuracy: 0.9707 - val_loss: 0.1045 - val_accuracy: 0.9639 - lr: 5.0000e-05\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.0985 - accuracy: 0.9752 - val_loss: 0.0955 - val_accuracy: 0.9675 - lr: 5.0000e-05\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 7s 27ms/step - loss: 0.0850 - accuracy: 0.9792 - val_loss: 0.0876 - val_accuracy: 0.9717 - lr: 5.0000e-05\n",
      "Epoch 31: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f5caeb80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, pruning_callbacks.UpdatePruningStep()],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.27141880989074707\n",
      "Pruned model accuracy:  0.9247787594795227\n",
      "Full-precision model accuracy:  0.8982300884955752\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# save the model\n",
    "pruned_model_unstructured.save('saved_models/ResNet24_pruned_unstructured.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpct30ebdz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpct30ebdz/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: /Users/liuxinqing/Documents/Fall_Detection/saved_models/ResNet24_pruned_unstructured.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 02:57:03.455807: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 02:57:03.455833: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 02:57:03.456058: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpct30ebdz\n",
      "2023-12-08 02:57:03.464065: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 02:57:03.464081: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpct30ebdz\n",
      "2023-12-08 02:57:03.483713: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 02:57:03.646118: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpct30ebdz\n",
      "2023-12-08 02:57:03.703207: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 247153 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = 'saved_models/ResNet24_pruned_unstructured.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  127740\n",
      "Size of the full-precision model:  201914\n",
      "The achieved compression ratio is 1.00x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(pruned_tflite_file_unstructured))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('saved_models/ResNet24.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('saved_models/ResNet24_pruned_unstructured.tflite') / get_gzipped_model_size(pruned_tflite_file_unstructured)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_1 (Quantize  (None, 50, 9)                3         ['input_1[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape (QuantizeWra  (None, 1, 50, 9)             1         ['quantize_layer_1[0][0]']    \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d (QuantizeWrap  (None, 1, 48, 64)            1923      ['quant_reshape[0][0]']       \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 46, 64)            12483     ['quant_conv2d[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 23, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 23, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 23, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 23, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_10[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_12[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_14[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_16[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_13[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_11[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_17[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_18[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_19[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_21[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_22[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_23[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_20[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_25[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_26[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 11, 64)            3         ['quant_re_lu_20[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 704)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1415      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (16390, 50, 9)\n",
      "y_train.shape:  (16390, 2)\n",
      "64\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 14s 38ms/step - loss: 0.9613 - accuracy: 0.7965 - val_loss: 0.0765 - val_accuracy: 0.9768 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 10s 39ms/step - loss: 0.5272 - accuracy: 0.8774 - val_loss: 0.2554 - val_accuracy: 0.8953 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 10s 37ms/step - loss: 0.4160 - accuracy: 0.9015 - val_loss: 0.2056 - val_accuracy: 0.9263 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 10s 38ms/step - loss: 0.3294 - accuracy: 0.9251 - val_loss: 0.1503 - val_accuracy: 0.9422 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 10s 37ms/step - loss: 0.2524 - accuracy: 0.9422 - val_loss: 0.1514 - val_accuracy: 0.9456 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9552\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 10s 38ms/step - loss: 0.1818 - accuracy: 0.9552 - val_loss: 0.1203 - val_accuracy: 0.9592 - lr: 5.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30113d100>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "print(batch_size)\n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, pruning_callbacks.UpdatePruningStep()],\n",
    "            class_weight=class_weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned QAT model loss:  0.1756395846605301\n",
      "Pruned QAT model accuracy:  0.9469026327133179\n",
      "Full-precision model accuracy:  0.8982300884955752\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_loss, pruned_qat_acc = pruned_qat_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned QAT model loss: ', pruned_qat_loss)\n",
    "print('Pruned QAT model accuracy: ', pruned_qat_acc)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpmpld3ma0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpmpld3ma0/assets\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "/Users/liuxinqing/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-08 02:58:19.906308: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-08 02:58:19.906326: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-08 02:58:19.906532: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpmpld3ma0\n",
      "2023-12-08 02:58:19.928002: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-08 02:58:19.928019: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpmpld3ma0\n",
      "2023-12-08 02:58:19.989127: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-08 02:58:20.578144: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /var/folders/6j/7v412jf92t1f972gcdmx7l8w0000gn/T/tmpmpld3ma0\n",
      "2023-12-08 02:58:20.752596: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 846065 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109104"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "open(\"saved_models/ResNet24_pruned_qat.tflite\", \"wb\").write(pruned_qat_tflite_model)\n",
    "\n",
    "# write TFLite model to a C source (or header) file\n",
    "#c_model_name = 'pruned_qat_fmnist'\n",
    "\n",
    "#with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "#    file.write(hex_to_c_array(pruned_qat_tflite_model, c_model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned QAT model:  53304\n",
      "Size of th QAT model:  69159\n",
      "Size of the full-precision model:  201914\n",
      "The achieved compression ratio is 3.79x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size('saved_models/ResNet24_pruned_qat.tflite'))\n",
    "print('Size of th QAT model: ', get_gzipped_model_size( 'saved_models/ResNet24_quant_int8_qat.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('saved_models/ResNet24.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('saved_models/ResNet24.tflite') / get_gzipped_model_size('saved_models/ResNet24_pruned_qat.tflite')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

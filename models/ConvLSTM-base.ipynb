{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=6, out_channels=64, kernel_size=3),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(in_features=64, out_features=32),\n",
    "                                 nn.BatchNorm1d(32),\n",
    "                                 nn.Dropout(0.5))\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=32)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=32, hidden_size=32)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.Linear(in_features=448, out_features=3),\n",
    "                                 nn.Softmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ConvLSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(6, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm1): LSTM(32, 32)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (lstm2): LSTM(32, 32)\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=448, out_features=3, bias=True)\n",
      "    (2): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.0.weight | Size: torch.Size([64, 6, 3]) | Values : tensor([[[ 0.1225, -0.1960,  0.2333],\n",
      "         [ 0.1284, -0.1142, -0.2060],\n",
      "         [ 0.0523,  0.1602,  0.1368],\n",
      "         [-0.0315, -0.0268,  0.1377],\n",
      "         [-0.1937,  0.0585,  0.2259],\n",
      "         [-0.0944,  0.0236, -0.1725]],\n",
      "\n",
      "        [[-0.1054, -0.1425,  0.1418],\n",
      "         [-0.1771, -0.1452, -0.1531],\n",
      "         [ 0.1589,  0.0804, -0.2021],\n",
      "         [ 0.1652, -0.0693,  0.0289],\n",
      "         [-0.0374, -0.1782,  0.0409],\n",
      "         [-0.1488, -0.2158, -0.0554]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0766, -0.1184], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-0.0584,  0.0453,  0.0303],\n",
      "         [-0.0693,  0.0591,  0.0352],\n",
      "         [-0.0225,  0.0615,  0.0451],\n",
      "         [-0.0538,  0.0225,  0.0048],\n",
      "         [-0.0307, -0.0573,  0.0513],\n",
      "         [-0.0383,  0.0208, -0.0556],\n",
      "         [ 0.0381,  0.0612,  0.0345],\n",
      "         [-0.0569, -0.0173,  0.0352],\n",
      "         [ 0.0295,  0.0513,  0.0292],\n",
      "         [-0.0131,  0.0657, -0.0644],\n",
      "         [ 0.0211, -0.0676, -0.0284],\n",
      "         [-0.0573,  0.0051, -0.0428],\n",
      "         [ 0.0385,  0.0171,  0.0249],\n",
      "         [ 0.0066,  0.0653,  0.0440],\n",
      "         [-0.0326, -0.0053,  0.0099],\n",
      "         [-0.0037, -0.0515, -0.0039],\n",
      "         [ 0.0094,  0.0275,  0.0086],\n",
      "         [-0.0455, -0.0272, -0.0262],\n",
      "         [-0.0356, -0.0505, -0.0132],\n",
      "         [ 0.0197,  0.0184,  0.0060],\n",
      "         [-0.0432,  0.0440, -0.0663],\n",
      "         [ 0.0531,  0.0332,  0.0237],\n",
      "         [-0.0633,  0.0684,  0.0663],\n",
      "         [ 0.0175,  0.0098, -0.0337],\n",
      "         [ 0.0578,  0.0216,  0.0226],\n",
      "         [-0.0024,  0.0512,  0.0249],\n",
      "         [ 0.0140, -0.0490,  0.0156],\n",
      "         [-0.0169, -0.0676,  0.0011],\n",
      "         [ 0.0067,  0.0280, -0.0147],\n",
      "         [-0.0057, -0.0563,  0.0227],\n",
      "         [ 0.0574, -0.0139, -0.0251],\n",
      "         [-0.0714, -0.0098, -0.0616],\n",
      "         [ 0.0192, -0.0636, -0.0291],\n",
      "         [ 0.0376,  0.0693,  0.0426],\n",
      "         [ 0.0256,  0.0544, -0.0036],\n",
      "         [-0.0439, -0.0329, -0.0270],\n",
      "         [-0.0322, -0.0176,  0.0153],\n",
      "         [-0.0092, -0.0461,  0.0286],\n",
      "         [ 0.0098, -0.0137,  0.0568],\n",
      "         [ 0.0327, -0.0442,  0.0208],\n",
      "         [ 0.0299,  0.0451,  0.0710],\n",
      "         [ 0.0541,  0.0275,  0.0405],\n",
      "         [-0.0274,  0.0289, -0.0226],\n",
      "         [ 0.0398, -0.0199,  0.0462],\n",
      "         [ 0.0199,  0.0085,  0.0537],\n",
      "         [ 0.0269,  0.0152, -0.0487],\n",
      "         [-0.0582, -0.0527,  0.0157],\n",
      "         [ 0.0698, -0.0043,  0.0220],\n",
      "         [-0.0095, -0.0008,  0.0270],\n",
      "         [ 0.0575, -0.0371,  0.0333],\n",
      "         [-0.0426, -0.0630,  0.0026],\n",
      "         [ 0.0010,  0.0110, -0.0675],\n",
      "         [ 0.0123, -0.0295, -0.0638],\n",
      "         [-0.0540, -0.0608,  0.0021],\n",
      "         [-0.0307,  0.0439,  0.0041],\n",
      "         [ 0.0329, -0.0242, -0.0509],\n",
      "         [ 0.0319, -0.0229,  0.0163],\n",
      "         [-0.0323, -0.0163, -0.0632],\n",
      "         [-0.0639, -0.0221, -0.0694],\n",
      "         [-0.0238,  0.0080,  0.0491],\n",
      "         [ 0.0371, -0.0544, -0.0706],\n",
      "         [-0.0519,  0.0062,  0.0561],\n",
      "         [ 0.0575,  0.0464,  0.0507],\n",
      "         [ 0.0596,  0.0234, -0.0023]],\n",
      "\n",
      "        [[-0.0257, -0.0472, -0.0410],\n",
      "         [-0.0023,  0.0586,  0.0099],\n",
      "         [-0.0226, -0.0692,  0.0300],\n",
      "         [-0.0319,  0.0504,  0.0061],\n",
      "         [ 0.0108, -0.0421, -0.0256],\n",
      "         [ 0.0359, -0.0254, -0.0283],\n",
      "         [-0.0263,  0.0461,  0.0499],\n",
      "         [-0.0384, -0.0170, -0.0060],\n",
      "         [ 0.0374,  0.0372, -0.0406],\n",
      "         [-0.0302,  0.0559,  0.0474],\n",
      "         [ 0.0342,  0.0335, -0.0260],\n",
      "         [ 0.0711,  0.0665, -0.0703],\n",
      "         [ 0.0141,  0.0075, -0.0430],\n",
      "         [-0.0180,  0.0202, -0.0573],\n",
      "         [-0.0500, -0.0356, -0.0491],\n",
      "         [ 0.0501, -0.0255,  0.0283],\n",
      "         [ 0.0543,  0.0496, -0.0201],\n",
      "         [ 0.0008, -0.0653,  0.0106],\n",
      "         [ 0.0033,  0.0144, -0.0622],\n",
      "         [-0.0592, -0.0102, -0.0059],\n",
      "         [-0.0403,  0.0040,  0.0397],\n",
      "         [-0.0404,  0.0108,  0.0069],\n",
      "         [ 0.0585,  0.0128, -0.0042],\n",
      "         [-0.0579, -0.0405, -0.0195],\n",
      "         [ 0.0206, -0.0537, -0.0386],\n",
      "         [-0.0229,  0.0689, -0.0069],\n",
      "         [-0.0022, -0.0701, -0.0327],\n",
      "         [ 0.0677, -0.0530, -0.0124],\n",
      "         [ 0.0586, -0.0151,  0.0193],\n",
      "         [ 0.0047,  0.0224,  0.0213],\n",
      "         [-0.0125, -0.0713,  0.0671],\n",
      "         [ 0.0023,  0.0372,  0.0376],\n",
      "         [ 0.0558, -0.0176, -0.0404],\n",
      "         [-0.0133, -0.0587,  0.0629],\n",
      "         [ 0.0023, -0.0241,  0.0391],\n",
      "         [-0.0633, -0.0246,  0.0452],\n",
      "         [-0.0129, -0.0496, -0.0378],\n",
      "         [ 0.0146, -0.0067,  0.0301],\n",
      "         [ 0.0263,  0.0240, -0.0053],\n",
      "         [ 0.0245, -0.0614, -0.0584],\n",
      "         [-0.0278, -0.0309, -0.0025],\n",
      "         [ 0.0357,  0.0402,  0.0383],\n",
      "         [-0.0092,  0.0265, -0.0556],\n",
      "         [-0.0515,  0.0506,  0.0173],\n",
      "         [ 0.0555, -0.0563, -0.0369],\n",
      "         [ 0.0193, -0.0569,  0.0093],\n",
      "         [-0.0413,  0.0097,  0.0045],\n",
      "         [ 0.0642,  0.0657,  0.0633],\n",
      "         [ 0.0246,  0.0646,  0.0407],\n",
      "         [-0.0411, -0.0513,  0.0399],\n",
      "         [ 0.0012,  0.0341,  0.0077],\n",
      "         [ 0.0382,  0.0505, -0.0170],\n",
      "         [ 0.0259,  0.0693, -0.0089],\n",
      "         [ 0.0548,  0.0607,  0.0263],\n",
      "         [ 0.0692,  0.0574,  0.0120],\n",
      "         [-0.0502, -0.0140, -0.0189],\n",
      "         [-0.0508,  0.0017, -0.0696],\n",
      "         [-0.0015, -0.0006,  0.0091],\n",
      "         [-0.0397,  0.0222,  0.0694],\n",
      "         [ 0.0583,  0.0265, -0.0444],\n",
      "         [-0.0024,  0.0192, -0.0126],\n",
      "         [ 0.0705, -0.0604, -0.0404],\n",
      "         [-0.0440,  0.0388,  0.0310],\n",
      "         [-0.0274, -0.0703,  0.0611]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0292, -0.0578], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[ 0.0268, -0.0266, -0.0413],\n",
      "         [-0.0221,  0.0087, -0.0075],\n",
      "         [-0.0266, -0.0437,  0.0425],\n",
      "         [ 0.0249, -0.0287, -0.0591],\n",
      "         [-0.0462,  0.0563, -0.0523],\n",
      "         [ 0.0079,  0.0704,  0.0159],\n",
      "         [-0.0242,  0.0199,  0.0585],\n",
      "         [ 0.0433,  0.0237,  0.0430],\n",
      "         [ 0.0046,  0.0580,  0.0720],\n",
      "         [ 0.0023,  0.0604,  0.0291],\n",
      "         [-0.0595, -0.0142,  0.0481],\n",
      "         [-0.0245, -0.0425,  0.0309],\n",
      "         [-0.0437,  0.0347,  0.0527],\n",
      "         [-0.0213, -0.0615, -0.0079],\n",
      "         [-0.0580,  0.0537, -0.0212],\n",
      "         [ 0.0055,  0.0184,  0.0239],\n",
      "         [ 0.0258,  0.0162,  0.0494],\n",
      "         [ 0.0136, -0.0537,  0.0566],\n",
      "         [ 0.0065,  0.0498,  0.0716],\n",
      "         [ 0.0550, -0.0210,  0.0230],\n",
      "         [ 0.0054,  0.0544,  0.0608],\n",
      "         [ 0.0300, -0.0553, -0.0356],\n",
      "         [-0.0709, -0.0147, -0.0433],\n",
      "         [-0.0323, -0.0145, -0.0429],\n",
      "         [ 0.0020, -0.0296,  0.0296],\n",
      "         [ 0.0259, -0.0530, -0.0428],\n",
      "         [-0.0068,  0.0662, -0.0245],\n",
      "         [ 0.0566, -0.0348, -0.0036],\n",
      "         [ 0.0035,  0.0030,  0.0560],\n",
      "         [ 0.0263, -0.0294,  0.0387],\n",
      "         [ 0.0023,  0.0515,  0.0635],\n",
      "         [ 0.0575, -0.0701, -0.0203],\n",
      "         [ 0.0106,  0.0645, -0.0635],\n",
      "         [ 0.0573, -0.0062,  0.0566],\n",
      "         [-0.0092,  0.0232,  0.0638],\n",
      "         [ 0.0522,  0.0226,  0.0491],\n",
      "         [ 0.0355,  0.0436,  0.0674],\n",
      "         [-0.0512, -0.0616,  0.0578],\n",
      "         [ 0.0155, -0.0545, -0.0446],\n",
      "         [-0.0575,  0.0295,  0.0036],\n",
      "         [ 0.0569,  0.0419, -0.0161],\n",
      "         [-0.0311,  0.0266, -0.0212],\n",
      "         [ 0.0183,  0.0473, -0.0625],\n",
      "         [ 0.0062, -0.0721, -0.0168],\n",
      "         [-0.0611, -0.0449, -0.0490],\n",
      "         [ 0.0517, -0.0010, -0.0053],\n",
      "         [ 0.0300, -0.0241,  0.0067],\n",
      "         [ 0.0221, -0.0701, -0.0526],\n",
      "         [ 0.0184, -0.0569, -0.0566],\n",
      "         [ 0.0177, -0.0103,  0.0289],\n",
      "         [-0.0064,  0.0276, -0.0666],\n",
      "         [-0.0333,  0.0620,  0.0092],\n",
      "         [-0.0697, -0.0202, -0.0416],\n",
      "         [-0.0202,  0.0387,  0.0144],\n",
      "         [ 0.0029, -0.0304,  0.0466],\n",
      "         [-0.0029, -0.0390,  0.0093],\n",
      "         [ 0.0341, -0.0447, -0.0338],\n",
      "         [ 0.0209, -0.0574, -0.0281],\n",
      "         [-0.0140, -0.0437, -0.0511],\n",
      "         [-0.0308, -0.0311, -0.0519],\n",
      "         [-0.0428, -0.0208, -0.0490],\n",
      "         [-0.0702, -0.0510, -0.0083],\n",
      "         [ 0.0308, -0.0655,  0.0447],\n",
      "         [-0.0071, -0.0298, -0.0641]],\n",
      "\n",
      "        [[ 0.0646, -0.0259, -0.0146],\n",
      "         [ 0.0070,  0.0240, -0.0588],\n",
      "         [ 0.0373,  0.0609,  0.0176],\n",
      "         [-0.0057, -0.0211,  0.0574],\n",
      "         [-0.0680,  0.0371,  0.0012],\n",
      "         [ 0.0060, -0.0544,  0.0006],\n",
      "         [-0.0535, -0.0246,  0.0464],\n",
      "         [ 0.0084,  0.0579, -0.0357],\n",
      "         [-0.0071,  0.0238,  0.0611],\n",
      "         [ 0.0532,  0.0565,  0.0119],\n",
      "         [-0.0029, -0.0468, -0.0188],\n",
      "         [-0.0622, -0.0188,  0.0007],\n",
      "         [-0.0246, -0.0459,  0.0532],\n",
      "         [-0.0650, -0.0323, -0.0330],\n",
      "         [ 0.0288,  0.0040, -0.0611],\n",
      "         [ 0.0470,  0.0560,  0.0288],\n",
      "         [-0.0070, -0.0015, -0.0385],\n",
      "         [-0.0634, -0.0583, -0.0459],\n",
      "         [ 0.0261,  0.0224, -0.0682],\n",
      "         [ 0.0551, -0.0374, -0.0266],\n",
      "         [-0.0239,  0.0404,  0.0323],\n",
      "         [-0.0637, -0.0100,  0.0465],\n",
      "         [ 0.0261, -0.0602,  0.0157],\n",
      "         [-0.0370, -0.0632, -0.0118],\n",
      "         [-0.0146,  0.0470, -0.0127],\n",
      "         [ 0.0089, -0.0316, -0.0279],\n",
      "         [ 0.0579, -0.0104,  0.0278],\n",
      "         [ 0.0547, -0.0565,  0.0575],\n",
      "         [ 0.0023, -0.0716,  0.0709],\n",
      "         [ 0.0597, -0.0657, -0.0630],\n",
      "         [-0.0570, -0.0647,  0.0351],\n",
      "         [ 0.0147,  0.0268, -0.0323],\n",
      "         [ 0.0131,  0.0195,  0.0523],\n",
      "         [ 0.0474, -0.0504, -0.0157],\n",
      "         [-0.0422,  0.0292, -0.0427],\n",
      "         [-0.0197,  0.0574,  0.0719],\n",
      "         [-0.0418, -0.0704,  0.0455],\n",
      "         [ 0.0440,  0.0084, -0.0282],\n",
      "         [-0.0593, -0.0058,  0.0033],\n",
      "         [-0.0519, -0.0471, -0.0492],\n",
      "         [ 0.0145,  0.0511, -0.0232],\n",
      "         [ 0.0236, -0.0333,  0.0401],\n",
      "         [-0.0603,  0.0046,  0.0496],\n",
      "         [ 0.0661, -0.0077, -0.0226],\n",
      "         [-0.0701,  0.0240, -0.0267],\n",
      "         [ 0.0206,  0.0241,  0.0593],\n",
      "         [ 0.0709,  0.0358,  0.0164],\n",
      "         [ 0.0157,  0.0477,  0.0380],\n",
      "         [ 0.0087,  0.0296,  0.0266],\n",
      "         [ 0.0530, -0.0102,  0.0267],\n",
      "         [ 0.0396,  0.0046,  0.0331],\n",
      "         [-0.0341,  0.0490, -0.0270],\n",
      "         [-0.0118, -0.0186, -0.0382],\n",
      "         [-0.0461,  0.0576, -0.0184],\n",
      "         [ 0.0005,  0.0429,  0.0170],\n",
      "         [-0.0598, -0.0672, -0.0261],\n",
      "         [-0.0095, -0.0166, -0.0561],\n",
      "         [-0.0628,  0.0698,  0.0417],\n",
      "         [ 0.0044, -0.0039, -0.0650],\n",
      "         [-0.0323, -0.0580,  0.0454],\n",
      "         [-0.0289,  0.0577,  0.0671],\n",
      "         [-0.0555, -0.0308,  0.0297],\n",
      "         [-0.0176,  0.0461,  0.0458],\n",
      "         [ 0.0156,  0.0654, -0.0515]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.bias | Size: torch.Size([64]) | Values : tensor([0.0576, 0.0717], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv4.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[ 5.8228e-02, -4.7001e-02,  3.2567e-02],\n",
      "         [ 6.3902e-02,  6.5290e-02, -5.2021e-02],\n",
      "         [-1.5413e-02,  3.5491e-02, -5.3707e-02],\n",
      "         [-3.0165e-02,  6.8870e-02,  1.5803e-02],\n",
      "         [-5.8566e-02,  1.9140e-02, -5.2075e-02],\n",
      "         [-1.7701e-02, -1.6263e-02, -6.3860e-02],\n",
      "         [-2.8341e-02, -3.6981e-02,  2.0044e-02],\n",
      "         [-6.8084e-02, -2.7693e-02, -4.2211e-02],\n",
      "         [-4.9875e-02, -5.5732e-02,  6.6049e-02],\n",
      "         [ 1.8640e-02,  4.9580e-02,  1.6212e-02],\n",
      "         [ 4.5817e-02,  6.8826e-02, -1.8333e-02],\n",
      "         [-5.3014e-02, -2.1129e-02, -1.5793e-02],\n",
      "         [-4.8775e-02, -6.8728e-02, -1.5191e-02],\n",
      "         [-2.1551e-03, -5.0908e-02,  4.2491e-02],\n",
      "         [-3.6114e-02, -5.2032e-02, -6.9200e-02],\n",
      "         [-5.3704e-02,  4.2687e-02,  3.4388e-03],\n",
      "         [ 2.6656e-02,  3.5356e-02, -4.4396e-02],\n",
      "         [ 4.2856e-02,  5.8832e-02, -3.2038e-02],\n",
      "         [ 2.1944e-02, -1.6351e-03,  1.4732e-02],\n",
      "         [ 1.5826e-02,  3.0179e-02,  6.2233e-02],\n",
      "         [-4.4202e-02, -4.0634e-02,  9.8674e-03],\n",
      "         [-4.7217e-02, -4.1813e-02,  7.1271e-02],\n",
      "         [ 7.0045e-02, -8.6648e-03, -5.5763e-02],\n",
      "         [-1.4716e-02, -2.0688e-02, -1.1737e-02],\n",
      "         [-1.7402e-03, -6.5935e-02,  4.1096e-02],\n",
      "         [-6.1210e-02, -7.1218e-02,  2.9949e-02],\n",
      "         [ 6.6553e-02, -6.8903e-02,  1.1750e-02],\n",
      "         [-4.5308e-02, -6.3301e-02,  6.6297e-02],\n",
      "         [-1.9651e-02,  6.6399e-04, -6.6746e-02],\n",
      "         [-5.5112e-03,  5.9422e-02,  2.0137e-02],\n",
      "         [ 7.1461e-03,  6.4574e-02, -4.5377e-03],\n",
      "         [-5.4567e-02, -1.8952e-02,  3.8829e-02],\n",
      "         [-4.7097e-02, -6.5159e-02, -6.9385e-02],\n",
      "         [ 3.3980e-02,  2.2343e-02, -3.3943e-02],\n",
      "         [ 5.6247e-02, -5.4173e-02,  5.0983e-02],\n",
      "         [-1.9078e-02,  4.3762e-02,  1.6917e-02],\n",
      "         [-3.3875e-02, -3.8149e-02,  2.7110e-02],\n",
      "         [ 4.4092e-02,  6.5566e-02, -6.1243e-02],\n",
      "         [-7.4906e-03,  4.0368e-02,  2.9138e-03],\n",
      "         [ 6.5788e-02, -5.5349e-02, -1.5756e-02],\n",
      "         [ 2.2439e-02, -5.5642e-02, -4.3859e-02],\n",
      "         [-1.3327e-02,  4.2429e-02,  3.4861e-02],\n",
      "         [-4.4318e-02, -3.4563e-02,  3.5118e-02],\n",
      "         [-3.2261e-02,  3.1016e-02,  2.5302e-02],\n",
      "         [ 6.9102e-02,  3.9797e-03, -3.7930e-02],\n",
      "         [-3.8546e-02,  4.6173e-02, -2.6706e-02],\n",
      "         [-6.1135e-02,  6.3730e-02,  1.1526e-02],\n",
      "         [-5.9723e-02,  1.1299e-02,  4.9346e-02],\n",
      "         [ 4.6176e-02, -1.0541e-02, -4.2822e-02],\n",
      "         [-6.4583e-02, -2.9619e-02, -4.8163e-02],\n",
      "         [-5.0588e-02, -1.6691e-02,  4.8655e-02],\n",
      "         [-5.5532e-02, -4.4820e-02,  6.1064e-02],\n",
      "         [ 3.2403e-02, -3.4083e-02,  3.5948e-02],\n",
      "         [ 1.6683e-02,  1.7993e-02, -2.9288e-02],\n",
      "         [-1.1078e-02, -7.8658e-03,  4.7813e-02],\n",
      "         [-3.4816e-02, -5.5423e-02, -2.5296e-02],\n",
      "         [ 6.8887e-03, -3.2400e-03, -6.4679e-05],\n",
      "         [ 1.4644e-02,  1.8144e-02,  5.0040e-02],\n",
      "         [-3.7826e-02,  5.0976e-02,  3.6446e-03],\n",
      "         [ 6.3498e-02,  5.6607e-02,  1.7010e-02],\n",
      "         [ 5.1606e-02, -6.8177e-02,  6.7657e-02],\n",
      "         [ 1.4132e-02,  3.8832e-02,  5.5250e-02],\n",
      "         [-4.0919e-02, -3.4654e-02, -6.4160e-03],\n",
      "         [ 2.8917e-02, -7.6903e-03,  3.8627e-02]],\n",
      "\n",
      "        [[ 6.0032e-02,  2.9570e-03,  1.3364e-02],\n",
      "         [ 5.1712e-02,  2.3943e-02,  1.6165e-02],\n",
      "         [ 4.1180e-02,  1.8294e-02, -5.1660e-02],\n",
      "         [-2.3876e-02,  4.2230e-02,  8.5744e-04],\n",
      "         [ 5.0129e-02, -6.3618e-02, -9.8203e-04],\n",
      "         [-1.2718e-02, -2.4992e-02,  1.8960e-03],\n",
      "         [-4.8251e-02,  1.1829e-02,  6.0372e-02],\n",
      "         [-7.1626e-02, -3.6173e-02,  4.3061e-02],\n",
      "         [ 5.3591e-02, -6.4869e-02, -3.7314e-02],\n",
      "         [-4.7649e-02, -7.0702e-02, -1.4486e-02],\n",
      "         [ 7.2340e-03,  2.9098e-02,  6.8742e-02],\n",
      "         [-1.9219e-02,  6.4507e-02,  4.8613e-02],\n",
      "         [-7.1346e-02,  9.7047e-03, -2.1351e-02],\n",
      "         [ 1.9722e-03, -1.5794e-02,  4.9949e-02],\n",
      "         [ 3.5376e-02,  3.9228e-02,  6.4789e-02],\n",
      "         [-4.8886e-02, -6.3884e-02,  4.2026e-02],\n",
      "         [ 1.0515e-02, -4.5751e-02,  6.5346e-02],\n",
      "         [-6.5282e-02, -4.2105e-02, -5.1613e-02],\n",
      "         [ 5.0039e-02,  4.4604e-03, -1.2312e-02],\n",
      "         [-6.7508e-04, -4.9276e-02, -5.6212e-03],\n",
      "         [ 2.8902e-02, -4.1704e-02, -6.3128e-02],\n",
      "         [-6.2750e-02, -7.0624e-02,  6.5593e-02],\n",
      "         [ 3.3415e-03, -2.6318e-02, -7.1761e-03],\n",
      "         [ 1.8024e-03,  3.3948e-02, -1.6420e-02],\n",
      "         [ 4.1053e-02, -2.6722e-02,  4.4370e-02],\n",
      "         [-2.8007e-02,  6.8777e-02, -4.6366e-03],\n",
      "         [-5.7608e-02,  5.6536e-02,  5.1400e-02],\n",
      "         [-2.2052e-02,  7.1346e-03,  5.4188e-02],\n",
      "         [ 3.5019e-02,  2.5888e-02, -5.3434e-02],\n",
      "         [-9.6097e-03,  9.3775e-06, -1.8185e-02],\n",
      "         [-2.7205e-02, -3.6388e-02, -3.3284e-02],\n",
      "         [-4.2498e-02,  5.8665e-02, -2.1095e-03],\n",
      "         [-3.2792e-02, -2.9715e-02,  4.0031e-02],\n",
      "         [-2.0551e-02,  5.5415e-02,  3.7556e-03],\n",
      "         [-5.4050e-03, -1.0769e-02,  1.9872e-02],\n",
      "         [ 2.6011e-02,  8.1414e-03, -1.8924e-02],\n",
      "         [-2.4263e-03, -4.9045e-02, -4.5501e-02],\n",
      "         [-5.6034e-02, -8.6418e-03,  5.1658e-02],\n",
      "         [ 7.5155e-03, -6.9472e-02, -1.3893e-02],\n",
      "         [-8.9402e-03,  5.6741e-02, -1.3078e-02],\n",
      "         [-2.2353e-02, -4.2051e-02, -5.4758e-02],\n",
      "         [ 3.9837e-02,  4.1598e-02, -6.7805e-02],\n",
      "         [ 2.7207e-02, -9.4560e-03,  4.3236e-02],\n",
      "         [-4.3348e-02,  6.0645e-02, -5.5666e-02],\n",
      "         [-6.6061e-02, -1.1307e-02,  3.3994e-02],\n",
      "         [-2.9407e-02, -1.9321e-02, -4.1680e-02],\n",
      "         [ 3.9309e-02, -2.1156e-02,  1.1396e-02],\n",
      "         [-1.5628e-02,  3.3545e-02, -1.6765e-02],\n",
      "         [ 3.5207e-03, -3.0076e-02,  4.5766e-02],\n",
      "         [ 5.9143e-02,  3.1994e-02, -1.1518e-02],\n",
      "         [-2.3971e-03, -3.8929e-02,  7.0381e-02],\n",
      "         [-4.7896e-02,  3.4510e-02,  4.0003e-02],\n",
      "         [-3.5852e-02, -3.5742e-02, -9.9436e-03],\n",
      "         [-5.4879e-02,  5.4644e-02, -3.7008e-02],\n",
      "         [-3.8264e-02,  2.3178e-02, -5.4100e-02],\n",
      "         [-5.3670e-02,  2.3994e-02,  4.8742e-03],\n",
      "         [-1.1602e-03, -9.6844e-04,  1.6592e-02],\n",
      "         [ 1.5477e-03, -4.2439e-03,  5.6960e-02],\n",
      "         [-3.4398e-02, -2.1392e-03,  3.8037e-03],\n",
      "         [ 5.2958e-02,  3.6842e-02,  5.6386e-02],\n",
      "         [-9.4396e-03,  7.1097e-02, -8.5757e-03],\n",
      "         [-2.6555e-02,  4.0405e-02, -5.0935e-02],\n",
      "         [ 4.4943e-02, -6.4702e-02,  2.7960e-02],\n",
      "         [ 4.4989e-02, -4.8595e-02, -2.8133e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv4.0.bias | Size: torch.Size([64]) | Values : tensor([ 0.0080, -0.0082], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv4.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv4.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.0.weight | Size: torch.Size([32, 64]) | Values : tensor([[ 0.0313, -0.0939,  0.0719,  0.1125,  0.0835,  0.0667,  0.0110,  0.0299,\n",
      "         -0.0730,  0.0872,  0.0347, -0.0906, -0.0692, -0.0257,  0.0596, -0.0839,\n",
      "         -0.0475,  0.0908, -0.0053,  0.0062, -0.1159,  0.0240,  0.0492,  0.0120,\n",
      "          0.0209,  0.0163,  0.1099, -0.0495, -0.0620, -0.1110,  0.0017, -0.0908,\n",
      "          0.1077, -0.0505,  0.0861,  0.0609,  0.0376,  0.0960,  0.1195,  0.0841,\n",
      "         -0.0248, -0.0125,  0.0822, -0.1044, -0.0806, -0.1070,  0.0816, -0.0816,\n",
      "         -0.1071,  0.0162,  0.0445,  0.0942,  0.0918, -0.0984, -0.1055, -0.0487,\n",
      "         -0.1198, -0.0812, -0.0821, -0.1036, -0.0242,  0.0421, -0.0937, -0.0562],\n",
      "        [ 0.0093, -0.0649, -0.1236, -0.0826, -0.0679, -0.1049,  0.0748,  0.0834,\n",
      "         -0.0260,  0.1082, -0.0378, -0.1243,  0.0399, -0.0780,  0.0202, -0.0765,\n",
      "          0.0816, -0.0982,  0.0181,  0.0711, -0.0380,  0.0593, -0.0921,  0.0054,\n",
      "          0.1047, -0.0808,  0.0978,  0.0123,  0.1016,  0.0642, -0.0028,  0.0084,\n",
      "         -0.0985,  0.0364, -0.0647, -0.0961, -0.0371, -0.0213,  0.0812,  0.1111,\n",
      "         -0.0269, -0.0160, -0.0343, -0.0006, -0.0109, -0.0085,  0.0572,  0.1012,\n",
      "          0.1117, -0.0373, -0.0862, -0.0444, -0.0131, -0.0319,  0.0351, -0.0595,\n",
      "          0.1207,  0.1203, -0.0201,  0.0073, -0.0349,  0.0285, -0.1100,  0.1129]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.0.bias | Size: torch.Size([32]) | Values : tensor([ 0.0934, -0.0273], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.1.weight | Size: torch.Size([32]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.1.bias | Size: torch.Size([32]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_ih_l0 | Size: torch.Size([128, 32]) | Values : tensor([[-0.1696, -0.1765,  0.1644, -0.1641,  0.0151, -0.0216,  0.1370,  0.0893,\n",
      "         -0.0515, -0.0255,  0.0836,  0.1712,  0.0663,  0.0885,  0.1564, -0.0780,\n",
      "          0.1089,  0.1118, -0.0981, -0.1386, -0.0664, -0.0710, -0.1501, -0.0803,\n",
      "          0.1103,  0.1169, -0.1237,  0.1658,  0.0856, -0.1025, -0.0328, -0.1110],\n",
      "        [ 0.0106, -0.0485, -0.0097,  0.1215, -0.0090,  0.0025,  0.0181,  0.1755,\n",
      "         -0.1047, -0.1328, -0.1706, -0.0086, -0.0628,  0.0436, -0.1014, -0.1110,\n",
      "         -0.1571, -0.0375, -0.0063,  0.0882,  0.0015,  0.0012, -0.0015,  0.1257,\n",
      "         -0.1418, -0.0614,  0.1670, -0.0645,  0.1618, -0.1448,  0.0049, -0.0878]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_hh_l0 | Size: torch.Size([128, 32]) | Values : tensor([[-0.1171, -0.0818, -0.0985, -0.1212, -0.1469,  0.1590, -0.1471,  0.1068,\n",
      "          0.1696,  0.0860,  0.0411, -0.0371, -0.0851,  0.1162,  0.0370, -0.0923,\n",
      "          0.0280,  0.0023,  0.0941, -0.1767,  0.1492,  0.0442,  0.0133, -0.1005,\n",
      "          0.1517, -0.0949,  0.1130, -0.1094, -0.1183,  0.0909, -0.0350, -0.0697],\n",
      "        [-0.0155, -0.0775, -0.1210, -0.1533,  0.0699,  0.0071, -0.1760, -0.1657,\n",
      "         -0.1355,  0.0106,  0.0418, -0.0554,  0.0555,  0.0124,  0.1723, -0.0359,\n",
      "          0.0996, -0.0895, -0.0008, -0.0173,  0.0408, -0.1544,  0.0957,  0.0698,\n",
      "          0.0639,  0.1226, -0.1367, -0.1218, -0.1629,  0.0233, -0.0300, -0.0242]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_ih_l0 | Size: torch.Size([128]) | Values : tensor([0.0344, 0.0702], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_hh_l0 | Size: torch.Size([128]) | Values : tensor([0.1043, 0.0964], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_ih_l0 | Size: torch.Size([128, 32]) | Values : tensor([[-0.1479, -0.1076, -0.1093,  0.0558,  0.0101, -0.0702, -0.0696,  0.1381,\n",
      "         -0.0008, -0.1338, -0.1488,  0.1354,  0.0318, -0.0597,  0.0691,  0.0458,\n",
      "         -0.0512, -0.0761, -0.1364,  0.1620, -0.1680,  0.0433, -0.1615,  0.0561,\n",
      "          0.0218, -0.0387,  0.0931,  0.0567, -0.0479, -0.0398,  0.0619,  0.0284],\n",
      "        [-0.0825,  0.0369,  0.1327,  0.0489,  0.1614,  0.0245,  0.0643,  0.1170,\n",
      "          0.0930, -0.1735,  0.1098, -0.0029, -0.1733, -0.0246,  0.0886, -0.1571,\n",
      "         -0.1617, -0.0255, -0.0488, -0.0680, -0.1387,  0.0950,  0.0114, -0.1567,\n",
      "         -0.1740, -0.1480, -0.1688,  0.0112,  0.1522,  0.1030,  0.1526, -0.1022]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_hh_l0 | Size: torch.Size([128, 32]) | Values : tensor([[ 0.0993, -0.0282,  0.1378, -0.0051, -0.1159,  0.1442, -0.1443,  0.0861,\n",
      "          0.0919, -0.0876,  0.1483,  0.1547, -0.0511,  0.0011, -0.1341,  0.0464,\n",
      "         -0.0492,  0.0093, -0.0770, -0.0330,  0.0245,  0.0186,  0.0895, -0.0714,\n",
      "         -0.0200,  0.1724, -0.0805, -0.0431, -0.0291,  0.1537, -0.1230, -0.0179],\n",
      "        [ 0.1569,  0.0229,  0.0117,  0.1199, -0.1453,  0.1498,  0.1323, -0.0921,\n",
      "          0.0448,  0.0573, -0.1243, -0.1092,  0.0082, -0.0288,  0.1177, -0.1617,\n",
      "          0.1219,  0.0522, -0.0528, -0.1131, -0.0858,  0.0725, -0.1215, -0.0266,\n",
      "          0.1036, -0.0875,  0.1036, -0.0385, -0.1611,  0.1072, -0.1248, -0.1163]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_ih_l0 | Size: torch.Size([128]) | Values : tensor([-0.0936, -0.0471], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_hh_l0 | Size: torch.Size([128]) | Values : tensor([-0.0293, -0.0617], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.1.weight | Size: torch.Size([3, 448]) | Values : tensor([[ 3.0298e-02,  2.3475e-02,  2.7132e-02, -3.6603e-02,  4.3706e-02,\n",
      "         -1.2434e-02, -3.7014e-02,  2.8728e-02, -1.3177e-02,  2.0085e-02,\n",
      "          1.9862e-02,  2.5255e-02,  4.1800e-02,  8.0727e-03, -3.0324e-02,\n",
      "          8.2325e-03,  5.0936e-03,  1.5616e-02, -2.0736e-02,  2.7306e-02,\n",
      "          1.2669e-02,  2.0523e-02,  3.4699e-02,  3.1383e-02, -3.6132e-02,\n",
      "         -2.4853e-02,  1.9218e-02, -4.3565e-02,  4.4167e-02,  1.9582e-02,\n",
      "          3.1379e-03,  3.9149e-02, -3.5246e-02, -4.1923e-02,  2.9611e-02,\n",
      "          1.9193e-02, -2.1635e-02,  7.7492e-03,  2.0991e-03, -9.4122e-03,\n",
      "          3.7319e-02, -3.0702e-02,  1.2829e-02, -4.1148e-02,  2.0007e-02,\n",
      "         -2.2549e-02,  1.2371e-02,  1.2661e-02, -3.8415e-02,  2.6763e-02,\n",
      "          2.2212e-02,  7.8488e-03,  4.6547e-02, -1.7386e-02,  1.0874e-02,\n",
      "         -2.5671e-02,  3.7607e-02, -6.2951e-03, -2.7110e-02,  3.9123e-02,\n",
      "          3.2763e-02, -8.1594e-03,  4.2008e-03,  1.3896e-02,  1.1775e-02,\n",
      "          1.4277e-02, -3.5630e-02,  1.4700e-02,  4.4071e-02,  4.0273e-02,\n",
      "          2.3955e-02, -2.0588e-02,  1.9921e-03, -9.9528e-03, -4.2546e-02,\n",
      "         -6.9564e-04,  1.6610e-02, -2.9817e-02, -3.5253e-02, -1.3401e-02,\n",
      "          4.5243e-03, -5.5554e-03,  4.4646e-02,  2.8168e-02, -4.1505e-02,\n",
      "          1.3160e-02,  2.6172e-02,  4.6458e-02, -2.6781e-02,  3.9742e-02,\n",
      "          4.0641e-02, -4.1428e-03,  1.3529e-02, -9.0350e-03,  3.6824e-02,\n",
      "         -3.4388e-02,  2.8838e-02,  2.7104e-02,  8.1900e-03, -4.7108e-02,\n",
      "          3.4070e-02, -3.8369e-02,  1.4091e-02,  1.0680e-02, -2.7454e-02,\n",
      "         -2.0597e-02,  2.7748e-02, -3.6885e-02, -9.5160e-05,  1.1140e-02,\n",
      "         -3.4952e-02, -1.9202e-02,  1.8462e-02, -1.0784e-02, -4.3577e-02,\n",
      "          1.7658e-02,  2.0439e-02, -8.8316e-03,  3.7031e-02,  3.7817e-02,\n",
      "          4.3443e-02, -3.4808e-02, -3.9879e-02,  1.1105e-02,  4.0151e-03,\n",
      "          5.4714e-03,  3.1383e-02,  8.9436e-04, -3.3895e-02, -3.0262e-02,\n",
      "          3.1866e-02, -4.0197e-02,  2.8967e-02, -8.3961e-03, -1.5845e-02,\n",
      "          7.4700e-03, -1.9469e-02, -3.7861e-02, -3.0649e-02, -3.3703e-02,\n",
      "          2.2564e-02,  1.8353e-02, -4.6448e-02,  9.1444e-03, -1.9599e-02,\n",
      "         -1.0719e-02, -3.2952e-03,  2.8645e-02,  3.5714e-02,  1.9118e-02,\n",
      "         -4.5604e-02, -3.8040e-02,  1.0300e-02, -3.1528e-02, -3.3512e-03,\n",
      "          4.5618e-02,  1.5569e-02,  1.4053e-02,  2.4539e-02,  2.7572e-02,\n",
      "          1.5832e-02,  4.4007e-02, -9.9672e-03,  2.3215e-02, -4.6895e-03,\n",
      "         -1.2055e-02,  2.6974e-02,  3.0905e-02,  3.9085e-02,  4.4015e-02,\n",
      "          3.9318e-05, -1.1842e-03,  1.6062e-02,  4.1903e-02, -3.8909e-03,\n",
      "          1.5945e-03, -1.2112e-02,  2.1023e-02,  2.9114e-02, -2.4355e-02,\n",
      "          3.3162e-02, -4.6808e-02,  4.2770e-02,  3.5420e-02, -2.6104e-03,\n",
      "         -5.5051e-04,  3.2764e-02, -2.9575e-02, -7.6767e-03,  1.8051e-02,\n",
      "         -3.8177e-02,  3.3480e-02, -3.3762e-02, -2.8522e-02,  8.7332e-03,\n",
      "          2.5958e-02,  2.7773e-02, -4.2844e-02, -3.5599e-02,  2.0544e-02,\n",
      "         -3.0919e-02, -1.7356e-03, -1.6404e-02, -2.2952e-02, -3.5548e-02,\n",
      "         -3.6518e-02, -6.3597e-03,  4.7206e-02,  4.5235e-02, -2.3809e-02,\n",
      "          3.3010e-02,  5.3318e-03, -2.0526e-02,  3.2909e-02,  1.0067e-02,\n",
      "         -8.3897e-03,  4.0425e-02, -1.9343e-02,  4.4172e-02, -3.1598e-02,\n",
      "         -1.0079e-02, -6.5767e-03,  4.1257e-02,  3.2630e-02, -1.6873e-02,\n",
      "         -1.0054e-02,  9.1323e-03,  1.2523e-02, -7.9392e-04,  4.3259e-02,\n",
      "         -1.2924e-02,  7.3147e-03,  2.0230e-02, -1.8275e-02,  4.3338e-02,\n",
      "         -1.4389e-02,  1.0036e-02,  5.4330e-03,  4.2900e-02,  1.4643e-02,\n",
      "          1.8230e-02,  4.4792e-02,  1.6786e-02, -3.3709e-02,  4.1568e-02,\n",
      "         -1.8856e-02, -2.4507e-02,  5.5535e-03,  1.9841e-02,  4.3850e-02,\n",
      "          3.6522e-02, -3.5145e-03,  2.3373e-02, -2.0510e-02,  3.4659e-02,\n",
      "         -1.9122e-03, -5.6969e-03, -2.1325e-02,  4.6257e-02, -7.3885e-03,\n",
      "         -3.5533e-02, -3.9812e-02,  7.9194e-03, -1.9543e-02, -9.3010e-04,\n",
      "          4.3161e-02, -2.2858e-02, -2.7835e-02, -2.0258e-04,  5.3043e-03,\n",
      "         -1.6250e-02,  1.6986e-03, -1.1227e-02,  1.3422e-03, -1.2249e-02,\n",
      "          2.8437e-02,  3.8750e-02, -2.0094e-02, -2.7467e-02,  7.6919e-03,\n",
      "         -3.5829e-02,  2.5727e-02,  4.4646e-02,  3.4669e-02, -4.3026e-02,\n",
      "         -3.2657e-02,  4.2584e-02, -2.1447e-02,  2.9145e-02, -3.6782e-02,\n",
      "          2.6200e-02,  4.1457e-02, -4.1573e-02,  1.9383e-02, -2.5849e-02,\n",
      "         -9.2461e-03, -4.3552e-02,  1.4127e-02, -3.0991e-02,  2.7520e-02,\n",
      "          3.7562e-02,  3.8225e-02,  3.1941e-02,  7.6029e-03,  4.3628e-02,\n",
      "          6.8909e-03,  8.9212e-03,  2.6184e-03,  4.2944e-02, -3.9067e-02,\n",
      "          7.7113e-03,  2.5160e-02,  3.2132e-02,  3.7138e-02,  4.2165e-02,\n",
      "          1.2960e-02,  3.6570e-03,  1.8536e-02,  3.8370e-02,  3.4007e-02,\n",
      "          1.8892e-02,  3.0583e-02, -3.6795e-02,  4.6019e-02, -1.0971e-02,\n",
      "          9.3726e-03, -2.8201e-02,  2.7031e-02, -1.5250e-02,  1.0509e-02,\n",
      "          3.5641e-02,  1.8562e-03,  9.0318e-03,  4.3419e-02,  1.7457e-02,\n",
      "          1.1753e-02,  1.2089e-02, -1.9719e-02, -1.1229e-02, -3.1384e-02,\n",
      "         -1.3543e-02,  3.9993e-02, -1.3888e-02, -9.2769e-03, -1.2537e-02,\n",
      "         -7.2945e-03,  3.6686e-02,  4.4691e-02,  1.3276e-02, -3.5909e-02,\n",
      "         -4.2621e-02, -4.8256e-03, -2.6783e-02, -2.4824e-02,  3.9250e-02,\n",
      "          2.8682e-02,  1.7853e-02, -1.1485e-02,  4.3108e-02, -1.8255e-02,\n",
      "         -1.7489e-02,  3.7113e-02, -4.2178e-02, -2.1669e-02,  1.7668e-02,\n",
      "         -3.0937e-02, -2.8479e-02,  2.8310e-02,  1.0954e-02, -4.5474e-02,\n",
      "         -1.4367e-02, -2.9370e-02, -4.2634e-02,  3.7439e-03,  1.6882e-02,\n",
      "         -2.4063e-02,  3.7509e-02, -3.6986e-02,  2.8286e-02,  3.7898e-02,\n",
      "          1.3261e-02, -7.1236e-03,  3.7379e-02, -3.9451e-02,  4.5848e-02,\n",
      "          3.8928e-02,  1.9860e-02, -1.1923e-02,  1.8570e-02,  2.6457e-02,\n",
      "         -3.1136e-03, -1.9987e-02, -3.1686e-02,  4.6587e-02, -5.7132e-03,\n",
      "         -3.4893e-02, -3.3354e-02,  1.3578e-02, -9.9495e-03,  1.9640e-02,\n",
      "          3.7881e-02,  1.3906e-02,  4.2939e-02,  2.0602e-02, -1.6427e-02,\n",
      "         -4.2540e-02, -2.4018e-02,  8.3290e-03,  4.1739e-02, -3.5814e-02,\n",
      "          3.8684e-03, -2.0531e-02, -4.2791e-02, -2.9442e-02,  4.2681e-02,\n",
      "          2.8893e-02,  2.2802e-02, -4.5304e-02, -3.8997e-02, -2.5527e-02,\n",
      "         -4.4815e-02,  3.7697e-02, -1.1286e-02, -1.2191e-02,  2.4749e-02,\n",
      "          2.9397e-02, -7.9440e-03,  2.3589e-02, -3.3579e-02, -4.6512e-03,\n",
      "          7.8510e-03,  1.9743e-02, -1.4691e-02,  4.7179e-02, -2.1270e-03,\n",
      "         -4.3021e-02, -3.3678e-02,  2.7377e-02, -1.4127e-02,  4.2169e-02,\n",
      "         -1.1979e-02, -3.3321e-02,  5.7057e-03, -3.6766e-03,  8.4141e-03,\n",
      "          3.3493e-02,  3.2023e-02,  1.2560e-02],\n",
      "        [ 2.6881e-03, -4.3537e-02, -3.0950e-02, -7.8596e-03,  4.5291e-02,\n",
      "          2.5042e-02, -3.1126e-02, -2.8076e-02,  9.5071e-03, -8.5326e-03,\n",
      "         -1.9264e-02, -1.6942e-02, -1.6629e-02, -2.8507e-02,  2.0845e-02,\n",
      "          2.5410e-02, -3.9413e-02, -1.7930e-02, -2.1740e-02, -3.9573e-02,\n",
      "         -3.8065e-02, -2.7655e-02,  3.6950e-02, -1.0113e-02,  5.0162e-03,\n",
      "          2.9955e-02, -1.5732e-02, -3.0115e-02,  4.1543e-02,  4.1271e-02,\n",
      "          2.2130e-02, -1.2096e-02, -2.1788e-03, -3.4685e-02, -4.8537e-03,\n",
      "          3.5581e-02, -4.1961e-02, -3.6874e-02, -2.7881e-02, -8.0805e-03,\n",
      "          4.7130e-02,  4.2440e-02,  3.0486e-02, -3.6336e-02,  2.9364e-02,\n",
      "         -7.1681e-03,  1.5930e-02,  2.5417e-02, -5.6874e-03,  1.4818e-04,\n",
      "          1.6354e-02,  1.0897e-02, -1.0319e-03,  1.4780e-03,  4.6058e-02,\n",
      "         -2.7889e-02, -1.0909e-02, -4.0810e-02,  2.4261e-02, -3.7853e-02,\n",
      "          2.2670e-02,  1.9586e-02, -1.7746e-02, -4.4703e-02,  3.7907e-02,\n",
      "          2.6184e-03, -3.5749e-02, -4.4964e-02,  3.8676e-02,  2.3943e-02,\n",
      "         -4.3091e-02,  2.2752e-02, -1.8456e-02, -4.3145e-02, -3.4948e-03,\n",
      "          7.2836e-03, -1.1161e-02,  8.4427e-03,  1.9383e-02, -3.9675e-03,\n",
      "          4.6357e-02,  3.1611e-02, -1.8096e-02, -6.5164e-03,  4.1382e-03,\n",
      "         -5.3541e-04, -3.7221e-02,  4.2921e-02,  3.1247e-02, -2.7125e-02,\n",
      "         -3.2581e-02,  1.8280e-02,  4.5702e-03,  6.5953e-03, -2.7258e-02,\n",
      "         -5.6479e-03, -2.2084e-02, -2.4296e-02,  2.5985e-02,  3.8581e-02,\n",
      "          2.2994e-02,  3.2308e-02, -4.1340e-02, -1.6562e-02, -1.8051e-02,\n",
      "         -1.3721e-02, -5.6254e-03, -2.6821e-02, -4.2424e-02, -6.0606e-03,\n",
      "         -4.1089e-02,  2.5168e-02, -2.7975e-02,  3.2325e-02,  1.0242e-02,\n",
      "          5.5851e-03,  2.7573e-02,  4.6718e-02,  2.3191e-02,  4.0012e-02,\n",
      "         -2.5148e-02, -3.8117e-02, -1.8244e-02,  3.4865e-02, -2.4863e-02,\n",
      "          1.1996e-03, -4.7996e-03,  7.4431e-03, -3.3241e-02, -1.3816e-02,\n",
      "         -2.8455e-02,  4.1463e-02,  3.9788e-02,  3.8490e-02, -1.1720e-02,\n",
      "         -3.8393e-02, -1.8087e-02,  2.3452e-02, -4.6866e-02,  1.2453e-02,\n",
      "          1.5314e-02, -3.8530e-02,  4.4078e-02,  7.2973e-03,  3.0089e-02,\n",
      "         -3.1244e-03,  1.1922e-02, -4.1295e-02,  2.6628e-03,  2.6307e-02,\n",
      "          1.9203e-02, -3.9170e-02,  9.6109e-03,  4.8798e-03,  2.3324e-02,\n",
      "         -4.5720e-02,  4.4564e-02,  4.1821e-02,  3.3826e-03,  2.7659e-03,\n",
      "         -1.1163e-02, -1.4570e-02,  1.6377e-02, -1.9894e-02,  4.8395e-03,\n",
      "         -4.0526e-02, -8.2927e-03, -4.1335e-02, -3.1341e-02, -1.1501e-02,\n",
      "          1.5234e-02,  3.4809e-02, -3.5200e-02, -4.5392e-02,  2.9292e-02,\n",
      "          8.1679e-03,  1.6790e-02, -2.7797e-02, -1.3212e-02, -1.5896e-02,\n",
      "         -2.7709e-02,  4.8569e-03,  9.8514e-03,  1.9137e-02,  4.3600e-02,\n",
      "          1.3963e-02, -2.8366e-02, -2.3160e-02, -8.8908e-04,  4.9952e-03,\n",
      "          3.7107e-04, -3.2250e-02,  1.2033e-02,  1.3038e-02,  1.8583e-02,\n",
      "          4.1612e-02,  2.1621e-02, -4.2383e-02, -2.5241e-04,  3.3443e-02,\n",
      "         -1.9941e-02, -3.5796e-02,  1.2945e-02, -2.6945e-02, -1.3402e-02,\n",
      "         -3.8013e-04,  1.4412e-02,  2.8798e-02, -1.1772e-02, -1.2596e-02,\n",
      "         -2.8789e-02,  6.0608e-03,  3.8746e-02,  1.9390e-02, -2.1865e-02,\n",
      "         -1.1022e-02, -4.3215e-02,  2.7963e-02,  2.0335e-02,  9.3705e-03,\n",
      "          1.2571e-02,  2.7731e-02, -4.7095e-02,  1.4795e-02,  4.6591e-02,\n",
      "          4.1126e-02,  1.6264e-02,  2.0091e-02,  8.7955e-03,  3.6636e-04,\n",
      "         -3.9277e-02, -2.6321e-02, -4.2742e-02, -4.6149e-03,  1.9623e-02,\n",
      "          4.1303e-02,  3.5263e-02, -8.8997e-03, -3.0357e-03, -3.7259e-02,\n",
      "         -2.1835e-02,  3.2113e-02, -1.5306e-02, -2.2697e-02,  1.1372e-02,\n",
      "         -1.9378e-02,  5.3363e-03,  3.8461e-03,  2.5829e-02, -3.9851e-02,\n",
      "         -2.0611e-03,  1.9487e-02,  3.8234e-02, -4.5734e-03, -1.3190e-02,\n",
      "         -4.0083e-02,  1.5943e-02,  4.5996e-02, -3.7022e-02, -1.7835e-02,\n",
      "          3.7418e-03,  3.3620e-02, -2.0082e-03,  9.9102e-03,  3.0359e-02,\n",
      "         -2.2608e-02,  7.5488e-03,  3.1012e-02, -4.0886e-02, -2.2401e-02,\n",
      "          3.1886e-02,  4.4893e-02,  2.3635e-02, -3.3568e-02,  2.3656e-02,\n",
      "         -3.0184e-02, -3.4921e-02,  4.3558e-02,  2.0042e-02, -4.2657e-02,\n",
      "          1.9565e-02, -2.5843e-02, -1.0498e-02, -2.4664e-02, -1.7845e-02,\n",
      "          1.5266e-02,  6.6996e-03,  3.6369e-02,  2.1921e-02, -4.1117e-03,\n",
      "         -2.6839e-02,  2.1834e-02, -3.1332e-02,  4.2007e-02, -4.6584e-02,\n",
      "          3.7733e-02,  2.9936e-02,  4.1488e-03,  3.6123e-02, -2.2689e-02,\n",
      "          6.9409e-03, -2.2720e-03, -1.0198e-02,  3.9276e-02,  1.6645e-03,\n",
      "         -1.0695e-02,  1.4775e-02, -3.6836e-02,  8.0290e-03, -4.0835e-02,\n",
      "          3.3631e-02,  3.5074e-02, -2.9780e-02,  3.7258e-02, -4.1553e-02,\n",
      "         -3.2290e-02,  1.2114e-02,  3.9751e-02, -3.7865e-02,  2.5521e-02,\n",
      "         -3.3313e-02, -2.3627e-03,  3.3908e-02, -7.1821e-03, -1.9798e-02,\n",
      "         -2.1385e-02, -1.2963e-02,  9.4297e-03,  2.1514e-02, -1.6805e-02,\n",
      "          8.4777e-03, -1.2704e-02,  5.9231e-03,  1.3657e-02, -6.5918e-03,\n",
      "          4.4976e-02,  1.9789e-02, -3.5741e-02,  4.5915e-02,  2.2255e-02,\n",
      "         -1.4870e-02, -9.2249e-03,  2.1663e-02,  2.3318e-02,  1.8785e-02,\n",
      "         -5.7394e-03, -6.3578e-04,  2.8675e-02,  7.5101e-03,  4.4472e-02,\n",
      "          1.6356e-02,  3.9306e-02,  3.1027e-02, -5.2246e-03, -1.6265e-02,\n",
      "         -1.8675e-02, -3.7466e-03,  2.1461e-03,  4.0138e-02,  3.7170e-02,\n",
      "          3.2046e-02, -4.3906e-02,  2.2161e-02, -4.5958e-02, -2.6078e-02,\n",
      "         -3.6009e-02, -1.8706e-02,  4.2137e-02,  2.4170e-03, -2.5363e-02,\n",
      "          7.9882e-03,  2.8069e-02,  4.2737e-02, -3.4334e-02, -2.8871e-02,\n",
      "          9.8460e-03,  4.3759e-02, -4.7006e-02,  3.5327e-02,  4.1996e-02,\n",
      "         -3.7682e-02, -1.3531e-02,  8.8662e-03,  4.0869e-02,  2.0170e-02,\n",
      "          2.8900e-02,  3.3941e-02,  2.1144e-03,  3.4198e-02, -3.6965e-03,\n",
      "          2.3070e-02, -3.7759e-02, -2.3442e-02,  1.5096e-02, -3.4765e-02,\n",
      "         -2.7782e-02, -1.7883e-02,  5.2542e-03,  3.3339e-02,  2.1961e-03,\n",
      "         -3.3977e-02,  2.5684e-02, -2.3795e-02, -2.0039e-02, -1.2022e-02,\n",
      "         -3.2876e-02,  4.6111e-02,  1.2098e-02, -6.7944e-03, -4.4430e-02,\n",
      "         -1.1128e-02, -4.6059e-02, -2.1657e-02, -4.1179e-02,  1.4106e-02,\n",
      "          9.3666e-03, -2.0016e-02,  5.3891e-03,  3.4941e-02,  4.2718e-02,\n",
      "          1.4871e-02,  1.2442e-02, -5.7177e-03, -1.1526e-02, -3.1444e-02,\n",
      "          1.9253e-02, -1.9421e-03, -4.2213e-02,  3.7001e-02, -1.7627e-02,\n",
      "         -4.0989e-02, -3.6752e-02, -2.7108e-02,  2.4009e-02, -1.5266e-02,\n",
      "         -3.6015e-03,  2.6200e-03,  2.9245e-02, -2.0415e-02, -1.1435e-02,\n",
      "         -5.2811e-03,  2.9757e-02, -6.5832e-03,  2.4499e-02, -1.4188e-02,\n",
      "          2.6807e-02,  3.7409e-02,  2.6624e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.1.bias | Size: torch.Size([3]) | Values : tensor([-0.0142, -0.0327], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = ConvLSTM().to(device)\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlonmcu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "#sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.1\n",
    "num_window_fall_data = 30\n",
    "num_window_not_fall_data = 3\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  50\n",
      "data.shape:  (15476, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# faltten the data\n",
    "\n",
    "#data = data.reshape(data.shape[0], -1)\n",
    "reshaped_data = data\n",
    "in_channels = reshaped_data.shape[1]\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', reshaped_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 9)\n",
      "X_train_tensor.dtype:  torch.float64\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# create a validate set\n",
    "\n",
    "# create test/validation/test data\n",
    "\"\"\" X_train, X_test, y_train, y_test = train_test_split(reshaped_data, \n",
    "                                                    label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) \"\"\"\n",
    "label = label.astype(np.int64)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reshaped_data, label, test_size=0.05, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "#print('X_test_false.shape: ', X_test_false.shape)\n",
    "\n",
    "#print('len(X_test): ', len)\n",
    "#print('X_test.shape: ', X_test.shape)\n",
    "#print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "y_val_tensor = torch.from_numpy(y_val)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "# print datatype of X_train_tensor\n",
    "X_train_tensor = X_train_tensor.double()\n",
    "print('X_train_tensor.dtype: ', X_train_tensor.dtype)\n",
    "X_test = X_train_tensor.double()\n",
    "\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(64, 2)  # No need for softmax here using nn.CrossEntropyLoss\n",
    "        #self.fc = nn.Sequential(nn.Linear(in_features=64, out_features=2),nn.Softmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = x.transpose(1, 2)  # Transpose to have the correct dimensions for Conv1d (batch, channels, length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        x = x.transpose(1, 2)  # Transpose back to (batch, seq_len, features)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Take the outputs of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # do i need softmax here?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ConvLSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm1): LSTM(64, 64, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (lstm2): LSTM(64, 64, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.0.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[-0.0565,  0.1015, -0.1445],\n",
      "         [-0.1226, -0.1271,  0.0826],\n",
      "         [-0.1791, -0.0012, -0.0069],\n",
      "         [ 0.0114,  0.1187,  0.0707],\n",
      "         [ 0.1761, -0.1436,  0.1191],\n",
      "         [-0.0612, -0.1417, -0.0850],\n",
      "         [ 0.0301,  0.1033,  0.1663],\n",
      "         [ 0.1372,  0.0079, -0.0075],\n",
      "         [ 0.0720,  0.0359,  0.0882]],\n",
      "\n",
      "        [[ 0.1412,  0.0953, -0.0710],\n",
      "         [-0.0614, -0.1884, -0.1065],\n",
      "         [ 0.0798, -0.0924,  0.1825],\n",
      "         [ 0.1682,  0.1119,  0.1167],\n",
      "         [ 0.0612,  0.1549,  0.0060],\n",
      "         [ 0.1627,  0.0109,  0.1827],\n",
      "         [ 0.0970, -0.1543, -0.1670],\n",
      "         [ 0.0289, -0.0196,  0.1608],\n",
      "         [ 0.1058,  0.0869,  0.1581]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0262,  0.0333], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-1.4575e-02,  3.0615e-02, -4.2071e-02],\n",
      "         [ 1.4044e-02,  5.7152e-02,  1.6166e-02],\n",
      "         [-5.8070e-02,  2.7377e-02, -6.7907e-02],\n",
      "         [-3.3713e-02,  6.6954e-02, -6.4689e-02],\n",
      "         [ 4.3105e-02, -1.4844e-02,  4.6234e-02],\n",
      "         [-4.5827e-03, -5.8507e-02,  2.1648e-02],\n",
      "         [-5.0948e-02,  5.6650e-02, -6.5510e-02],\n",
      "         [-1.4197e-03,  1.3576e-02,  3.9514e-02],\n",
      "         [-6.8360e-02,  9.2439e-03,  6.6397e-02],\n",
      "         [ 2.6434e-02,  4.9335e-02,  2.8253e-02],\n",
      "         [ 7.0171e-02,  1.8531e-02, -5.6422e-02],\n",
      "         [-3.4185e-02, -3.3333e-02, -5.8052e-02],\n",
      "         [-3.7571e-02,  1.2165e-02, -2.8833e-02],\n",
      "         [-2.2749e-02, -6.8722e-02,  4.9052e-02],\n",
      "         [ 6.5414e-02, -3.6379e-02, -4.1182e-02],\n",
      "         [-4.5016e-02, -6.9281e-02,  6.7073e-02],\n",
      "         [ 4.5046e-02,  5.9091e-02,  2.9903e-02],\n",
      "         [-2.7467e-02, -9.7294e-03,  1.2598e-02],\n",
      "         [-5.0600e-02,  5.1403e-02,  3.5607e-02],\n",
      "         [ 1.3717e-03,  3.3034e-02, -5.9633e-02],\n",
      "         [ 6.1647e-02, -6.4568e-02,  6.6351e-02],\n",
      "         [ 3.0224e-02, -6.1787e-02,  1.2100e-03],\n",
      "         [ 4.1001e-02, -1.6305e-03,  3.4100e-02],\n",
      "         [ 6.4915e-02, -5.8217e-02, -1.3522e-02],\n",
      "         [ 5.2874e-02, -5.9149e-02,  1.0079e-02],\n",
      "         [ 4.7971e-02,  7.1068e-03,  6.5940e-02],\n",
      "         [ 5.1648e-02, -5.0428e-02, -2.1831e-02],\n",
      "         [ 2.6241e-02, -4.9081e-02, -5.0742e-02],\n",
      "         [-3.8879e-02,  4.6791e-02, -5.7165e-02],\n",
      "         [-5.2590e-02,  1.3885e-02, -5.7641e-02],\n",
      "         [-4.1388e-02,  3.5001e-02,  4.5431e-04],\n",
      "         [ 2.2171e-02,  4.5168e-02, -4.6850e-03],\n",
      "         [ 5.0904e-02, -5.5285e-02,  2.0982e-03],\n",
      "         [-4.5011e-02, -7.2987e-03,  2.5377e-02],\n",
      "         [-2.0047e-02,  3.5324e-03,  3.0009e-02],\n",
      "         [-2.0494e-02,  4.5131e-03, -6.8885e-02],\n",
      "         [ 6.6998e-02, -2.4335e-02, -4.9985e-02],\n",
      "         [-5.1625e-02,  4.9348e-02, -1.0029e-02],\n",
      "         [ 6.3823e-02,  2.7406e-02, -1.0969e-02],\n",
      "         [-4.0406e-02,  3.2178e-02,  1.5749e-02],\n",
      "         [-4.0771e-02,  5.3759e-02,  7.0038e-03],\n",
      "         [-4.0551e-02, -6.5330e-02, -3.1842e-02],\n",
      "         [ 2.8636e-02,  1.9122e-02,  1.9778e-02],\n",
      "         [-3.1386e-02,  2.9206e-02,  2.5475e-02],\n",
      "         [ 2.8210e-02, -2.9959e-02, -2.1518e-03],\n",
      "         [ 2.7551e-02,  9.5173e-03,  1.9748e-02],\n",
      "         [ 6.6696e-02,  2.3263e-02, -1.4173e-02],\n",
      "         [ 5.9882e-02,  3.3577e-02,  3.7374e-02],\n",
      "         [-1.7333e-03,  4.1021e-02,  1.4576e-03],\n",
      "         [ 6.4567e-02, -4.5271e-02, -4.2633e-02],\n",
      "         [-1.1849e-02,  5.3201e-02,  4.8253e-02],\n",
      "         [-1.2005e-03, -6.7551e-02, -2.5643e-02],\n",
      "         [-3.0720e-02, -6.3743e-02, -5.2268e-02],\n",
      "         [ 2.7580e-02, -1.2331e-02,  4.1288e-02],\n",
      "         [ 1.1179e-02,  2.3184e-02, -4.3822e-02],\n",
      "         [-3.6582e-02,  3.5221e-02,  1.4879e-02],\n",
      "         [-4.7035e-02, -2.7358e-02, -5.3384e-02],\n",
      "         [-1.5809e-02, -4.3406e-02,  3.1894e-02],\n",
      "         [-5.7053e-02, -5.3261e-02, -5.0814e-02],\n",
      "         [ 5.5272e-03, -1.0834e-02,  3.0706e-03],\n",
      "         [ 6.1533e-02, -6.5476e-02, -5.4611e-02],\n",
      "         [-6.5013e-03, -6.1290e-02,  3.2843e-02],\n",
      "         [ 5.4286e-02, -2.3071e-04, -4.8957e-02],\n",
      "         [-1.4961e-02,  5.2876e-02, -3.6159e-02]],\n",
      "\n",
      "        [[ 3.1151e-02, -2.2155e-02, -2.5228e-02],\n",
      "         [ 6.5380e-02,  3.3727e-02,  5.4572e-02],\n",
      "         [ 1.7255e-02, -6.0908e-02,  1.5258e-02],\n",
      "         [-4.5283e-02, -7.2421e-03,  5.3989e-02],\n",
      "         [-8.7991e-05, -1.3379e-02, -2.5567e-02],\n",
      "         [-4.9765e-02, -3.4658e-02, -6.3826e-02],\n",
      "         [-2.6588e-02,  5.6575e-02, -2.4074e-02],\n",
      "         [ 6.0020e-02,  6.2392e-02,  3.5730e-02],\n",
      "         [-2.5996e-02,  3.2140e-03,  1.9122e-02],\n",
      "         [ 6.8969e-02,  6.1258e-02,  4.6181e-03],\n",
      "         [ 4.7905e-02,  4.2656e-02, -6.7272e-02],\n",
      "         [-5.9690e-02, -5.0088e-02, -4.5278e-02],\n",
      "         [-3.9441e-02,  5.5742e-02,  1.9205e-02],\n",
      "         [ 6.1395e-02,  2.3056e-02, -3.7303e-02],\n",
      "         [-6.2495e-02,  5.7453e-04,  2.0994e-02],\n",
      "         [-5.9830e-02, -3.6125e-02, -2.7497e-02],\n",
      "         [-1.6010e-02,  4.9587e-02, -5.8951e-02],\n",
      "         [ 5.8406e-02,  4.0954e-02, -2.2977e-02],\n",
      "         [ 5.1121e-02, -2.8989e-02, -4.8434e-02],\n",
      "         [-4.9312e-02,  2.2135e-03,  6.9842e-02],\n",
      "         [ 2.9360e-02,  4.0308e-02, -5.8446e-02],\n",
      "         [ 6.4793e-02,  2.9366e-02, -1.4307e-02],\n",
      "         [ 3.5527e-02,  2.1571e-02,  9.3509e-03],\n",
      "         [-3.8288e-02,  8.3257e-03,  6.5112e-03],\n",
      "         [-1.5064e-02, -2.5981e-02,  3.5525e-02],\n",
      "         [-1.9259e-02,  4.6884e-02,  5.3021e-02],\n",
      "         [-3.2890e-02, -3.2999e-02, -3.5412e-02],\n",
      "         [ 4.4377e-02,  5.6492e-02, -5.7825e-02],\n",
      "         [-2.6370e-02,  3.7722e-02,  2.8293e-02],\n",
      "         [ 4.1882e-02, -4.8589e-02, -7.0364e-02],\n",
      "         [ 8.1932e-03, -3.4984e-02, -2.0216e-02],\n",
      "         [ 8.7437e-03, -4.4265e-02, -5.7200e-02],\n",
      "         [-5.7716e-02,  2.5125e-02,  5.7463e-02],\n",
      "         [-3.3878e-02,  2.0265e-02,  5.8613e-03],\n",
      "         [-6.0329e-02,  1.7148e-02, -2.4484e-02],\n",
      "         [ 9.0090e-03, -5.8816e-02, -5.1357e-02],\n",
      "         [-3.7888e-02,  5.7966e-03,  4.1454e-02],\n",
      "         [ 5.7151e-02, -6.2302e-02, -5.6579e-02],\n",
      "         [-5.3404e-02,  3.9418e-02, -4.3387e-02],\n",
      "         [-3.1819e-03,  7.1388e-02, -5.8631e-02],\n",
      "         [-2.0416e-02, -5.1324e-02,  5.2518e-02],\n",
      "         [ 5.5059e-02, -6.7592e-02,  7.1124e-02],\n",
      "         [-2.9225e-03, -5.6563e-02,  1.2017e-02],\n",
      "         [-3.5363e-03,  1.3934e-02,  7.1244e-03],\n",
      "         [ 3.5976e-02,  6.6853e-02, -2.6907e-02],\n",
      "         [-2.7332e-03,  3.0593e-02, -4.7989e-02],\n",
      "         [ 1.5681e-02,  3.9153e-02, -4.6811e-02],\n",
      "         [-5.5643e-02, -7.0311e-02,  1.2175e-04],\n",
      "         [ 2.9267e-02, -4.8004e-02, -6.9626e-02],\n",
      "         [ 1.8412e-02,  1.0826e-02,  9.2771e-03],\n",
      "         [-6.5400e-02,  6.5547e-03, -6.6761e-02],\n",
      "         [ 6.4503e-02, -3.6250e-02, -6.3822e-02],\n",
      "         [-4.5281e-02, -1.6086e-02, -4.6222e-02],\n",
      "         [-2.7073e-02, -2.3565e-02,  3.1896e-02],\n",
      "         [-1.9196e-03, -7.1420e-02, -5.2283e-02],\n",
      "         [-1.3819e-02, -5.0269e-02,  6.5736e-02],\n",
      "         [ 4.8580e-02,  2.3134e-02,  7.7778e-03],\n",
      "         [ 1.8925e-02,  3.8151e-02, -3.9073e-02],\n",
      "         [ 2.8767e-02, -4.8593e-02, -9.5170e-03],\n",
      "         [-4.8209e-02,  2.2148e-02, -4.9445e-02],\n",
      "         [ 2.3856e-02,  4.3099e-02,  3.2867e-02],\n",
      "         [-1.0628e-02, -3.9605e-02,  7.0048e-02],\n",
      "         [ 7.1869e-02, -5.0061e-02, -5.0588e-02],\n",
      "         [ 1.9970e-02, -5.1505e-02,  7.0806e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.bias | Size: torch.Size([64]) | Values : tensor([ 0.0318, -0.0684], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-1.9794e-02, -6.9629e-02,  6.5385e-02],\n",
      "         [ 2.3951e-03,  5.0491e-02,  6.4821e-02],\n",
      "         [ 6.0307e-02,  3.7394e-02,  6.1938e-02],\n",
      "         [-6.8400e-02,  3.5160e-03, -1.5452e-02],\n",
      "         [-6.2456e-02,  4.9953e-02,  2.0437e-02],\n",
      "         [-4.8329e-02,  2.3984e-02,  5.3727e-02],\n",
      "         [-5.2749e-02,  1.7235e-02,  2.0503e-02],\n",
      "         [-6.9276e-02, -6.9265e-02,  2.8654e-02],\n",
      "         [-1.5761e-02, -6.1950e-02, -3.5504e-02],\n",
      "         [-5.9980e-03, -4.2736e-02,  2.3310e-02],\n",
      "         [-2.2507e-02, -2.4920e-02,  3.4210e-02],\n",
      "         [ 3.2861e-03, -4.7794e-02, -4.8684e-02],\n",
      "         [-1.0131e-02, -5.2465e-02, -5.5438e-02],\n",
      "         [-7.0899e-02, -4.7466e-02, -2.9461e-02],\n",
      "         [-6.8494e-02, -2.6458e-02,  1.9353e-02],\n",
      "         [ 3.2619e-02, -3.6588e-02,  1.5430e-02],\n",
      "         [-2.6137e-02,  6.5893e-02,  6.8241e-02],\n",
      "         [-5.8131e-02, -6.2295e-02,  2.0963e-02],\n",
      "         [ 2.3898e-02, -1.4058e-03,  5.4608e-02],\n",
      "         [-3.9962e-02,  3.1046e-03, -6.7585e-02],\n",
      "         [ 6.5562e-02, -3.6700e-02, -3.2477e-02],\n",
      "         [ 4.2637e-02, -1.4590e-02,  1.3176e-02],\n",
      "         [-3.5668e-02,  5.7130e-02, -6.8191e-02],\n",
      "         [ 1.7562e-03,  2.6587e-02,  2.0053e-02],\n",
      "         [-4.3907e-03, -1.0157e-02,  1.0524e-02],\n",
      "         [-4.2275e-02,  3.0521e-03,  2.0570e-02],\n",
      "         [-2.9704e-03, -2.4518e-02,  4.3811e-02],\n",
      "         [ 5.0136e-02, -1.7063e-02, -3.1878e-02],\n",
      "         [ 4.6272e-02,  1.7049e-02, -3.6305e-02],\n",
      "         [ 5.3978e-02, -4.5321e-02,  7.1387e-02],\n",
      "         [-9.3442e-03,  5.8455e-02, -1.9660e-02],\n",
      "         [ 6.5712e-02,  4.3598e-02, -1.7444e-02],\n",
      "         [-2.6133e-02,  2.6014e-02,  2.1171e-02],\n",
      "         [-3.9126e-03, -6.5245e-02, -3.0454e-02],\n",
      "         [-6.7738e-02, -6.1446e-02, -8.3558e-03],\n",
      "         [-1.8117e-03,  3.6318e-02,  1.3013e-02],\n",
      "         [-4.0206e-02, -5.6221e-02,  4.6831e-02],\n",
      "         [ 3.4348e-04,  2.6441e-02,  6.3436e-02],\n",
      "         [ 6.1461e-02,  5.2909e-02,  1.7635e-02],\n",
      "         [-6.2612e-02, -6.6932e-02,  3.8294e-02],\n",
      "         [ 3.3917e-03,  1.2625e-02,  6.4741e-02],\n",
      "         [-2.9911e-02,  4.1701e-02,  3.0233e-02],\n",
      "         [ 7.1634e-02, -3.3604e-02,  3.9771e-02],\n",
      "         [-1.1895e-02, -1.7651e-02,  1.2575e-02],\n",
      "         [-2.7459e-02,  6.4146e-02,  1.2747e-02],\n",
      "         [-5.3431e-02,  4.1364e-02, -5.3320e-02],\n",
      "         [ 5.9395e-02, -6.3435e-02,  3.5667e-02],\n",
      "         [-3.3979e-02, -9.9348e-03, -5.9941e-02],\n",
      "         [ 5.8860e-02, -2.6972e-02,  6.0292e-02],\n",
      "         [-3.4957e-02,  1.6536e-02,  5.8454e-02],\n",
      "         [-2.4992e-02, -4.3125e-02,  1.1813e-02],\n",
      "         [ 4.9819e-02,  7.1244e-02, -3.9584e-02],\n",
      "         [-1.2686e-02,  1.6623e-02,  3.7692e-03],\n",
      "         [ 3.8325e-02, -5.5280e-02, -4.8332e-02],\n",
      "         [ 6.3009e-02,  1.3294e-02,  3.0278e-02],\n",
      "         [-2.8974e-02,  6.2804e-02,  3.8226e-02],\n",
      "         [ 6.6888e-02, -3.6434e-02, -4.1912e-02],\n",
      "         [-3.4100e-03, -1.8481e-02, -1.8533e-02],\n",
      "         [ 3.6972e-02, -5.6004e-02, -1.2478e-02],\n",
      "         [ 6.4692e-02, -2.0674e-02,  6.2071e-04],\n",
      "         [ 2.8383e-02,  6.7745e-02, -3.0673e-02],\n",
      "         [-6.1051e-02, -2.3278e-02,  4.6997e-02],\n",
      "         [ 1.7731e-02, -2.2751e-02, -2.6050e-02],\n",
      "         [-1.8024e-02,  2.9385e-02,  4.2126e-02]],\n",
      "\n",
      "        [[-2.5528e-02,  1.5694e-02, -4.8680e-02],\n",
      "         [ 1.6362e-02, -4.0074e-02,  4.8303e-04],\n",
      "         [-5.8717e-02, -3.9091e-03,  4.4636e-02],\n",
      "         [ 5.9691e-02,  1.2052e-02, -2.2764e-02],\n",
      "         [-1.7270e-03,  4.6504e-02,  4.2432e-02],\n",
      "         [ 2.0056e-02, -3.4925e-02,  4.1055e-03],\n",
      "         [-6.8768e-02,  3.1170e-02, -2.2827e-02],\n",
      "         [-2.3928e-02,  3.9108e-02,  4.1133e-02],\n",
      "         [-3.7137e-02,  6.4057e-02, -3.6757e-02],\n",
      "         [-6.8498e-03, -3.8567e-02, -6.1618e-02],\n",
      "         [ 3.6813e-02, -3.2659e-02,  7.0453e-02],\n",
      "         [-5.8586e-02,  2.6839e-02,  2.8119e-02],\n",
      "         [ 1.4535e-02, -1.9469e-02,  2.9831e-02],\n",
      "         [ 3.7559e-02, -6.0172e-03,  2.3339e-02],\n",
      "         [ 5.9104e-02, -1.3680e-02,  9.0472e-03],\n",
      "         [-7.1984e-02, -2.5989e-02, -2.2855e-02],\n",
      "         [-2.9673e-02, -3.5610e-02, -2.4982e-05],\n",
      "         [-3.7304e-02,  5.8601e-02, -5.5352e-02],\n",
      "         [ 3.0999e-02, -4.0921e-02, -6.9704e-02],\n",
      "         [-1.5170e-02, -6.1872e-03, -6.0513e-02],\n",
      "         [ 7.1666e-02, -2.6226e-02,  4.1925e-02],\n",
      "         [-5.7513e-02, -5.3661e-02, -1.3118e-02],\n",
      "         [ 2.1739e-03, -6.3233e-03,  2.7263e-02],\n",
      "         [-4.9651e-02,  1.2440e-02,  3.4290e-02],\n",
      "         [ 2.3124e-02, -1.5610e-02,  6.3606e-02],\n",
      "         [ 4.0282e-02, -3.1370e-02, -1.0117e-02],\n",
      "         [ 3.7971e-02, -7.3085e-04, -3.1211e-02],\n",
      "         [-4.7551e-02, -3.6626e-02, -7.1858e-02],\n",
      "         [-4.6591e-02,  5.0920e-02,  2.7419e-02],\n",
      "         [ 4.2455e-02, -5.7747e-02,  6.1194e-03],\n",
      "         [ 3.1984e-02,  2.7115e-02, -3.6295e-02],\n",
      "         [ 5.1648e-02,  3.2979e-02, -3.0019e-02],\n",
      "         [ 1.4314e-02, -3.4667e-02,  2.6235e-02],\n",
      "         [ 3.6296e-02, -7.5144e-03, -5.0346e-02],\n",
      "         [ 1.9951e-02,  4.6574e-02,  2.7517e-02],\n",
      "         [-2.4546e-02, -3.7539e-02, -2.5854e-02],\n",
      "         [-2.8864e-02, -3.1155e-02,  2.8997e-02],\n",
      "         [ 9.6376e-04,  6.2939e-02,  9.2237e-04],\n",
      "         [-3.7458e-02,  4.3577e-02,  3.9666e-02],\n",
      "         [-5.5262e-02, -3.3804e-02,  3.2985e-02],\n",
      "         [-1.2735e-03,  3.5981e-02, -4.9638e-02],\n",
      "         [ 4.0063e-02,  4.8254e-02,  4.3623e-02],\n",
      "         [ 4.7435e-02,  4.6687e-02,  5.2892e-02],\n",
      "         [-4.3432e-02, -4.6916e-02,  3.6622e-02],\n",
      "         [-5.3960e-02, -6.1574e-02, -5.5034e-02],\n",
      "         [ 8.0106e-03, -2.8942e-02,  6.3403e-02],\n",
      "         [-2.7856e-02, -7.2072e-02, -2.2491e-02],\n",
      "         [-3.5548e-02,  5.1394e-02, -7.0822e-02],\n",
      "         [-4.3384e-02, -4.6291e-02,  5.7961e-02],\n",
      "         [ 5.1313e-02,  4.8661e-02, -2.0445e-02],\n",
      "         [ 1.1611e-02, -6.3371e-02,  5.9816e-02],\n",
      "         [-2.6576e-02, -6.5125e-02,  3.4675e-02],\n",
      "         [ 6.4756e-02, -1.5412e-02, -3.5682e-02],\n",
      "         [-6.9290e-02, -3.9489e-02,  4.7926e-02],\n",
      "         [ 6.7368e-02, -5.5211e-02,  3.3621e-03],\n",
      "         [ 1.4791e-02, -2.2233e-02,  1.0917e-02],\n",
      "         [-1.9983e-03, -1.0059e-02,  1.9977e-02],\n",
      "         [ 1.9949e-02,  1.1634e-02,  3.1288e-02],\n",
      "         [ 7.0868e-02,  4.9549e-02,  1.9566e-03],\n",
      "         [ 5.5140e-03,  4.5274e-02,  6.3349e-02],\n",
      "         [-8.7529e-04,  6.1626e-02,  1.4774e-02],\n",
      "         [ 6.0910e-02,  1.6073e-02,  5.7763e-03],\n",
      "         [ 3.4212e-03,  2.7928e-02,  1.6143e-02],\n",
      "         [ 6.7860e-02, -4.0219e-02, -6.9068e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0616, -0.0703], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0328,  0.0597,  0.1092,  0.1070,  0.0231, -0.0290, -0.0519,  0.0676,\n",
      "          0.0244,  0.1155, -0.0252,  0.0360,  0.0904,  0.0857,  0.0861,  0.0266,\n",
      "         -0.0760, -0.1080,  0.0177,  0.0869,  0.0567,  0.1196,  0.0234,  0.1087,\n",
      "          0.0390,  0.0831,  0.0325, -0.0689, -0.0405, -0.0167, -0.1179,  0.1106,\n",
      "          0.1182,  0.0019,  0.0965, -0.0778,  0.0945, -0.0020,  0.1228,  0.0017,\n",
      "         -0.0810,  0.0084, -0.0438,  0.0364, -0.0264, -0.1232,  0.0812, -0.0419,\n",
      "          0.0591, -0.1205, -0.0781,  0.0735,  0.1224,  0.0776, -0.0326,  0.0436,\n",
      "         -0.0072, -0.1127, -0.0072, -0.0624, -0.0091, -0.0915, -0.0737,  0.0233],\n",
      "        [-0.1186,  0.1020,  0.0603,  0.0105,  0.0566, -0.0007,  0.0673,  0.0587,\n",
      "         -0.1071, -0.0628, -0.1131,  0.0565, -0.0941, -0.0531, -0.0198, -0.0131,\n",
      "          0.0971, -0.0752, -0.0110, -0.0721,  0.0742,  0.0861,  0.0810,  0.0612,\n",
      "          0.0736,  0.0245,  0.0695, -0.0457, -0.0544,  0.0580,  0.0921,  0.0856,\n",
      "         -0.0696, -0.0961,  0.1173,  0.0019, -0.0730,  0.0517,  0.1207, -0.1177,\n",
      "         -0.0664,  0.0161,  0.0782,  0.0619,  0.1147, -0.0403, -0.0076, -0.0822,\n",
      "         -0.0011, -0.0733, -0.1039,  0.0259, -0.0881,  0.0720,  0.0884, -0.0485,\n",
      "          0.0493,  0.0085,  0.1250, -0.0253,  0.0977,  0.0891, -0.1033,  0.0145]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[ 0.0686, -0.1017, -0.1049,  0.0209, -0.0478, -0.1054, -0.0298, -0.1001,\n",
      "          0.0797, -0.1168,  0.0241, -0.1152, -0.0351,  0.0197, -0.0957, -0.0984,\n",
      "         -0.0006,  0.0844, -0.0439, -0.0869, -0.0075,  0.0270,  0.0928,  0.0821,\n",
      "         -0.0636,  0.0411, -0.0088,  0.0848,  0.1203,  0.1106,  0.1243, -0.0243,\n",
      "         -0.0628,  0.0224,  0.0289,  0.1231,  0.1210,  0.0338,  0.0837,  0.0876,\n",
      "         -0.0028, -0.0446, -0.0765,  0.0133,  0.0176, -0.0326,  0.0330,  0.0898,\n",
      "         -0.1067,  0.0104,  0.0529,  0.0060,  0.1075, -0.1201,  0.0149, -0.0978,\n",
      "          0.0140,  0.0072,  0.0541,  0.1176,  0.1069,  0.0954,  0.0417, -0.1151],\n",
      "        [ 0.0173,  0.0892,  0.0382, -0.1018, -0.0930,  0.1033,  0.1213, -0.0136,\n",
      "          0.0123, -0.1004,  0.0619,  0.0034,  0.0918, -0.1220,  0.0269,  0.1194,\n",
      "          0.0336, -0.0457,  0.0931,  0.0883,  0.0171, -0.0167,  0.0345, -0.0049,\n",
      "          0.0983,  0.0569, -0.0184, -0.0582, -0.0909, -0.0497,  0.0232, -0.1042,\n",
      "         -0.0513,  0.0573, -0.0201, -0.1157, -0.1032,  0.1055, -0.0792,  0.0629,\n",
      "          0.0659, -0.0325,  0.0599,  0.0845, -0.0513, -0.1246,  0.0412,  0.0404,\n",
      "          0.0966, -0.0466, -0.0333,  0.0828, -0.0127,  0.0735, -0.1187,  0.0793,\n",
      "          0.0959, -0.1024, -0.0515,  0.0687,  0.0348, -0.0684, -0.0530,  0.0313]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([-0.0094,  0.1132], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([-0.0119,  0.0173], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[ 0.0902,  0.1080,  0.0047, -0.1209,  0.0663,  0.0179,  0.0900,  0.0046,\n",
      "          0.0879,  0.0555,  0.0191, -0.0071,  0.1076,  0.0770, -0.0699, -0.0756,\n",
      "          0.1066,  0.1205, -0.0173, -0.1040,  0.1074,  0.1233,  0.0494,  0.1067,\n",
      "          0.1170,  0.0294,  0.0874,  0.0381, -0.1111,  0.0129,  0.0159,  0.0519,\n",
      "         -0.0763, -0.1050, -0.0888, -0.0783,  0.0330,  0.1054,  0.0551,  0.1081,\n",
      "         -0.0956,  0.0982,  0.1107, -0.0339, -0.0325, -0.0841,  0.0238,  0.1047,\n",
      "         -0.0946,  0.0280,  0.1120,  0.0919, -0.0312, -0.0264, -0.0125, -0.0017,\n",
      "         -0.0046,  0.0722,  0.0548,  0.1041,  0.0103,  0.0798, -0.0808,  0.0489],\n",
      "        [ 0.0008, -0.0841,  0.0547,  0.0282, -0.0357,  0.0157,  0.0928, -0.0729,\n",
      "         -0.0323,  0.0711,  0.0318, -0.0355, -0.1016, -0.0569, -0.1219,  0.0134,\n",
      "         -0.0770, -0.0543,  0.0323, -0.0627, -0.0107, -0.1206,  0.0773, -0.0681,\n",
      "         -0.0328,  0.1085,  0.0698,  0.0552,  0.0988,  0.0001, -0.0378,  0.0954,\n",
      "          0.0094, -0.0114,  0.0920, -0.0481,  0.1142,  0.0406,  0.0451, -0.0358,\n",
      "          0.0733, -0.0653, -0.0565, -0.0293,  0.0735,  0.1170,  0.0837,  0.0709,\n",
      "          0.1169, -0.0486,  0.0685, -0.0576,  0.1048,  0.0852,  0.1040,  0.1009,\n",
      "          0.0196,  0.0131,  0.1004, -0.0896,  0.1153,  0.0904, -0.0372,  0.0382]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0700, -0.0230, -0.0113,  0.1171,  0.0122, -0.1010,  0.0118,  0.0365,\n",
      "          0.0762,  0.1232,  0.0275, -0.0400,  0.0797,  0.0026,  0.1135, -0.0110,\n",
      "          0.0224, -0.0218, -0.0304,  0.1188,  0.0708, -0.0277,  0.0520, -0.1112,\n",
      "         -0.1088, -0.0213,  0.0740,  0.0165, -0.0300, -0.0010, -0.0464,  0.0693,\n",
      "          0.0734,  0.0620,  0.0340, -0.0963, -0.0211,  0.0271, -0.0059,  0.0649,\n",
      "         -0.0754, -0.1235, -0.0569,  0.0887,  0.0465, -0.0237,  0.0798, -0.0420,\n",
      "          0.0364, -0.0697, -0.0165,  0.0493, -0.0236, -0.0600,  0.0328, -0.1039,\n",
      "         -0.0259,  0.0536,  0.1218,  0.0103,  0.0495,  0.1074,  0.0949,  0.1245],\n",
      "        [-0.0207,  0.0173, -0.1136,  0.0880, -0.1184, -0.0089, -0.0209,  0.0511,\n",
      "         -0.0458, -0.0511,  0.0864, -0.0750,  0.0244,  0.0142, -0.0293,  0.0770,\n",
      "         -0.0821, -0.0968,  0.0905,  0.0413, -0.1109, -0.0688,  0.0304,  0.0040,\n",
      "         -0.0818, -0.0546,  0.1196, -0.1129,  0.1045,  0.0845,  0.0205,  0.0788,\n",
      "          0.1152, -0.0723,  0.0509,  0.1121,  0.0603, -0.0478,  0.1068,  0.0554,\n",
      "         -0.0506,  0.0565,  0.1012, -0.1113,  0.0741,  0.0844, -0.0042, -0.0067,\n",
      "         -0.0535, -0.0113,  0.1022, -0.0602, -0.0461, -0.1084, -0.1117, -0.0985,\n",
      "          0.0508,  0.1111, -0.1130, -0.0775, -0.0520,  0.0341,  0.0894,  0.0601]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([-0.0918,  0.0326], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([-0.0560, -0.0141], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.weight | Size: torch.Size([2, 64]) | Values : tensor([[ 0.0033, -0.1126,  0.0470, -0.0659,  0.0758, -0.0698, -0.1099,  0.0168,\n",
      "          0.0508,  0.0922,  0.0967, -0.0376,  0.0937, -0.0936,  0.0036,  0.0628,\n",
      "          0.0741,  0.1182, -0.0702,  0.0347,  0.0698, -0.0751,  0.0474, -0.0105,\n",
      "          0.0065,  0.1097, -0.0930,  0.0026,  0.1107,  0.0831,  0.0759, -0.0329,\n",
      "         -0.1056,  0.0275, -0.0388, -0.1052, -0.0703, -0.0190, -0.1078, -0.0493,\n",
      "          0.0424, -0.0724, -0.0345,  0.0110,  0.0459,  0.0538, -0.1082, -0.0475,\n",
      "          0.0511, -0.1038, -0.0598,  0.0845, -0.0745,  0.0425,  0.0205, -0.1223,\n",
      "          0.0391,  0.1112,  0.0037, -0.0036,  0.1098,  0.1016,  0.0863, -0.0570],\n",
      "        [ 0.0767,  0.0625, -0.0872,  0.1231,  0.0552, -0.0955,  0.0811,  0.0238,\n",
      "          0.0352, -0.0479, -0.0426, -0.0135,  0.0951, -0.0087, -0.0369,  0.0567,\n",
      "         -0.1046,  0.1129, -0.0935,  0.1213, -0.0518, -0.0563, -0.0656, -0.0343,\n",
      "          0.0667,  0.0772, -0.0917, -0.1116, -0.0831,  0.1077,  0.0125,  0.0039,\n",
      "          0.0152, -0.0673,  0.0554, -0.1202, -0.0285,  0.0739, -0.0478, -0.0106,\n",
      "         -0.1015,  0.0029,  0.0712,  0.1248, -0.0698,  0.0124, -0.0460,  0.0296,\n",
      "          0.0846,  0.0191,  0.0089, -0.1004,  0.0348,  0.1058,  0.1090, -0.0757,\n",
      "         -0.0726, -0.0185,  0.0718,  0.0004, -0.0054,  0.1219,  0.0817, -0.0241]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.bias | Size: torch.Size([2]) | Values : tensor([ 0.1000, -0.0654], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from models.ConvLSTM_KFall import ConvLSTM\n",
    "# Create an instance of the model\n",
    "model = ConvLSTM()\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    #print(dataloader.dataset)\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print('X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        # transpose X\n",
    "        #X = X.transpose(1, 2)\n",
    "        #print('transposed X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        B_size = (y_train == 0).sum()\n",
    "        A_size = (y_train == 1).sum()\n",
    "        #print('B_size: ', B_size)\n",
    "        #print('A_size: ', A_size)\n",
    "        B_multiplier = 1\n",
    "        A_multiplier = B_size / A_size\n",
    "        multipliers = torch.where(y == 0, B_multiplier, A_multiplier)\n",
    "        pred = model(X.float())\n",
    "        #loss = loss_fn(pred, y)\n",
    "        loss = (loss_fn(pred, y) * multipliers).mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            print('pred: ', pred.argmax(1), 'y: ', y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.231337  [   64/11761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.128106  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.086562 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.248804  [   64/11761]\n",
      "loss: 0.098480  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.077748 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.165165  [   64/11761]\n",
      "loss: 0.076500  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.076534 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.028733  [   64/11761]\n",
      "loss: 0.148246  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.069203 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.199277  [   64/11761]\n",
      "loss: 0.041786  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.071689 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.059306  [   64/11761]\n",
      "loss: 0.562419  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.067631 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.103712  [   64/11761]\n",
      "loss: 0.033529  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.056350 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.015416  [   64/11761]\n",
      "loss: 0.022656  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.059032 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.062376  [   64/11761]\n",
      "loss: 0.014730  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.050287 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.032438  [   64/11761]\n",
      "loss: 0.071016  [ 6464/11761]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.049723 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]) y:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.275132 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final test\n",
    "\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

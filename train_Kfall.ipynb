{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "#sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.1\n",
    "num_window_fall_data = 10\n",
    "num_window_not_fall_data = 3\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  50\n",
      "data.shape:  (15161, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# faltten the data\n",
    "\n",
    "#data = data.reshape(data.shape[0], -1)\n",
    "reshaped_data = data\n",
    "in_channels = reshaped_data.shape[1]\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', reshaped_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor.dtype:  torch.float64\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# create a validate set\n",
    "\n",
    "# create test/validation/test data\n",
    "\"\"\" X_train, X_test, y_train, y_test = train_test_split(reshaped_data, \n",
    "                                                    label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) \"\"\"\n",
    "label = label.astype(np.int64)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reshaped_data, label, test_size=0.05, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "y_val_tensor = torch.from_numpy(y_val)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "# print datatype of X_train_tensor\n",
    "X_train_tensor = X_train_tensor.double()\n",
    "print('X_train_tensor.dtype: ', X_train_tensor.dtype)\n",
    "X_test = X_train_tensor.double()\n",
    "\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(64, 2)  # No need for softmax here using nn.CrossEntropyLoss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = x.transpose(1, 2)  # Transpose to have the correct dimensions for Conv1d (batch, channels, length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        x = x.transpose(1, 2)  # Transpose back to (batch, seq_len, features)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Take the outputs of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ConvLSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm1): LSTM(64, 64, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (lstm2): LSTM(64, 64, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.0.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[-0.1734,  0.1576, -0.0459],\n",
      "         [-0.0966, -0.0710,  0.0710],\n",
      "         [-0.1820, -0.0922, -0.1848],\n",
      "         [ 0.0662, -0.0064, -0.0849],\n",
      "         [-0.1457, -0.0842,  0.1421],\n",
      "         [-0.0272,  0.1859,  0.0261],\n",
      "         [ 0.0674, -0.1607, -0.0895],\n",
      "         [ 0.0236, -0.0819,  0.0130],\n",
      "         [-0.1229, -0.1265, -0.0451]],\n",
      "\n",
      "        [[-0.0439,  0.1908,  0.0090],\n",
      "         [ 0.0153,  0.0641,  0.1517],\n",
      "         [ 0.0426,  0.1155, -0.1891],\n",
      "         [ 0.1573, -0.0884, -0.0692],\n",
      "         [ 0.1288, -0.0783, -0.1151],\n",
      "         [-0.1669, -0.0796,  0.1826],\n",
      "         [-0.0740,  0.0259,  0.0923],\n",
      "         [-0.0585,  0.1451, -0.0190],\n",
      "         [-0.0728,  0.1012,  0.1267]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0670,  0.0575], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-7.1020e-02,  5.3003e-02,  4.3012e-02],\n",
      "         [-5.6145e-02,  1.2983e-02, -1.9536e-02],\n",
      "         [-5.8863e-03, -3.4429e-02, -4.5882e-02],\n",
      "         [ 5.9380e-02,  6.5963e-03,  7.3686e-03],\n",
      "         [ 3.5932e-02,  6.1781e-02,  1.4089e-02],\n",
      "         [ 4.9032e-02,  1.2438e-02, -5.3471e-02],\n",
      "         [ 1.5878e-02, -6.0659e-02,  5.4752e-02],\n",
      "         [-8.0227e-03, -6.8086e-03, -5.6978e-02],\n",
      "         [-5.0501e-02,  1.0427e-03,  7.0848e-02],\n",
      "         [-3.4848e-02,  6.9760e-02,  6.9212e-02],\n",
      "         [ 5.2465e-03, -3.5321e-02,  8.6089e-03],\n",
      "         [-5.2639e-02, -6.8488e-02,  1.6618e-02],\n",
      "         [ 3.1745e-02,  6.4846e-02, -3.8136e-02],\n",
      "         [ 6.3981e-02, -3.7319e-02, -1.5791e-02],\n",
      "         [ 2.3801e-02, -4.8254e-03, -6.2814e-02],\n",
      "         [ 1.9286e-02,  6.2569e-02, -5.5721e-02],\n",
      "         [ 1.5555e-02,  2.5666e-02,  3.5555e-02],\n",
      "         [-6.3219e-02, -4.6800e-02,  5.5525e-02],\n",
      "         [ 6.9851e-02,  7.1524e-02, -2.4386e-02],\n",
      "         [ 5.1386e-02,  6.6205e-02, -2.9611e-03],\n",
      "         [-4.7441e-02, -6.8190e-02,  1.9502e-02],\n",
      "         [ 6.9899e-02,  6.6916e-02, -1.5535e-02],\n",
      "         [-1.8317e-02, -9.4583e-03,  1.4863e-03],\n",
      "         [ 4.9788e-02,  1.3257e-02,  3.1307e-02],\n",
      "         [-2.6515e-02,  3.4829e-02, -5.3836e-02],\n",
      "         [ 6.7476e-02, -6.2121e-02,  1.1665e-02],\n",
      "         [-3.7897e-02,  4.6732e-02, -2.2545e-03],\n",
      "         [-9.6025e-03, -3.7306e-02,  2.6795e-02],\n",
      "         [ 5.5121e-02, -5.3972e-02,  6.2495e-04],\n",
      "         [ 5.1546e-02, -6.0419e-02,  1.9182e-02],\n",
      "         [-3.7510e-02, -3.6593e-02,  5.4495e-03],\n",
      "         [-2.5835e-02,  2.5460e-02, -7.0844e-02],\n",
      "         [-2.6369e-02,  2.1927e-02, -3.7591e-02],\n",
      "         [ 5.6967e-02,  1.4044e-02,  4.8400e-02],\n",
      "         [ 1.8172e-02, -4.4140e-02,  4.8820e-02],\n",
      "         [ 1.4704e-02,  7.0135e-02, -6.3309e-02],\n",
      "         [-2.5330e-03,  3.3961e-02, -2.4487e-02],\n",
      "         [-3.3255e-02,  4.3675e-02, -2.9249e-02],\n",
      "         [ 4.7493e-02,  1.1072e-02, -4.1669e-02],\n",
      "         [ 3.3671e-02,  4.1118e-02, -4.6346e-02],\n",
      "         [-2.5248e-02, -5.2492e-02, -2.4629e-02],\n",
      "         [-6.3065e-03, -1.5344e-02,  6.1353e-02],\n",
      "         [ 5.0250e-02, -2.0240e-02,  6.4019e-02],\n",
      "         [ 1.8394e-02, -6.6752e-03, -5.9783e-03],\n",
      "         [-5.5057e-02, -7.1904e-02,  5.8769e-02],\n",
      "         [-1.2803e-02,  2.6447e-02, -3.7798e-04],\n",
      "         [ 1.0376e-02, -5.2021e-02,  2.1286e-02],\n",
      "         [-4.9267e-02,  5.8963e-02,  4.7843e-02],\n",
      "         [-4.6484e-03, -5.3722e-02, -3.9060e-02],\n",
      "         [ 1.6566e-02, -8.5380e-03,  3.0430e-02],\n",
      "         [ 1.1666e-02,  3.3225e-02,  2.9491e-02],\n",
      "         [ 6.9227e-02, -3.2395e-02, -2.9504e-02],\n",
      "         [ 5.3714e-02,  2.1190e-02, -1.6995e-02],\n",
      "         [-2.9796e-02,  2.1114e-02,  7.1013e-02],\n",
      "         [ 9.4888e-03,  4.0096e-02,  1.1210e-03],\n",
      "         [-1.1946e-02,  6.9685e-02, -5.5280e-02],\n",
      "         [-1.2971e-02, -3.9326e-02,  6.1831e-02],\n",
      "         [ 4.5604e-02, -6.9262e-02,  4.5284e-02],\n",
      "         [-3.3841e-02, -2.1965e-02,  6.7369e-02],\n",
      "         [ 4.1351e-04,  3.0946e-02,  2.7150e-03],\n",
      "         [-1.1775e-02,  1.6074e-02,  4.2369e-02],\n",
      "         [-1.9910e-02,  5.9813e-02,  1.2337e-02],\n",
      "         [ 4.8401e-02, -9.9753e-04,  6.7007e-02],\n",
      "         [-1.6871e-02,  4.0123e-02,  3.4914e-02]],\n",
      "\n",
      "        [[ 3.7957e-03, -4.7884e-02, -1.7325e-02],\n",
      "         [-4.3448e-02, -4.8111e-02,  5.7367e-02],\n",
      "         [-2.1377e-03,  1.0667e-02,  2.3401e-02],\n",
      "         [-1.3845e-04,  4.5051e-02,  4.7436e-02],\n",
      "         [-6.3516e-02,  4.7403e-02, -6.1210e-02],\n",
      "         [-5.8190e-02, -4.4330e-02, -3.8279e-02],\n",
      "         [ 5.7392e-02,  6.3787e-02, -4.2508e-02],\n",
      "         [-3.7135e-02, -3.9834e-02, -3.8799e-02],\n",
      "         [ 3.9041e-02,  6.4282e-02,  5.4150e-02],\n",
      "         [-4.4737e-02, -6.4251e-02,  6.7846e-02],\n",
      "         [ 4.5620e-02,  1.0249e-02, -5.9950e-02],\n",
      "         [ 4.3196e-02,  3.0365e-02,  7.1953e-02],\n",
      "         [-1.6107e-02, -3.3139e-02, -6.3081e-02],\n",
      "         [-4.9459e-02,  4.7523e-02, -1.6852e-02],\n",
      "         [ 3.3186e-02,  3.2394e-02, -3.4867e-02],\n",
      "         [ 6.4559e-02, -3.9929e-02, -6.0664e-02],\n",
      "         [ 7.9279e-03, -4.9448e-03,  2.2626e-03],\n",
      "         [-1.6021e-03, -4.2342e-02, -3.0022e-02],\n",
      "         [-4.4846e-02, -1.7002e-02,  8.0189e-03],\n",
      "         [ 5.7754e-02, -6.2240e-02,  1.5749e-02],\n",
      "         [ 4.1255e-02, -4.5649e-02, -1.4311e-02],\n",
      "         [ 5.9821e-02, -4.6958e-02,  3.3029e-02],\n",
      "         [-1.8836e-02,  4.1019e-02, -3.2547e-02],\n",
      "         [-1.2271e-02, -5.6060e-02,  4.0056e-02],\n",
      "         [ 5.6518e-02, -1.9480e-02, -2.9480e-02],\n",
      "         [-4.6814e-02, -5.7811e-02,  5.7601e-02],\n",
      "         [-5.6764e-02, -6.0142e-02, -5.2877e-02],\n",
      "         [ 3.0326e-02,  6.1494e-03,  7.2009e-02],\n",
      "         [-2.5634e-02, -5.0145e-02, -5.3135e-02],\n",
      "         [-9.0790e-03, -1.9769e-02,  3.0177e-02],\n",
      "         [ 5.6829e-02, -2.9723e-02,  4.7035e-02],\n",
      "         [ 5.8607e-02,  3.9276e-02,  6.2274e-02],\n",
      "         [-6.8241e-02,  6.3079e-02,  5.6125e-04],\n",
      "         [-2.2846e-02, -1.2176e-03, -2.6547e-02],\n",
      "         [ 5.0921e-02,  4.4197e-02, -2.4086e-02],\n",
      "         [ 6.3260e-02, -1.5580e-02, -1.4447e-04],\n",
      "         [-4.3614e-02, -5.3236e-02,  1.9754e-02],\n",
      "         [ 5.6063e-02,  1.1165e-03, -1.1215e-02],\n",
      "         [-4.6405e-02,  5.0199e-02,  3.2887e-03],\n",
      "         [ 4.6005e-02,  2.0395e-02,  2.1325e-02],\n",
      "         [ 3.0020e-02, -5.2829e-02,  4.0218e-03],\n",
      "         [ 2.5102e-02,  8.1876e-03,  5.0368e-03],\n",
      "         [ 5.4562e-02, -1.1073e-02, -2.6362e-02],\n",
      "         [-4.7448e-02,  1.1802e-02, -3.3921e-02],\n",
      "         [ 2.7565e-02,  2.1465e-02,  3.2010e-02],\n",
      "         [ 4.8238e-03, -2.3147e-02,  5.2882e-03],\n",
      "         [-6.9546e-03,  1.5428e-02,  1.6822e-02],\n",
      "         [-2.9564e-02, -5.9789e-02, -4.1812e-02],\n",
      "         [-3.5089e-02, -5.6358e-02,  4.7836e-02],\n",
      "         [-3.0448e-02, -6.8925e-02, -3.5931e-02],\n",
      "         [ 1.8030e-02, -3.4077e-02,  7.0326e-03],\n",
      "         [-6.3480e-02,  6.2940e-02, -4.6629e-02],\n",
      "         [-1.2636e-02,  4.8922e-03,  7.1430e-02],\n",
      "         [ 1.1328e-02, -4.8322e-02,  4.3449e-02],\n",
      "         [ 2.4585e-02,  4.5707e-02, -6.7395e-02],\n",
      "         [ 5.5977e-02, -1.4171e-02, -1.3884e-03],\n",
      "         [ 6.5209e-02,  6.7291e-02, -6.2142e-02],\n",
      "         [ 2.0763e-03, -4.4737e-02, -5.1365e-02],\n",
      "         [ 3.8377e-02, -2.7373e-02, -3.9368e-02],\n",
      "         [-5.5348e-02,  4.3601e-05, -1.4976e-02],\n",
      "         [ 1.3679e-04, -1.9925e-02,  4.8611e-02],\n",
      "         [-5.9944e-02,  5.0031e-02, -4.3499e-02],\n",
      "         [-6.5597e-02, -3.1456e-02, -7.0096e-02],\n",
      "         [-1.6029e-03, -6.9880e-02,  3.0873e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0334, -0.0397], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[ 1.0885e-02, -6.3502e-02, -4.9558e-02],\n",
      "         [-1.4598e-02, -2.5648e-04, -5.5650e-02],\n",
      "         [-5.1731e-02, -2.6692e-02, -5.2271e-02],\n",
      "         [ 4.9222e-02,  1.1096e-02,  6.1913e-02],\n",
      "         [-4.2493e-02, -2.9445e-02,  3.5125e-02],\n",
      "         [-4.4258e-02,  3.5338e-02,  3.3556e-02],\n",
      "         [-6.9227e-02,  3.3191e-02, -4.9937e-02],\n",
      "         [-2.7897e-02,  2.7487e-02,  5.5333e-02],\n",
      "         [-6.3209e-02, -5.6949e-02, -1.3819e-03],\n",
      "         [ 1.4091e-02,  1.4033e-02, -1.8240e-02],\n",
      "         [-4.1942e-02,  3.9689e-02,  6.3121e-02],\n",
      "         [ 6.9916e-02, -6.9952e-02, -1.9934e-02],\n",
      "         [-1.3296e-02,  2.6282e-02, -4.9562e-02],\n",
      "         [-4.2148e-02, -6.7723e-02,  4.8142e-02],\n",
      "         [ 2.3123e-02,  2.6935e-02, -2.4975e-02],\n",
      "         [-5.8968e-02,  6.4710e-02, -7.0236e-03],\n",
      "         [ 1.2017e-02,  3.8378e-02, -6.6558e-02],\n",
      "         [ 8.9052e-04, -2.4227e-02, -5.6952e-02],\n",
      "         [-1.3213e-02,  4.4904e-02, -6.8809e-03],\n",
      "         [ 4.1492e-02, -5.4415e-02,  7.5742e-03],\n",
      "         [ 5.1746e-02,  5.9961e-02,  2.1604e-02],\n",
      "         [-3.1320e-02,  6.9969e-02, -5.5777e-03],\n",
      "         [ 7.0889e-02,  1.9642e-02,  5.5163e-02],\n",
      "         [ 4.1741e-02, -1.0094e-02, -5.7912e-02],\n",
      "         [ 1.4223e-02, -1.4545e-02, -4.0416e-03],\n",
      "         [ 6.2818e-02,  1.3766e-02, -2.6714e-02],\n",
      "         [ 5.2835e-02, -3.8379e-02,  4.2158e-02],\n",
      "         [-5.7789e-02, -4.8057e-02,  1.3034e-02],\n",
      "         [-2.7114e-02, -2.8583e-03,  2.1767e-02],\n",
      "         [-4.9248e-02,  1.5623e-02, -2.0033e-02],\n",
      "         [-2.4848e-02,  3.0490e-02,  5.8400e-02],\n",
      "         [ 1.5342e-02, -5.3743e-02,  3.7143e-02],\n",
      "         [ 1.8609e-02,  1.0942e-02,  2.5449e-03],\n",
      "         [ 3.5555e-02,  8.2236e-03,  1.2379e-02],\n",
      "         [ 5.9696e-02,  1.0647e-02,  1.8737e-02],\n",
      "         [ 5.5882e-02,  1.7816e-02, -5.8106e-02],\n",
      "         [ 4.1036e-02,  5.4107e-02, -3.7360e-02],\n",
      "         [-6.6614e-02,  4.3133e-02, -6.4128e-02],\n",
      "         [ 2.1260e-02, -4.3803e-02, -6.8332e-02],\n",
      "         [ 6.0225e-02, -5.4980e-02, -1.5478e-02],\n",
      "         [ 3.8249e-02,  4.8979e-02, -5.5446e-02],\n",
      "         [-3.4665e-02,  4.2841e-02,  8.4506e-04],\n",
      "         [-2.2527e-02,  7.0562e-02, -6.9323e-02],\n",
      "         [ 6.1576e-02, -9.8752e-03,  6.5830e-03],\n",
      "         [-2.2407e-02, -4.9381e-02,  2.6019e-02],\n",
      "         [-3.6511e-02, -1.2376e-02, -4.5577e-02],\n",
      "         [ 3.7468e-02, -3.6058e-02,  6.2134e-02],\n",
      "         [ 4.8490e-03, -6.5202e-02,  6.2977e-02],\n",
      "         [ 1.7064e-02,  5.2125e-02, -4.8202e-03],\n",
      "         [ 5.7212e-03,  1.3058e-02, -6.4120e-02],\n",
      "         [ 2.1560e-02,  9.8712e-03,  3.8337e-02],\n",
      "         [ 5.1395e-02,  4.8424e-02,  2.6469e-02],\n",
      "         [ 7.7293e-04,  6.4972e-02,  4.7480e-02],\n",
      "         [-1.5324e-03,  3.6526e-02,  1.3030e-02],\n",
      "         [-1.4082e-02, -6.0056e-03,  5.8742e-02],\n",
      "         [-8.0913e-03,  4.8085e-02, -6.6100e-02],\n",
      "         [-1.9273e-02, -4.7574e-02, -3.2669e-02],\n",
      "         [-6.4302e-02,  8.3410e-04,  3.1508e-03],\n",
      "         [-5.8956e-02, -6.2136e-02,  2.6132e-02],\n",
      "         [-2.0556e-02,  6.1971e-02,  4.7497e-02],\n",
      "         [-2.4338e-03, -5.5071e-02, -6.6159e-02],\n",
      "         [-4.7004e-02,  6.4508e-02,  6.7819e-02],\n",
      "         [ 5.1504e-02, -5.8196e-02,  3.6356e-03],\n",
      "         [ 3.3389e-02,  6.1482e-02,  3.0326e-02]],\n",
      "\n",
      "        [[ 3.9794e-02,  3.2440e-02, -1.2473e-02],\n",
      "         [-7.0900e-05,  6.1933e-02, -5.4930e-02],\n",
      "         [-2.0987e-02,  3.2511e-02, -6.6603e-02],\n",
      "         [-5.8340e-02,  3.0255e-02, -6.0820e-02],\n",
      "         [ 7.0625e-02, -1.6163e-02, -3.3974e-02],\n",
      "         [-4.5333e-02, -5.7596e-02,  9.0827e-03],\n",
      "         [-6.0990e-02,  5.3362e-02, -7.1199e-02],\n",
      "         [ 4.5836e-02,  4.5066e-02, -4.3183e-02],\n",
      "         [ 5.5328e-02, -6.7202e-02, -3.3888e-02],\n",
      "         [ 2.6684e-02,  5.1398e-02,  7.9854e-03],\n",
      "         [-2.6712e-02, -3.2699e-02,  3.3914e-02],\n",
      "         [ 1.5438e-02, -6.7577e-02,  3.5512e-02],\n",
      "         [ 1.5854e-02, -5.5586e-02, -3.2549e-02],\n",
      "         [-1.5831e-02,  7.9957e-03,  6.1447e-02],\n",
      "         [-3.1218e-02,  5.6699e-02, -4.3549e-02],\n",
      "         [ 3.0915e-02,  5.0997e-02, -1.9685e-02],\n",
      "         [-3.8680e-02,  4.4877e-02, -5.1167e-02],\n",
      "         [ 6.6657e-02, -3.1640e-02,  5.8628e-02],\n",
      "         [ 5.3783e-02, -7.8832e-03, -3.0530e-02],\n",
      "         [-5.3738e-02, -5.4392e-02,  1.7355e-02],\n",
      "         [ 4.9778e-02,  1.2851e-02, -4.8974e-02],\n",
      "         [-1.5548e-02, -5.5580e-02, -3.5620e-02],\n",
      "         [-2.9598e-02, -5.9984e-03,  2.9559e-02],\n",
      "         [ 3.9414e-02,  5.9412e-02, -1.7381e-02],\n",
      "         [-6.0885e-02,  3.5399e-02,  7.1531e-02],\n",
      "         [ 3.2986e-03,  1.5867e-02, -1.3003e-02],\n",
      "         [ 3.4828e-02,  4.4753e-02, -3.6487e-02],\n",
      "         [-5.8986e-02, -4.6241e-02, -6.6522e-02],\n",
      "         [-1.0130e-02,  5.1311e-02, -4.7726e-03],\n",
      "         [ 1.4472e-02,  1.6615e-02,  6.4495e-02],\n",
      "         [-6.3668e-02, -4.2011e-02,  1.9500e-02],\n",
      "         [-4.8635e-03, -4.7877e-02, -3.4415e-02],\n",
      "         [-4.8943e-02,  1.9493e-02, -5.3341e-02],\n",
      "         [-6.9321e-02, -3.1966e-02,  3.1414e-02],\n",
      "         [ 2.1833e-02,  2.8356e-02, -4.5229e-02],\n",
      "         [-4.0013e-02,  6.4027e-02,  6.2711e-02],\n",
      "         [-1.2879e-02,  4.0361e-02, -4.3597e-02],\n",
      "         [-2.7430e-02, -2.9703e-02, -6.7940e-02],\n",
      "         [-1.0840e-02,  3.3200e-02,  5.9860e-02],\n",
      "         [ 2.7592e-02, -7.1176e-03,  6.8084e-02],\n",
      "         [ 6.0358e-03,  1.3124e-02, -2.2781e-02],\n",
      "         [-6.7745e-02, -1.1423e-02,  2.1552e-02],\n",
      "         [ 6.9709e-02,  3.1989e-02, -2.9426e-02],\n",
      "         [-1.8821e-02,  3.0398e-02, -6.9990e-02],\n",
      "         [ 2.9468e-02,  2.9735e-04, -3.0643e-02],\n",
      "         [-2.0581e-02,  3.4855e-02,  6.0311e-02],\n",
      "         [ 4.6776e-02,  4.1132e-02,  7.0575e-02],\n",
      "         [-6.0811e-02,  5.5403e-02, -4.5728e-02],\n",
      "         [ 1.8390e-02, -3.3803e-02, -6.5506e-02],\n",
      "         [-1.1055e-02, -6.9220e-02, -2.4931e-02],\n",
      "         [-4.3347e-02,  6.4474e-02,  3.3958e-02],\n",
      "         [-3.7021e-02, -3.0630e-02, -1.0091e-02],\n",
      "         [-4.2397e-02,  2.4087e-02, -6.5586e-02],\n",
      "         [ 6.9256e-02,  3.9234e-02, -7.0227e-02],\n",
      "         [-1.7227e-02, -6.9916e-02, -5.5720e-02],\n",
      "         [ 1.3862e-02, -3.0965e-03, -3.7558e-02],\n",
      "         [-6.5657e-02,  6.8846e-02, -6.3417e-02],\n",
      "         [ 1.0555e-02,  5.2392e-02, -1.3270e-02],\n",
      "         [-1.2697e-02, -2.2747e-02,  5.7449e-02],\n",
      "         [ 3.1995e-02,  3.5311e-02, -6.8807e-02],\n",
      "         [-7.1046e-02,  5.3224e-03, -2.4988e-02],\n",
      "         [-6.8470e-02,  7.0059e-02,  8.9879e-03],\n",
      "         [-3.8579e-02, -3.5642e-02,  6.1584e-02],\n",
      "         [-2.7025e-02, -5.0930e-02, -4.8202e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.bias | Size: torch.Size([64]) | Values : tensor([ 0.0300, -0.0557], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0735,  0.0689,  0.0066,  0.0201, -0.0443, -0.1217,  0.0639,  0.0953,\n",
      "         -0.0281,  0.1192, -0.0923, -0.0843, -0.1239, -0.0431, -0.0637, -0.0904,\n",
      "          0.0322, -0.1199,  0.0685,  0.0085, -0.0135, -0.0337,  0.0002,  0.0610,\n",
      "         -0.0865,  0.0980, -0.0748,  0.1162,  0.0540, -0.0773, -0.0752, -0.1072,\n",
      "         -0.0633, -0.0982,  0.0990,  0.0332,  0.0568, -0.0230,  0.1126,  0.0853,\n",
      "         -0.0868,  0.0625, -0.0417, -0.0355, -0.0206,  0.0303,  0.0490, -0.0471,\n",
      "         -0.1108, -0.0558, -0.0621, -0.0731, -0.1059, -0.0689,  0.0427,  0.1136,\n",
      "          0.0248,  0.0817,  0.0512, -0.0284, -0.1218, -0.0521, -0.0836,  0.0629],\n",
      "        [-0.0640, -0.0677, -0.0414,  0.0899,  0.1127,  0.1042, -0.0613, -0.1061,\n",
      "         -0.0756,  0.0635, -0.0701, -0.1167,  0.0982,  0.1185,  0.0719,  0.1031,\n",
      "         -0.0516, -0.0094,  0.0585, -0.0241, -0.0932, -0.1166, -0.0480,  0.0042,\n",
      "         -0.1219,  0.0690,  0.0590, -0.0561,  0.0162, -0.1179, -0.0392,  0.0697,\n",
      "          0.0191,  0.0428,  0.1152, -0.1054,  0.0816,  0.0157, -0.0197,  0.0259,\n",
      "         -0.0185, -0.0494,  0.0580,  0.0513, -0.0076,  0.0999, -0.0009, -0.0031,\n",
      "         -0.0927,  0.1235,  0.0450, -0.1125,  0.0632, -0.0765,  0.0615,  0.0156,\n",
      "         -0.0789,  0.0971,  0.1105, -0.1216,  0.0444,  0.0186, -0.1180,  0.1022]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[ 0.0202,  0.0684,  0.0439,  0.0451, -0.0102,  0.0342, -0.0021,  0.0230,\n",
      "          0.0997, -0.0904,  0.0961, -0.0244,  0.0880, -0.0963,  0.0436,  0.0115,\n",
      "         -0.0701, -0.0683,  0.0281, -0.1134, -0.0835,  0.1226, -0.0263,  0.0211,\n",
      "         -0.0941,  0.0574, -0.0268,  0.1100, -0.1036,  0.0937, -0.0948, -0.0703,\n",
      "         -0.0734, -0.0384, -0.0384, -0.0483, -0.1079,  0.0075, -0.0503,  0.0753,\n",
      "          0.1207,  0.0797, -0.0152, -0.0717,  0.0355,  0.1175, -0.0320, -0.0964,\n",
      "         -0.0456, -0.0983,  0.0163, -0.0371,  0.0063, -0.0540,  0.1065, -0.0202,\n",
      "          0.1152, -0.0586, -0.0163, -0.0287,  0.0871,  0.0717,  0.1145,  0.0517],\n",
      "        [-0.1093,  0.0168,  0.0652,  0.0022,  0.1147, -0.0241,  0.0286, -0.0574,\n",
      "         -0.0590, -0.0005,  0.0788, -0.0169, -0.0771, -0.0756,  0.0386,  0.0518,\n",
      "         -0.0317,  0.0301, -0.0617, -0.0718, -0.1160, -0.0147, -0.1029,  0.1134,\n",
      "         -0.0032, -0.1168, -0.0913,  0.0600, -0.0932,  0.0273,  0.0630, -0.0655,\n",
      "         -0.1108,  0.1065,  0.0236,  0.0318, -0.0432, -0.0538,  0.0006,  0.0843,\n",
      "         -0.0432, -0.1219, -0.0398, -0.0483, -0.0320, -0.0388,  0.0292, -0.0170,\n",
      "          0.0762,  0.0520, -0.0987, -0.0262, -0.1047, -0.0321, -0.0311, -0.0972,\n",
      "          0.0254,  0.0787, -0.0066, -0.0872,  0.1051,  0.0003, -0.0005, -0.0931]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([-0.0929,  0.0327], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([ 0.0209, -0.1188], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0147,  0.0458,  0.0189, -0.0727,  0.0445,  0.0735, -0.0371,  0.0237,\n",
      "          0.0591, -0.0106,  0.0878,  0.0650,  0.0526,  0.0515, -0.0843, -0.0666,\n",
      "          0.0709,  0.0493,  0.0699,  0.1183, -0.0136, -0.0352, -0.1149, -0.0306,\n",
      "          0.0844,  0.0213,  0.0741,  0.0227,  0.0837,  0.1182,  0.0897,  0.0696,\n",
      "         -0.0758, -0.1213,  0.0708,  0.0623, -0.0850, -0.0089,  0.0519,  0.1022,\n",
      "         -0.1062,  0.1155,  0.0282,  0.0003,  0.0574, -0.0054, -0.1018,  0.0664,\n",
      "         -0.0207, -0.0371, -0.0234, -0.0154,  0.0844, -0.0579,  0.1081, -0.1008,\n",
      "          0.0757, -0.0618, -0.1120, -0.0515,  0.1016, -0.0139,  0.0455,  0.0801],\n",
      "        [ 0.0985,  0.0589, -0.0927,  0.0362,  0.0947,  0.0731,  0.0606,  0.0853,\n",
      "          0.1219, -0.0061,  0.0467, -0.0443, -0.0766,  0.0794, -0.0934, -0.1141,\n",
      "          0.0196,  0.0031, -0.0119, -0.0465, -0.0294, -0.0691, -0.0492,  0.0018,\n",
      "         -0.0364,  0.1229,  0.0883,  0.0384, -0.0911, -0.0661,  0.0393, -0.0253,\n",
      "          0.0020, -0.1038, -0.0813, -0.0787, -0.0213,  0.0406,  0.0020, -0.0179,\n",
      "         -0.0866,  0.0192, -0.0364,  0.0374,  0.0652, -0.0631, -0.1005, -0.0305,\n",
      "         -0.0654,  0.1229,  0.1067,  0.0359, -0.0884, -0.0897, -0.0469,  0.0054,\n",
      "          0.0759,  0.1206, -0.0548,  0.1201,  0.0588,  0.0301, -0.0777, -0.0397]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0520,  0.0987,  0.0224, -0.0487, -0.0370, -0.0772,  0.0610, -0.0959,\n",
      "         -0.0922, -0.0433, -0.0597,  0.0928, -0.1071, -0.0880, -0.0031, -0.0273,\n",
      "         -0.0474, -0.0803, -0.0057, -0.0065, -0.0220,  0.0217, -0.0370,  0.0632,\n",
      "         -0.0446, -0.0780,  0.0003,  0.1005, -0.0299, -0.1148, -0.1001, -0.0547,\n",
      "         -0.0021, -0.0625, -0.0289, -0.0387,  0.0296, -0.0185, -0.0091, -0.0187,\n",
      "          0.1000, -0.0297,  0.0043, -0.0019, -0.0116,  0.0963, -0.0869, -0.0688,\n",
      "          0.1023,  0.0144, -0.0018, -0.0121, -0.0146,  0.0877,  0.0788, -0.1119,\n",
      "          0.0062, -0.0556, -0.0163,  0.0555,  0.0969, -0.0842,  0.0786,  0.0439],\n",
      "        [-0.0828,  0.0096,  0.0981,  0.0712, -0.0066, -0.0819, -0.0613,  0.0649,\n",
      "         -0.0187, -0.0401,  0.0591, -0.0072, -0.0712,  0.0463,  0.0262, -0.0510,\n",
      "         -0.1050, -0.0459,  0.0280,  0.1040,  0.0675, -0.0131,  0.0653, -0.1228,\n",
      "         -0.0466,  0.0603,  0.0898, -0.0545, -0.0108,  0.0515, -0.0793, -0.1185,\n",
      "          0.1018,  0.0930, -0.1185, -0.0616,  0.1246,  0.1181, -0.0805, -0.1100,\n",
      "          0.0003, -0.1134, -0.0984, -0.0833,  0.0255, -0.1159, -0.0524,  0.1212,\n",
      "         -0.0890, -0.1056, -0.0218,  0.0408,  0.0108,  0.0567,  0.0146,  0.0097,\n",
      "          0.1069,  0.0528, -0.1174, -0.0196, -0.0284,  0.0555, -0.0131,  0.1057]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([-0.0922, -0.0339], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([ 0.0277, -0.0571], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.weight | Size: torch.Size([2, 64]) | Values : tensor([[ 0.0585,  0.0187, -0.0837, -0.1127,  0.0193,  0.1149,  0.0188, -0.0006,\n",
      "         -0.1140,  0.0734, -0.0029,  0.0181,  0.0136, -0.0484,  0.1213, -0.0667,\n",
      "         -0.0638,  0.0415, -0.0900,  0.0980, -0.0571,  0.0419,  0.0299,  0.0538,\n",
      "         -0.0465,  0.1053,  0.0700,  0.1032,  0.0100, -0.0697,  0.0588, -0.1250,\n",
      "          0.0585,  0.1077,  0.0357,  0.0945,  0.0620,  0.0673, -0.1110,  0.1091,\n",
      "          0.0102, -0.0241,  0.0416,  0.0219, -0.1195, -0.0983,  0.0554,  0.0322,\n",
      "          0.0300, -0.0342, -0.1025,  0.1174, -0.0415,  0.0164,  0.1235,  0.0522,\n",
      "         -0.0903,  0.0532, -0.1223,  0.0271,  0.0746,  0.0652, -0.0729,  0.0391],\n",
      "        [ 0.1108,  0.0438, -0.0172, -0.0282, -0.0762, -0.0805, -0.0562, -0.0009,\n",
      "         -0.1190, -0.0938,  0.1203,  0.0271,  0.0881,  0.0052, -0.0861, -0.0686,\n",
      "         -0.0031,  0.0209,  0.0245, -0.0649, -0.0011,  0.0255, -0.0048, -0.0392,\n",
      "         -0.1136, -0.0437,  0.0470,  0.0947, -0.0044, -0.1223, -0.0743,  0.0783,\n",
      "         -0.0804,  0.0142,  0.0031, -0.1036,  0.1236, -0.0011,  0.0222,  0.0985,\n",
      "         -0.0535, -0.1246,  0.0682,  0.1018,  0.0749, -0.0250, -0.1120,  0.0010,\n",
      "          0.0570,  0.0548, -0.0464,  0.0205,  0.0288, -0.0883,  0.1193,  0.0292,\n",
      "         -0.0521, -0.0404, -0.0885,  0.0153, -0.0830,  0.0402,  0.0858, -0.0056]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.bias | Size: torch.Size([2]) | Values : tensor([-0.0172,  0.0107], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from models.ConvLSTM_KFall import ConvLSTM\n",
    "# Create an instance of the model\n",
    "model = ConvLSTM()\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print('X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        # transpose X\n",
    "        #X = X.transpose(1, 2)\n",
    "        #print('transposed X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.675038  [   64/12007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012313  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.025725 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.098383  [   64/12007]\n",
      "loss: 0.122668  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.025063 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.012660  [   64/12007]\n",
      "loss: 0.019009  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.023471 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.036535  [   64/12007]\n",
      "loss: 0.070502  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.022846 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.013082  [   64/12007]\n",
      "loss: 0.006317  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.019120 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.005599  [   64/12007]\n",
      "loss: 0.008335  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.025665 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.009072  [   64/12007]\n",
      "loss: 0.025341  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.023345 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.015983  [   64/12007]\n",
      "loss: 0.008978  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.018293 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.003697  [   64/12007]\n",
      "loss: 0.015058  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.025266 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.006096  [   64/12007]\n",
      "loss: 0.045064  [ 6464/12007]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.018519 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.165641 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final test\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

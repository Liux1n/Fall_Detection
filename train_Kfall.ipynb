{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "#sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.1\n",
    "num_window_fall_data = 50\n",
    "num_window_not_fall_data = 3\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  50\n",
      "data.shape:  (15770, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# faltten the data\n",
    "\n",
    "#data = data.reshape(data.shape[0], -1)\n",
    "reshaped_data = data\n",
    "in_channels = reshaped_data.shape[1]\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', reshaped_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 50, 9)\n",
      "X_train_tensor.dtype:  torch.float64\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# create a validate set\n",
    "\n",
    "# create test/validation/test data\n",
    "\"\"\" X_train, X_test, y_train, y_test = train_test_split(reshaped_data, \n",
    "                                                    label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) \"\"\"\n",
    "label = label.astype(np.int64)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reshaped_data, label, test_size=0.05, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "#print('X_test_false.shape: ', X_test_false.shape)\n",
    "\n",
    "#print('len(X_test): ', len)\n",
    "#print('X_test.shape: ', X_test.shape)\n",
    "#print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "y_val_tensor = torch.from_numpy(y_val)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "# print datatype of X_train_tensor\n",
    "X_train_tensor = X_train_tensor.double()\n",
    "print('X_train_tensor.dtype: ', X_train_tensor.dtype)\n",
    "X_test = X_train_tensor.double()\n",
    "\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(64, 2)  # No need for softmax here using nn.CrossEntropyLoss\n",
    "        #self.fc = nn.Sequential(nn.Linear(in_features=64, out_features=2),nn.Softmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = x.transpose(1, 2)  # Transpose to have the correct dimensions for Conv1d (batch, channels, length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        x = x.transpose(1, 2)  # Transpose back to (batch, seq_len, features)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Take the outputs of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # do i need softmax here?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ConvLSTM(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm1): LSTM(64, 64, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (lstm2): LSTM(64, 64, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.0.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[-0.0265, -0.1612, -0.0891],\n",
      "         [ 0.1595,  0.1881,  0.0586],\n",
      "         [ 0.0793,  0.1168, -0.0701],\n",
      "         [ 0.1279,  0.0854,  0.0784],\n",
      "         [ 0.0123,  0.1367,  0.1272],\n",
      "         [ 0.0201, -0.0158,  0.1403],\n",
      "         [ 0.0085, -0.1241,  0.1199],\n",
      "         [-0.0838, -0.0033,  0.0804],\n",
      "         [ 0.0527,  0.1671, -0.0343]],\n",
      "\n",
      "        [[ 0.1502,  0.1707, -0.0132],\n",
      "         [-0.0831,  0.0620, -0.0827],\n",
      "         [-0.1037, -0.1416, -0.1543],\n",
      "         [ 0.0371,  0.1520,  0.1727],\n",
      "         [-0.0987, -0.1472, -0.1758],\n",
      "         [ 0.1558,  0.0720,  0.1171],\n",
      "         [-0.1675, -0.1135,  0.1791],\n",
      "         [-0.0445, -0.0823,  0.1898],\n",
      "         [-0.1242,  0.1453, -0.0912]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0749, -0.1015], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[ 6.5787e-02, -5.8614e-02,  3.0360e-03],\n",
      "         [ 3.2584e-02, -3.1316e-03,  3.1069e-02],\n",
      "         [ 6.9761e-02,  5.4476e-02, -1.8610e-02],\n",
      "         [-6.0206e-02,  4.6943e-02, -4.9951e-02],\n",
      "         [-4.1962e-03,  6.1590e-02, -6.1882e-02],\n",
      "         [-2.1948e-03, -2.2723e-02, -4.1893e-02],\n",
      "         [-7.0366e-02, -5.6731e-02, -1.9330e-02],\n",
      "         [ 3.8367e-02, -5.3486e-02,  8.6838e-03],\n",
      "         [ 7.2159e-02,  5.9374e-02,  5.3368e-02],\n",
      "         [ 2.2147e-02,  4.9838e-02, -5.8211e-02],\n",
      "         [-4.0135e-02, -1.6513e-02,  6.6218e-02],\n",
      "         [ 3.2766e-02,  5.2289e-02,  1.0668e-02],\n",
      "         [ 4.8268e-02, -3.5800e-02, -5.0219e-02],\n",
      "         [-4.1647e-02, -6.1255e-02,  3.7231e-02],\n",
      "         [ 6.4734e-02,  4.8114e-02, -3.5218e-02],\n",
      "         [ 1.8033e-02,  3.1838e-02, -3.7227e-02],\n",
      "         [ 2.6410e-02, -3.2396e-02, -4.7093e-02],\n",
      "         [ 2.8817e-02, -5.2855e-02,  5.1810e-02],\n",
      "         [ 5.2058e-02, -4.5468e-02, -3.8686e-02],\n",
      "         [ 5.1951e-02, -7.1842e-02,  2.8543e-02],\n",
      "         [ 6.8418e-02, -5.4380e-02, -5.1411e-02],\n",
      "         [ 5.6274e-05, -5.6442e-02, -7.3678e-03],\n",
      "         [-2.5779e-02,  3.3228e-02, -5.3601e-02],\n",
      "         [-2.8921e-02, -2.0955e-02,  6.7991e-02],\n",
      "         [-2.1576e-02,  7.1829e-02, -4.9425e-02],\n",
      "         [ 4.4360e-02,  1.1996e-02, -5.3018e-02],\n",
      "         [ 7.1579e-02, -5.4791e-02, -2.5071e-02],\n",
      "         [ 1.1308e-02, -7.1935e-02,  8.6186e-03],\n",
      "         [-2.1985e-02,  3.7506e-02, -1.7252e-02],\n",
      "         [ 4.0011e-02, -6.4281e-02, -2.2437e-02],\n",
      "         [-1.4385e-02,  5.3780e-02,  3.9735e-02],\n",
      "         [ 9.9940e-03, -3.6890e-02, -7.1021e-02],\n",
      "         [ 5.5944e-02, -2.5152e-02,  4.8294e-02],\n",
      "         [-4.5094e-03,  5.2403e-02,  7.0492e-03],\n",
      "         [-4.2005e-03, -3.6847e-02, -3.8851e-02],\n",
      "         [-6.6768e-02, -4.5150e-02, -2.4798e-02],\n",
      "         [ 4.1549e-02,  5.6831e-02, -2.2480e-02],\n",
      "         [ 6.1508e-02, -3.2177e-02, -4.5862e-02],\n",
      "         [ 6.5182e-02,  2.2219e-02, -4.4563e-02],\n",
      "         [ 1.2282e-02, -3.7842e-02, -1.9327e-02],\n",
      "         [ 3.5034e-02, -3.2026e-02,  5.1509e-02],\n",
      "         [-2.3735e-02, -3.1675e-02, -6.3716e-02],\n",
      "         [-5.5932e-02,  5.5119e-02,  5.3060e-02],\n",
      "         [ 7.0664e-02, -2.3422e-02,  2.4885e-02],\n",
      "         [-9.4981e-04,  6.8601e-02,  2.0969e-02],\n",
      "         [ 4.2787e-02, -5.6666e-02, -1.9888e-02],\n",
      "         [-3.2965e-03, -2.2064e-02,  9.8675e-03],\n",
      "         [-4.3246e-02, -4.1597e-03, -2.1959e-02],\n",
      "         [ 5.5824e-02,  6.2190e-04, -5.3953e-02],\n",
      "         [ 2.0706e-03, -1.9189e-02,  5.0240e-02],\n",
      "         [ 3.7036e-02,  1.9480e-02, -1.0282e-02],\n",
      "         [-4.5285e-02, -4.5966e-02,  3.0610e-02],\n",
      "         [-2.9851e-02,  9.9062e-03,  7.4752e-03],\n",
      "         [ 1.4455e-02, -1.5242e-02, -5.2751e-02],\n",
      "         [ 5.0529e-02,  4.4179e-02, -6.2842e-02],\n",
      "         [ 3.6098e-02, -1.7215e-02, -4.7174e-02],\n",
      "         [ 4.0194e-02, -2.3405e-02, -3.7081e-02],\n",
      "         [ 6.8708e-02, -7.1294e-02, -5.4316e-02],\n",
      "         [-4.3846e-03, -6.0697e-02, -6.5257e-02],\n",
      "         [-3.6146e-02, -1.2051e-02, -1.5469e-02],\n",
      "         [ 5.0961e-02,  1.7973e-02, -2.6086e-02],\n",
      "         [ 6.4311e-02,  5.1046e-02, -6.2464e-02],\n",
      "         [-1.3305e-02,  1.5715e-02,  3.3583e-02],\n",
      "         [-3.6005e-02, -4.5153e-02, -5.0581e-02]],\n",
      "\n",
      "        [[-7.1298e-02, -3.4799e-02, -3.5648e-03],\n",
      "         [-5.7331e-02,  2.9218e-02, -3.8554e-02],\n",
      "         [-9.1710e-03, -4.2944e-02,  4.9119e-02],\n",
      "         [-1.1161e-02, -7.0984e-02,  1.6876e-02],\n",
      "         [-2.5177e-02,  9.2896e-03, -2.1822e-02],\n",
      "         [-3.1886e-02, -6.5923e-02, -1.0453e-02],\n",
      "         [-4.3179e-02, -1.4881e-02, -9.1351e-03],\n",
      "         [ 6.3393e-02,  6.6126e-02, -3.8436e-02],\n",
      "         [ 5.7849e-02, -2.1379e-02, -4.1503e-02],\n",
      "         [ 3.0038e-02, -5.4097e-02, -6.3191e-02],\n",
      "         [-6.1905e-02,  2.2194e-02, -3.5197e-02],\n",
      "         [-2.9669e-02,  4.6020e-03, -2.5456e-02],\n",
      "         [-6.4750e-02,  1.9241e-02, -3.2502e-02],\n",
      "         [ 1.5166e-03, -2.1833e-02,  5.8282e-02],\n",
      "         [ 3.0719e-03,  8.4455e-03, -5.0547e-02],\n",
      "         [ 3.2136e-03,  4.5653e-02, -4.5779e-02],\n",
      "         [ 6.8523e-02,  1.6457e-02,  4.6236e-02],\n",
      "         [ 1.3902e-02, -2.9018e-02, -3.1140e-02],\n",
      "         [-1.5570e-02, -6.4266e-02,  5.1617e-02],\n",
      "         [ 5.3277e-02, -5.0918e-02,  4.5648e-02],\n",
      "         [-7.1749e-02,  4.5800e-02,  2.0406e-02],\n",
      "         [ 5.5686e-02, -4.7850e-02,  6.5679e-02],\n",
      "         [ 3.8349e-02, -5.1149e-02,  1.4817e-02],\n",
      "         [-5.3674e-03, -6.9188e-02, -1.7908e-02],\n",
      "         [-1.9898e-02, -6.3040e-02,  7.0319e-02],\n",
      "         [-5.8072e-02,  1.6525e-02,  2.6698e-02],\n",
      "         [-2.5602e-02,  5.8301e-02,  3.2660e-02],\n",
      "         [ 4.5489e-02,  2.3834e-02,  3.9119e-02],\n",
      "         [-1.2816e-02, -5.1191e-02, -2.7411e-02],\n",
      "         [ 6.2973e-02, -3.8528e-02, -5.5426e-02],\n",
      "         [-3.9534e-02,  3.7954e-02, -5.7407e-02],\n",
      "         [ 5.4577e-02, -7.2038e-02,  2.3875e-02],\n",
      "         [ 1.6469e-02, -7.0297e-02,  3.8777e-02],\n",
      "         [-6.2579e-02, -7.8934e-03,  7.0918e-02],\n",
      "         [ 6.0894e-02, -6.1245e-02,  7.2151e-02],\n",
      "         [ 7.7452e-03,  1.0357e-02,  3.0509e-03],\n",
      "         [ 6.6740e-02, -6.8015e-02,  4.3277e-02],\n",
      "         [ 3.3291e-02,  5.0214e-02, -1.1997e-02],\n",
      "         [-6.4972e-02,  2.8998e-02,  2.3684e-02],\n",
      "         [-6.6515e-02,  4.8801e-02, -3.6364e-02],\n",
      "         [ 1.9649e-02,  3.8169e-02,  3.1665e-02],\n",
      "         [-2.8465e-02, -3.1419e-02, -2.3659e-03],\n",
      "         [ 6.1797e-03, -3.3709e-02, -3.7015e-02],\n",
      "         [-2.6426e-02,  5.8600e-02, -4.4102e-04],\n",
      "         [ 3.6915e-02, -2.2795e-02,  3.7149e-02],\n",
      "         [ 3.0143e-03, -1.6911e-02,  2.5375e-02],\n",
      "         [ 6.3854e-02, -4.9085e-02,  4.1407e-02],\n",
      "         [ 4.3721e-02,  3.9178e-02,  2.8044e-04],\n",
      "         [ 3.5690e-02, -4.0440e-02,  1.2182e-02],\n",
      "         [ 5.1900e-02,  5.4988e-02,  2.4711e-02],\n",
      "         [-1.3528e-02, -3.5791e-02,  5.1933e-02],\n",
      "         [-4.8519e-03,  7.1515e-02, -3.8339e-02],\n",
      "         [-4.1605e-02,  7.6983e-03,  3.7370e-02],\n",
      "         [ 2.6983e-02,  2.0588e-02,  6.7304e-02],\n",
      "         [ 2.3960e-02,  2.4772e-02,  3.2247e-02],\n",
      "         [ 6.1709e-02,  1.7528e-02,  1.0200e-02],\n",
      "         [-9.9062e-03,  7.7944e-03, -4.8501e-02],\n",
      "         [-2.7204e-02, -2.4973e-02, -3.3331e-02],\n",
      "         [ 1.9931e-03, -3.7524e-02,  2.2244e-02],\n",
      "         [ 5.6500e-02,  2.2136e-02, -5.7160e-02],\n",
      "         [ 1.0584e-03,  1.6749e-02,  9.0949e-03],\n",
      "         [ 3.4365e-02,  6.3534e-02, -2.0315e-02],\n",
      "         [ 4.7872e-02, -2.7731e-02,  1.9206e-02],\n",
      "         [-1.7149e-02,  8.9623e-03,  4.9273e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.bias | Size: torch.Size([64]) | Values : tensor([0.0712, 0.0414], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-1.4722e-03, -4.5608e-02,  5.4063e-02],\n",
      "         [ 3.0967e-02,  1.2614e-02, -5.2416e-02],\n",
      "         [-3.2219e-02,  5.1195e-02,  5.8572e-02],\n",
      "         [ 4.4037e-02,  4.3800e-03,  1.9446e-02],\n",
      "         [-4.3420e-02, -5.9529e-02,  3.8186e-02],\n",
      "         [ 6.2984e-02,  1.4774e-02,  6.7048e-02],\n",
      "         [ 3.7372e-02,  4.9433e-02, -3.8730e-02],\n",
      "         [-6.7467e-02,  2.2188e-02,  1.8838e-02],\n",
      "         [-2.0221e-02, -5.0138e-02, -3.1775e-02],\n",
      "         [ 5.4788e-02,  6.9505e-02,  5.8438e-02],\n",
      "         [-1.9807e-02,  6.2044e-02, -3.8828e-02],\n",
      "         [-6.4878e-02,  6.1284e-02, -3.7007e-02],\n",
      "         [ 3.1324e-02, -2.3810e-02,  6.7074e-03],\n",
      "         [-7.0621e-02, -3.2262e-02, -4.1902e-02],\n",
      "         [ 1.0107e-02,  4.4047e-02, -5.1667e-02],\n",
      "         [ 4.6020e-03, -2.7361e-02,  3.8011e-02],\n",
      "         [ 3.3654e-02, -6.3161e-03,  5.6554e-02],\n",
      "         [-4.4524e-02, -5.8477e-03,  5.1869e-02],\n",
      "         [ 2.6858e-02,  1.7137e-02, -3.9809e-02],\n",
      "         [-5.1204e-02,  2.7175e-04, -5.5208e-02],\n",
      "         [ 1.6044e-02,  7.1483e-02,  3.8126e-02],\n",
      "         [ 6.8633e-02, -4.8449e-02, -4.7496e-02],\n",
      "         [-1.7184e-02,  1.9576e-02, -4.8524e-02],\n",
      "         [ 1.8885e-02, -6.7700e-02, -3.2359e-02],\n",
      "         [ 7.3785e-03,  5.0233e-02,  1.6996e-02],\n",
      "         [-3.2251e-02,  3.2730e-02, -6.0558e-02],\n",
      "         [ 5.9936e-02,  2.5365e-02,  5.9724e-02],\n",
      "         [-2.6383e-02, -3.5994e-02,  4.4756e-02],\n",
      "         [ 1.4199e-02, -1.3250e-02,  6.5186e-02],\n",
      "         [ 6.3511e-03, -3.8315e-02, -3.2051e-02],\n",
      "         [-7.4037e-04, -2.7998e-02, -6.0457e-02],\n",
      "         [ 1.6689e-02,  6.7404e-02, -4.4501e-02],\n",
      "         [-3.0665e-02,  2.7435e-02, -2.2908e-02],\n",
      "         [-5.5769e-02, -2.5094e-02,  8.3670e-03],\n",
      "         [-5.3929e-02,  1.1320e-02,  6.3166e-03],\n",
      "         [-6.0849e-02, -4.2055e-02, -3.2917e-02],\n",
      "         [ 6.9566e-02, -5.5125e-02,  6.0574e-02],\n",
      "         [ 3.3010e-03,  4.1906e-02,  3.2161e-02],\n",
      "         [ 4.9564e-02,  4.4372e-02, -2.5405e-02],\n",
      "         [-9.6780e-04,  6.1020e-02, -1.9355e-02],\n",
      "         [ 2.7817e-02, -2.0136e-03, -1.5169e-02],\n",
      "         [ 3.5855e-02, -6.4553e-02, -1.9782e-02],\n",
      "         [ 1.3516e-02, -5.1281e-03,  3.2203e-02],\n",
      "         [-2.3677e-02, -2.9897e-02,  7.1045e-02],\n",
      "         [ 6.9491e-02, -5.3829e-02, -1.0351e-02],\n",
      "         [ 6.3860e-02, -4.3701e-02, -2.0114e-02],\n",
      "         [-3.8815e-02, -2.7237e-02,  2.1073e-02],\n",
      "         [ 3.0112e-02,  6.3524e-02, -1.6924e-02],\n",
      "         [ 2.4492e-02,  6.7271e-02,  1.3089e-02],\n",
      "         [ 5.0314e-02,  6.1850e-02, -6.1501e-02],\n",
      "         [-3.5005e-02,  2.7294e-02, -7.0266e-02],\n",
      "         [-5.4640e-03,  2.6956e-02, -6.0703e-02],\n",
      "         [ 3.3755e-02, -7.0714e-03,  5.3617e-02],\n",
      "         [-5.3381e-02, -2.6618e-02,  6.3913e-02],\n",
      "         [-3.9170e-02,  4.3215e-02,  4.0044e-02],\n",
      "         [ 2.0218e-02,  2.6793e-03,  2.8666e-02],\n",
      "         [-4.5632e-02,  4.0578e-02, -1.5413e-02],\n",
      "         [-6.6290e-02, -4.3840e-02,  2.8715e-02],\n",
      "         [-5.5292e-02, -2.9496e-02,  9.7936e-03],\n",
      "         [ 6.6274e-02, -4.8272e-04,  6.2656e-02],\n",
      "         [-4.4835e-02, -6.9004e-02, -6.7950e-02],\n",
      "         [ 3.2173e-02,  4.7091e-02, -2.7494e-02],\n",
      "         [ 6.3367e-02, -7.1347e-02, -1.6267e-02],\n",
      "         [-3.0629e-02,  5.1614e-02,  7.1063e-02]],\n",
      "\n",
      "        [[-2.7407e-02,  6.7719e-02, -1.4256e-02],\n",
      "         [-6.2910e-02,  5.0762e-02, -1.2195e-02],\n",
      "         [ 6.9044e-02,  1.7925e-02,  1.3217e-02],\n",
      "         [-6.4621e-02,  1.3283e-02, -2.0926e-02],\n",
      "         [ 6.4198e-02,  3.1243e-02,  3.8346e-03],\n",
      "         [-1.8567e-02, -7.0800e-02,  4.7153e-02],\n",
      "         [-2.5925e-02,  4.8703e-03,  6.9566e-03],\n",
      "         [-6.4626e-02,  6.0871e-02, -6.8296e-03],\n",
      "         [ 4.3585e-02,  5.1949e-02, -4.8484e-02],\n",
      "         [ 1.2556e-02, -2.8802e-02, -6.2351e-02],\n",
      "         [-2.4016e-02, -3.6054e-02,  1.1163e-02],\n",
      "         [-6.7531e-03,  1.0178e-02,  5.6584e-02],\n",
      "         [ 5.2467e-02,  1.2328e-03,  5.3196e-02],\n",
      "         [-6.0651e-02, -1.4385e-02,  3.2221e-02],\n",
      "         [-7.8097e-03,  4.3289e-02, -6.8840e-02],\n",
      "         [ 2.8813e-03,  2.8528e-02,  2.8162e-02],\n",
      "         [-6.7211e-02, -6.8332e-03,  3.0843e-02],\n",
      "         [-3.6930e-02,  6.7961e-02, -1.9351e-02],\n",
      "         [ 2.7434e-02,  9.8604e-03, -6.1096e-03],\n",
      "         [ 4.2004e-02, -1.9318e-02,  2.6133e-02],\n",
      "         [ 5.8670e-02, -2.9481e-02,  4.1478e-02],\n",
      "         [ 1.5505e-02,  7.1430e-02, -6.7146e-02],\n",
      "         [-2.2005e-02,  4.9668e-03,  3.0716e-02],\n",
      "         [ 5.3943e-02,  1.9138e-02,  1.9325e-02],\n",
      "         [-4.9008e-02, -3.1369e-02, -3.4010e-02],\n",
      "         [ 2.3539e-02, -6.4312e-02, -2.3254e-02],\n",
      "         [ 5.3704e-02, -3.3969e-02,  1.4929e-02],\n",
      "         [-6.2902e-02, -8.2539e-03, -1.5616e-02],\n",
      "         [ 5.5668e-02, -1.6160e-02, -5.4698e-03],\n",
      "         [-4.7694e-02, -1.9360e-02, -1.5611e-02],\n",
      "         [-6.1017e-02, -4.5958e-02,  5.4528e-02],\n",
      "         [-2.1298e-02, -1.5250e-02, -1.7877e-02],\n",
      "         [ 5.7389e-02,  1.6650e-02,  6.9080e-02],\n",
      "         [-7.0426e-02, -1.3027e-02, -1.3818e-02],\n",
      "         [-1.3992e-02, -2.2455e-02,  4.8397e-02],\n",
      "         [-6.4648e-02, -4.0183e-02,  2.1375e-02],\n",
      "         [ 3.0015e-02, -3.6927e-02, -1.3511e-02],\n",
      "         [ 1.4266e-02, -5.0395e-02,  2.0286e-02],\n",
      "         [ 5.5059e-02, -4.6749e-03, -4.1740e-02],\n",
      "         [-9.7197e-03, -2.5026e-02, -7.0974e-02],\n",
      "         [-1.3798e-05,  7.0966e-02, -1.9596e-02],\n",
      "         [-1.1199e-02,  6.6833e-02, -3.4197e-02],\n",
      "         [ 6.7519e-03,  4.3793e-02, -5.2170e-02],\n",
      "         [ 4.6586e-02, -2.8823e-02,  3.7348e-02],\n",
      "         [-6.0303e-02, -6.5570e-02, -4.5849e-03],\n",
      "         [-6.9864e-02, -3.5256e-02,  2.6470e-03],\n",
      "         [-1.6073e-02, -1.5583e-02, -6.5812e-02],\n",
      "         [-2.4169e-02,  8.3453e-04,  2.4395e-02],\n",
      "         [-1.8881e-02,  4.7924e-03,  6.0108e-02],\n",
      "         [ 6.8902e-02,  3.8334e-02,  3.0071e-02],\n",
      "         [-3.3181e-02, -1.6508e-02, -5.7804e-02],\n",
      "         [-6.1694e-02, -5.4265e-02,  6.4956e-02],\n",
      "         [ 3.5597e-02, -6.0604e-02, -6.8892e-03],\n",
      "         [-2.0325e-02,  4.1111e-02,  5.8675e-03],\n",
      "         [-1.7816e-02,  3.4895e-02,  5.1273e-02],\n",
      "         [-2.1399e-02,  1.3063e-02,  6.2456e-03],\n",
      "         [ 4.8406e-02,  4.4788e-02, -6.1920e-02],\n",
      "         [ 3.0131e-02, -1.2011e-02, -1.8610e-02],\n",
      "         [-1.8013e-02, -3.5302e-02, -1.1966e-02],\n",
      "         [-6.2117e-02, -1.8672e-02, -6.6862e-02],\n",
      "         [ 2.8949e-02, -5.2796e-02,  2.7808e-03],\n",
      "         [ 4.4724e-02,  4.1263e-02, -4.9968e-02],\n",
      "         [ 6.0830e-02,  4.2934e-02,  3.1096e-02],\n",
      "         [-5.7062e-02, -1.7730e-02,  4.5076e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0108, -0.0588], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[ 0.0177, -0.1105, -0.1190,  0.0604, -0.0081,  0.1074,  0.0591,  0.0813,\n",
      "         -0.0856, -0.0262, -0.0777,  0.0688,  0.0758, -0.0009, -0.0108,  0.1230,\n",
      "          0.0650, -0.1244, -0.1086,  0.0689, -0.0564,  0.0742, -0.0370, -0.0849,\n",
      "         -0.0947,  0.0528, -0.0309,  0.0455,  0.0310,  0.0560,  0.0209, -0.1087,\n",
      "          0.0400, -0.0595, -0.0910,  0.0267, -0.0035,  0.0569, -0.1050,  0.0725,\n",
      "          0.1022,  0.0367, -0.0616, -0.0735, -0.0922, -0.0538,  0.1059,  0.0996,\n",
      "         -0.1048, -0.0624, -0.0344,  0.0561,  0.0947,  0.0482, -0.0966, -0.0313,\n",
      "         -0.1093,  0.0939,  0.0931,  0.0400,  0.0220,  0.0377,  0.0340,  0.0200],\n",
      "        [ 0.0098,  0.0300, -0.0330,  0.1206,  0.0883,  0.0168,  0.0924, -0.0284,\n",
      "          0.0825,  0.0410,  0.0481, -0.0572,  0.0946,  0.0318,  0.0404,  0.0992,\n",
      "         -0.0233, -0.0071,  0.1018, -0.1126, -0.0774, -0.0501, -0.0686, -0.1215,\n",
      "         -0.1181,  0.0190,  0.0049, -0.0951, -0.1243, -0.0756,  0.0812,  0.1249,\n",
      "         -0.0413, -0.0839, -0.0059, -0.0501, -0.1048,  0.0183,  0.0027, -0.0812,\n",
      "         -0.1167,  0.0993, -0.0385,  0.1169,  0.0720, -0.0557,  0.0300, -0.0374,\n",
      "          0.0823, -0.0656,  0.0370, -0.0878,  0.0408,  0.0992,  0.0898,  0.0280,\n",
      "         -0.0821,  0.0335,  0.0110, -0.0081,  0.0633, -0.1122,  0.0478,  0.0238]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0295,  0.0111,  0.0695,  0.0706,  0.0742, -0.1184, -0.0051,  0.0730,\n",
      "          0.0845, -0.0846, -0.0441, -0.1072, -0.0960,  0.0411, -0.0178,  0.0507,\n",
      "          0.0260,  0.0448, -0.0913, -0.0397, -0.0143,  0.0778, -0.0468,  0.0819,\n",
      "          0.0786, -0.0454, -0.0400,  0.0389,  0.0155,  0.1234, -0.0136, -0.1090,\n",
      "         -0.1010, -0.1052,  0.0752,  0.0380, -0.0809, -0.0856, -0.0920,  0.0121,\n",
      "          0.0948,  0.0259, -0.0894,  0.0017,  0.0450,  0.1159,  0.0168,  0.1206,\n",
      "         -0.1238,  0.0522,  0.0482,  0.0209, -0.0362,  0.0108, -0.1157,  0.0934,\n",
      "         -0.1026,  0.0993, -0.0617,  0.0159,  0.1236, -0.0131, -0.0961,  0.0005],\n",
      "        [ 0.0198, -0.0420,  0.0014, -0.0117,  0.0470, -0.0539,  0.0539,  0.1151,\n",
      "          0.0759, -0.0421,  0.0386,  0.0315,  0.0972, -0.0962,  0.0414, -0.0979,\n",
      "         -0.0207, -0.0243, -0.0594,  0.0429, -0.0707,  0.1120, -0.0676, -0.0459,\n",
      "         -0.0311, -0.1108,  0.0197,  0.0964, -0.0869, -0.0225,  0.0947, -0.0204,\n",
      "         -0.0222,  0.0012,  0.0106,  0.0440,  0.0061, -0.0073,  0.0931,  0.0894,\n",
      "         -0.0838, -0.0759,  0.1178, -0.0181,  0.0195, -0.0540, -0.1167,  0.1056,\n",
      "         -0.0901, -0.1121,  0.1140,  0.0585, -0.0381,  0.0679,  0.0167, -0.0448,\n",
      "          0.0098,  0.0103, -0.1110, -0.0874,  0.0181, -0.0245, -0.0939,  0.0382]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([-0.1182, -0.1191], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm1.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([ 0.0097, -0.0127], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_ih_l0 | Size: torch.Size([256, 64]) | Values : tensor([[-0.0665, -0.1187, -0.0457,  0.0025,  0.1205,  0.0064, -0.1172, -0.0660,\n",
      "         -0.1187, -0.1112,  0.0668,  0.1127, -0.0189,  0.1240,  0.0407,  0.0541,\n",
      "          0.0662, -0.1057, -0.1062, -0.0997,  0.0500, -0.1205,  0.0792,  0.0239,\n",
      "         -0.0232,  0.0373, -0.1105,  0.0032,  0.0668,  0.0095, -0.0413, -0.0384,\n",
      "          0.0331,  0.0785, -0.1017, -0.0643, -0.0759, -0.0767,  0.0360,  0.0690,\n",
      "         -0.0547,  0.0936, -0.1226,  0.0253,  0.0276, -0.0935, -0.0875, -0.0600,\n",
      "         -0.1057, -0.1157,  0.0749,  0.0761, -0.1223, -0.0522,  0.1107,  0.1199,\n",
      "         -0.0509,  0.0583, -0.0312,  0.0798,  0.0806,  0.0118,  0.0162,  0.1143],\n",
      "        [-0.0463,  0.0435, -0.0012, -0.0071,  0.0835, -0.0350,  0.0513,  0.0357,\n",
      "          0.0548, -0.0435,  0.0157,  0.1014, -0.0061, -0.0447, -0.0724, -0.0259,\n",
      "         -0.1209, -0.1109,  0.0798, -0.0673, -0.0104,  0.0579, -0.0407, -0.1163,\n",
      "         -0.0429, -0.0421,  0.0077,  0.0938, -0.0423,  0.0096, -0.1025, -0.0880,\n",
      "         -0.0511,  0.0190,  0.1208,  0.0655, -0.1142, -0.1044,  0.0478,  0.1050,\n",
      "         -0.0537,  0.0505,  0.0410, -0.0844, -0.0755,  0.0503, -0.0218,  0.0052,\n",
      "         -0.0498,  0.0939,  0.0505,  0.1109, -0.0849,  0.0070,  0.0027,  0.1203,\n",
      "          0.0934, -0.0072,  0.0526, -0.0674,  0.0624, -0.0174, -0.0727,  0.0325]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.weight_hh_l0 | Size: torch.Size([256, 64]) | Values : tensor([[ 0.0823, -0.1160,  0.0814,  0.0878, -0.1034,  0.0341, -0.1204,  0.0257,\n",
      "          0.0288, -0.0687, -0.0852, -0.0737, -0.0130, -0.1130, -0.1116, -0.0301,\n",
      "          0.0309,  0.0014,  0.0960, -0.0443, -0.1080,  0.1071,  0.0033, -0.0345,\n",
      "          0.0635, -0.0935,  0.0826,  0.0867,  0.0916,  0.1069,  0.0567,  0.1118,\n",
      "          0.0926,  0.0910, -0.0224, -0.0856, -0.0300, -0.0866,  0.1017, -0.0567,\n",
      "          0.0818,  0.0849, -0.0599, -0.0448, -0.0153,  0.0804,  0.1225, -0.0991,\n",
      "          0.0823, -0.0390,  0.0928,  0.0825,  0.0820,  0.1187,  0.0654,  0.0266,\n",
      "          0.1157,  0.0666, -0.0701, -0.1021, -0.0335,  0.1189,  0.0607,  0.1048],\n",
      "        [-0.0888, -0.0211, -0.1213, -0.0794,  0.0597, -0.0866,  0.0960, -0.0553,\n",
      "          0.1155, -0.0320,  0.0604,  0.0257,  0.1058,  0.0558,  0.0324,  0.0711,\n",
      "         -0.0948, -0.0936,  0.0254, -0.0423, -0.1200,  0.1029, -0.1030, -0.0729,\n",
      "          0.1073, -0.0562,  0.0379, -0.1193,  0.1197,  0.0151,  0.0012,  0.1025,\n",
      "         -0.1124,  0.0580, -0.0260, -0.1217, -0.0231, -0.0317, -0.0855,  0.1055,\n",
      "          0.0728,  0.1220, -0.1022, -0.1168, -0.0272, -0.0437, -0.0610, -0.0358,\n",
      "          0.0654,  0.1214, -0.0350,  0.0750, -0.0885,  0.0888, -0.0203, -0.0775,\n",
      "         -0.1136, -0.0044, -0.0940,  0.0114,  0.0222, -0.0943,  0.0242,  0.0990]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_ih_l0 | Size: torch.Size([256]) | Values : tensor([0.1134, 0.0991], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: lstm2.bias_hh_l0 | Size: torch.Size([256]) | Values : tensor([0.0947, 0.1217], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.weight | Size: torch.Size([2, 64]) | Values : tensor([[-0.0484,  0.1055, -0.0371,  0.0633,  0.0410,  0.1190,  0.0907,  0.0570,\n",
      "          0.0026,  0.0456,  0.1004, -0.0872, -0.0506, -0.0227, -0.1145,  0.0106,\n",
      "         -0.0305,  0.0441,  0.1197, -0.0433,  0.0900, -0.1044, -0.0036,  0.0660,\n",
      "         -0.1119, -0.1079,  0.0516, -0.0798,  0.0892,  0.0713, -0.0783,  0.0052,\n",
      "         -0.0762, -0.0238, -0.0429,  0.1188, -0.0081, -0.0641, -0.0968,  0.0863,\n",
      "         -0.0336,  0.1057,  0.0992,  0.0342,  0.1202, -0.0571, -0.0445, -0.0472,\n",
      "          0.0299,  0.0114, -0.0051,  0.0015,  0.0924, -0.0541, -0.1157,  0.0583,\n",
      "         -0.0083,  0.0779, -0.0065,  0.0019, -0.0226,  0.0002,  0.0699,  0.0404],\n",
      "        [-0.0261, -0.0642,  0.0157, -0.1218,  0.0329, -0.1189,  0.0249, -0.0482,\n",
      "          0.0967,  0.0542, -0.0443, -0.1217,  0.0111, -0.0170,  0.0733, -0.0824,\n",
      "         -0.1027, -0.0588,  0.0147,  0.1159,  0.0328,  0.0105, -0.1021,  0.0945,\n",
      "          0.0291,  0.0502,  0.0275, -0.0544, -0.0560,  0.0898,  0.0620, -0.0654,\n",
      "          0.0819, -0.0880,  0.0248, -0.0103, -0.0025, -0.0017,  0.0496,  0.0652,\n",
      "          0.0929,  0.0176, -0.1204, -0.0180,  0.0093,  0.0234,  0.0833, -0.1194,\n",
      "         -0.0371, -0.0997,  0.0689,  0.0182,  0.1118, -0.0709,  0.0247,  0.0262,\n",
      "          0.0741, -0.0221, -0.1041,  0.0143, -0.0424,  0.0854,  0.0915, -0.1142]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.bias | Size: torch.Size([2]) | Values : tensor([ 0.0414, -0.0572], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from models.ConvLSTM_KFall import ConvLSTM\n",
    "# Create an instance of the model\n",
    "model = ConvLSTM()\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Implementation\\ndef train_loop(dataloader, model, loss_fn, optimizer, patience=5):\\n    #print(dataloader.dataset)\\n    size = len(dataloader.dataset)\\n    # Set the model to training mode\\n    model.train()\\n    for batch, (X, y) in enumerate(dataloader):\\n        # Compute prediction and loss\\n        #print(\\'X.shape: \\', X.shape)  # [64, 50, 9]\\n        # transpose X\\n        #X = X.transpose(1, 2)\\n        #print(\\'transposed X.shape: \\', X.shape)  # [64, 50, 9]\\n        B_size = (y_train == 0).sum()\\n        A_size = (y_train == 1).sum()\\n        #print(\\'B_size: \\', B_size)\\n        #print(\\'A_size: \\', A_size)\\n        B_multiplier = 1\\n        A_multiplier = B_size / A_size\\n        multipliers = torch.where(y == 0, B_multiplier, A_multiplier)\\n        pred = model(X.float())\\n        #loss = loss_fn(pred, y)\\n        loss = (loss_fn(pred, y) * multipliers).mean()\\n\\n        # Backpropagation\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n\\n        if batch % 100 == 0:\\n            loss, current = loss.item(), (batch + 1) * len(X)\\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\\n\\n\\ndef val_loop(dataloader, model, loss_fn):\\n    # Set the model to evaluation mode\\n    model.eval()\\n    size = len(dataloader.dataset)\\n    num_batches = len(dataloader)\\n    test_loss, correct = 0, 0\\n\\n    # Evaluate the model with torch.no_grad()\\n    with torch.no_grad():\\n        for X, y in dataloader:\\n            pred = model(X.float())\\n            test_loss += loss_fn(pred, y).item()\\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\\n\\n    test_loss /= num_batches\\n    correct /= size\\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\\n\\ndef test_loop(dataloader, model, loss_fn):\\n    # Set the model to evaluation mode\\n    model.eval()\\n    size = len(dataloader.dataset)\\n    num_batches = len(dataloader)\\n    test_loss, correct = 0, 0\\n\\n    # Evaluate the model with torch.no_grad()\\n    with torch.no_grad():\\n        for X, y in dataloader:\\n            pred = model(X.float())\\n            print(\\'pred: \\', pred.argmax(1), \\'y: \\', y)\\n            test_loss += loss_fn(pred, y).item()\\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\\n\\n    test_loss /= num_batches\\n    correct /= size\\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\\n '"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Implementation\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, patience=5):\n",
    "    #print(dataloader.dataset)\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        #print('X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        # transpose X\n",
    "        #X = X.transpose(1, 2)\n",
    "        #print('transposed X.shape: ', X.shape)  # [64, 50, 9]\n",
    "        B_size = (y_train == 0).sum()\n",
    "        A_size = (y_train == 1).sum()\n",
    "        #print('B_size: ', B_size)\n",
    "        #print('A_size: ', A_size)\n",
    "        B_multiplier = 1\n",
    "        A_multiplier = B_size / A_size\n",
    "        multipliers = torch.where(y == 0, B_multiplier, A_multiplier)\n",
    "        pred = model(X.float())\n",
    "        #loss = loss_fn(pred, y)\n",
    "        loss = (loss_fn(pred, y) * multipliers).mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            print('pred: ', pred.argmax(1), 'y: ', y)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for t in range(epochs):\\n    print(f\"Epoch {t+1}\\n-------------------------------\")\\n    train_loop(train_dataloader, model, loss_fn, optimizer)\\n    val_loop(val_dataloader, model, loss_fn)\\nprint(\"Done!\") '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth_test(_test,_pred, i):\n",
    "    _test = np.array(_test)\n",
    "    _pred = np.array(_pred)\n",
    "    \n",
    "    _pred_pos = _test[_pred == i]\n",
    "    _pred_neg = _test[_pred != i]\n",
    "    \n",
    "    _true_pos = len(_pred_pos[_pred_pos == i])\n",
    "    _fals_pos = len(_pred_pos[_pred_pos != i])\n",
    "    \n",
    "    _true_neg = len(_pred_neg[_pred_neg != i])\n",
    "    _fals_neg = len(_pred_neg[_pred_neg == i])\n",
    "    \n",
    "    return _true_pos, _fals_pos, _true_neg, _fals_neg\n",
    "\n",
    "def sensitivity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tp / ( tp + fn)\n",
    "\n",
    "def specificity(_test,_pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return tn / ( tn + fp)\n",
    "\n",
    "def accuracy(_test, _pred, i):\n",
    "    tp, fp, tn, fn = truth_test(_test, _pred, i)\n",
    "    return (tp+tn) / (tp + fp + tn + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, val_dataloader, patience=5, scheduler=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            B_size = (y_train == 0).sum()\n",
    "            A_size = (y_train == 1).sum()\n",
    "            B_multiplier = 1\n",
    "            A_multiplier = B_size / A_size\n",
    "            multipliers = torch.where(y == 0, B_multiplier, A_multiplier)\n",
    "            pred = model(X.float())\n",
    "            loss = (loss_fn(pred, y) * multipliers).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        val_loss = val_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            if no_improve_count >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(pred.argmax(1).tolist())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # Calculate accuracy, specificity, and sensitivity for each class\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    # Specificity = TN / (TN + FP)\n",
    "    # Sensitivity = TP / (TP + FN)\n",
    "    for i in range(2):  # Adjust the range depending on the number of classes\n",
    "        print(\" Label\", i)\n",
    "        print(\"    accuracy\\t%5.3f\"%accuracy(y_true, y_pred, i))\n",
    "        print(\" specificity\\t%5.3f\"%specificity(y_true, y_pred, i))\n",
    "        print(\" sensitivity\\t%5.3f\"%sensitivity(y_true, y_pred, i))\n",
    "\n",
    "    # create a confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    # show the confusion matrix on a plot\n",
    "    plt.matshow(cm)\n",
    "    # add legend\n",
    "    plt.colorbar()\n",
    "    # add title\n",
    "    plt.title('Confusion Matrix (0: not fall, 1: fall)')\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss: 1.536826  [   64/11984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.167088  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.095532 \n",
      "\n",
      "Epoch 1:\n",
      "loss: 0.070830  [   64/11984]\n",
      "loss: 0.320696  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.086473 \n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.350267  [   64/11984]\n",
      "loss: 0.180156  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.078969 \n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.048345  [   64/11984]\n",
      "loss: 0.213142  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.075323 \n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.144589  [   64/11984]\n",
      "loss: 0.023312  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.070083 \n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.043800  [   64/11984]\n",
      "loss: 0.051276  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.070990 \n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.102492  [   64/11984]\n",
      "loss: 0.023714  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.064094 \n",
      "\n",
      "Epoch 7:\n",
      "loss: 0.023180  [   64/11984]\n",
      "loss: 0.062944  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.060466 \n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.049100  [   64/11984]\n",
      "loss: 0.081193  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.053575 \n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.140039  [   64/11984]\n",
      "loss: 0.140915  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.075611 \n",
      "\n",
      "Epoch 10:\n",
      "loss: 0.022694  [   64/11984]\n",
      "loss: 0.025227  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.048108 \n",
      "\n",
      "Epoch 11:\n",
      "loss: 0.044997  [   64/11984]\n",
      "loss: 0.012072  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.055323 \n",
      "\n",
      "Epoch 12:\n",
      "loss: 0.010788  [   64/11984]\n",
      "loss: 0.016850  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.050333 \n",
      "\n",
      "Epoch 13:\n",
      "loss: 0.025815  [   64/11984]\n",
      "loss: 0.030713  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047146 \n",
      "\n",
      "Epoch 14:\n",
      "loss: 0.006243  [   64/11984]\n",
      "loss: 0.063621  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073881 \n",
      "\n",
      "Epoch 15:\n",
      "loss: 0.060448  [   64/11984]\n",
      "loss: 0.029936  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045417 \n",
      "\n",
      "Epoch 16:\n",
      "loss: 0.071089  [   64/11984]\n",
      "loss: 0.059434  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.055606 \n",
      "\n",
      "Epoch 17:\n",
      "loss: 0.035149  [   64/11984]\n",
      "loss: 0.046456  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044384 \n",
      "\n",
      "Epoch 18:\n",
      "loss: 0.015793  [   64/11984]\n",
      "loss: 0.031223  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.051507 \n",
      "\n",
      "Epoch 19:\n",
      "loss: 0.034366  [   64/11984]\n",
      "loss: 0.021075  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049779 \n",
      "\n",
      "Epoch 20:\n",
      "loss: 0.099970  [   64/11984]\n",
      "loss: 0.032777  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049713 \n",
      "\n",
      "Epoch 21:\n",
      "loss: 0.028290  [   64/11984]\n",
      "loss: 0.004953  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.051134 \n",
      "\n",
      "Epoch 00022: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 22:\n",
      "loss: 0.003626  [   64/11984]\n",
      "loss: 0.004469  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.040677 \n",
      "\n",
      "Epoch 23:\n",
      "loss: 0.004142  [   64/11984]\n",
      "loss: 0.008953  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.040468 \n",
      "\n",
      "Epoch 24:\n",
      "loss: 0.002937  [   64/11984]\n",
      "loss: 0.003296  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.040737 \n",
      "\n",
      "Epoch 25:\n",
      "loss: 0.001967  [   64/11984]\n",
      "loss: 0.001507  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.041439 \n",
      "\n",
      "Epoch 26:\n",
      "loss: 0.001334  [   64/11984]\n",
      "loss: 0.011589  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.042042 \n",
      "\n",
      "Epoch 27:\n",
      "loss: 0.001467  [   64/11984]\n",
      "loss: 0.000090  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.042967 \n",
      "\n",
      "Epoch 00028: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 28:\n",
      "loss: 0.000323  [   64/11984]\n",
      "loss: 0.000968  [ 6464/11984]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.042408 \n",
      "\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "train_loop(train_dataloader, model, loss_fn, optimizer,val_dataloader, patience=5, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.068045 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.982\n",
      " specificity\t0.964\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.982\n",
      " specificity\t1.000\n",
      " sensitivity\t0.964\n",
      "[[28  0]\n",
      " [ 1 27]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final test\n",
    "\n",
    "test_loop(test_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

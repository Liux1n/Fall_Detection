{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "(1194, 9, 50)\n",
      "Processing 2/32 folder...\n",
      "(750, 9, 50)\n",
      "Processing 3/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 4/32 folder...\n",
      "(765, 9, 50)\n",
      "Processing 5/32 folder...\n",
      "(795, 9, 50)\n",
      "Processing 6/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 7/32 folder...\n",
      "(710, 9, 50)\n",
      "Processing 8/32 folder...\n",
      "(800, 9, 50)\n",
      "Processing 9/32 folder...\n",
      "(810, 9, 50)\n",
      "Processing 10/32 folder...\n",
      "(810, 9, 50)\n",
      "Processing 11/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 12/32 folder...\n",
      "(805, 9, 50)\n",
      "Processing 13/32 folder...\n",
      "(800, 9, 50)\n",
      "Processing 14/32 folder...\n",
      "(790, 9, 50)\n",
      "Processing 15/32 folder...\n",
      "(780, 9, 50)\n",
      "Processing 16/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 17/32 folder...\n",
      "(805, 9, 50)\n",
      "Processing 18/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 19/32 folder...\n",
      "(765, 9, 50)\n",
      "Processing 20/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 21/32 folder...\n",
      "(805, 9, 50)\n",
      "Processing 22/32 folder...\n",
      "(785, 9, 50)\n",
      "Processing 23/32 folder...\n",
      "(780, 9, 50)\n",
      "Processing 24/32 folder...\n",
      "(790, 9, 50)\n",
      "Processing 25/32 folder...\n",
      "(805, 9, 50)\n",
      "Processing 26/32 folder...\n",
      "(780, 9, 50)\n",
      "Processing 27/32 folder...\n",
      "(815, 9, 50)\n",
      "Processing 28/32 folder...\n",
      "(810, 9, 50)\n",
      "Processing 29/32 folder...\n",
      "(800, 9, 50)\n",
      "Processing 30/32 folder...\n",
      "(800, 9, 50)\n",
      "Processing 31/32 folder...\n",
      "(740, 9, 50)\n",
      "Processing 32/32 folder...\n",
      "(810, 9, 50)\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "#sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "#sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.1\n",
    "num_window_fall_data = 50\n",
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  9\n",
      "data.shape:  (25789, 9, 50)\n"
     ]
    }
   ],
   "source": [
    "in_channels = data.shape[1]\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_size:  25020\n",
      "A_size:  769\n",
      "data.shape:  (25789, 9, 50)\n",
      "(64, 9, 50)\n",
      "X_train_tensor.dtype:  torch.float64\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "label = label.astype(np.int64)\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "# transpose the data to (batch_size, in_channels, sequence_length)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "print('data.shape: ', data.shape)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.05, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "y_val_tensor = torch.from_numpy(y_val)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "# print datatype of X_train_tensor\n",
    "X_train_tensor = X_train_tensor.double()\n",
    "print('X_train_tensor.dtype: ', X_train_tensor.dtype)\n",
    "X_test = X_train_tensor.double()\n",
    "\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM_backup(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "\n",
    "        # LSTM layers\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(64, 2)  # No need for softmax here when using nn.CrossEntropyLoss\n",
    "        \n",
    "        self.dequant = torch.quantization.DeQuantStub()    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # x.shape = (batch_size, channels, sequence_length)\n",
    "        # Prepare for LSTM\n",
    "        # batch_first=True: (batch, sequence_length, channels)\n",
    "\n",
    "\n",
    "        # transpose x to (batch_size, sequence_length, channels)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)  # Only take the output, ignore hidden states\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Take the outputs of the last time step\n",
    "        x = x.select(1, -1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm1d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(2))\n",
    "\n",
    "        # LSTM layers\n",
    "        \n",
    "        #self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        #self.dropout1 = nn.Dropout(0.5)\n",
    "        #self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Linear(384, 2)  # No need for softmax here when using nn.CrossEntropyLoss\n",
    "        \n",
    "        self.dequant = torch.quantization.DeQuantStub()    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        #print('x.shape: ', x.shape) # x.shape:  torch.Size([64, 64, 6])\n",
    "        # x.shape = (batch_size, channels, sequence_length)\n",
    "        # Prepare for LSTM\n",
    "        # batch_first=True: (batch, sequence_length, channels)\n",
    "\n",
    "\n",
    "        # transpose x to (batch_size, sequence_length, channels)\n",
    "        #x = x.transpose(1, 2)\n",
    "        \n",
    "        # LSTM layers\n",
    "        #x, _ = self.lstm1(x)  # Only take the output, ignore hidden states\n",
    "        #x = self.dropout1(x)\n",
    "        #x, _ = self.lstm2(x)  # Only take the output, ignore hidden states\n",
    "        #x = self.dropout2(x)\n",
    "        \n",
    "        # Take the outputs of the last time step\n",
    "        #x = x.select(1, -1)\n",
    "        # Flatten the output of the convolutional layers\n",
    "        #x = x.reshape(x.size(0), -1)\n",
    "        # dont use reshape\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        #x = x.reshape(x.size(0), -1) \n",
    "        #x = x.contiguous().view(x.size(0), -1)\n",
    "        #\n",
    "        #print('x.shape: ', x.shape) # x.shape:  torch.Size([64, 384])\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ConvLSTM(\n",
      "  (quant): QuantStub()\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(9, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.0.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[ 0.0903,  0.0025,  0.1092],\n",
      "         [ 0.1371,  0.1434, -0.1068],\n",
      "         [-0.1328, -0.0778, -0.1259],\n",
      "         [-0.0896, -0.1761, -0.0532],\n",
      "         [-0.1207,  0.0696, -0.1691],\n",
      "         [ 0.1459, -0.1281, -0.1789],\n",
      "         [-0.1452, -0.1859, -0.1501],\n",
      "         [ 0.0294, -0.0293,  0.1367],\n",
      "         [ 0.1751,  0.0541,  0.1015]],\n",
      "\n",
      "        [[-0.0214,  0.0754, -0.0099],\n",
      "         [ 0.1628, -0.1366,  0.1481],\n",
      "         [-0.0070, -0.1565,  0.0612],\n",
      "         [-0.0457,  0.0153, -0.0536],\n",
      "         [-0.1517,  0.0892,  0.1130],\n",
      "         [ 0.1190,  0.1530,  0.0190],\n",
      "         [ 0.1151,  0.1558,  0.1169],\n",
      "         [ 0.1206,  0.0718,  0.0430],\n",
      "         [-0.1027, -0.1112,  0.1364]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.0.bias | Size: torch.Size([64]) | Values : tensor([-0.0536,  0.0704], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[ 0.0401,  0.0515,  0.0551],\n",
      "         [ 0.0556,  0.0468,  0.0210],\n",
      "         [ 0.0402, -0.0040,  0.0525],\n",
      "         [-0.0496, -0.0502, -0.0285],\n",
      "         [ 0.0184,  0.0054, -0.0279],\n",
      "         [-0.0459, -0.0128,  0.0570],\n",
      "         [ 0.0190,  0.0327, -0.0424],\n",
      "         [-0.0626,  0.0420, -0.0217],\n",
      "         [ 0.0186,  0.0713, -0.0495],\n",
      "         [ 0.0464,  0.0404,  0.0368],\n",
      "         [-0.0106,  0.0428,  0.0592],\n",
      "         [-0.0475, -0.0596, -0.0475],\n",
      "         [-0.0649, -0.0639, -0.0382],\n",
      "         [ 0.0693, -0.0536,  0.0498],\n",
      "         [-0.0542,  0.0100, -0.0672],\n",
      "         [-0.0222, -0.0508, -0.0650],\n",
      "         [ 0.0264, -0.0586,  0.0198],\n",
      "         [-0.0517,  0.0097,  0.0592],\n",
      "         [-0.0315, -0.0301, -0.0509],\n",
      "         [ 0.0071,  0.0449,  0.0194],\n",
      "         [ 0.0637,  0.0545, -0.0628],\n",
      "         [-0.0110,  0.0371,  0.0547],\n",
      "         [ 0.0315,  0.0714, -0.0019],\n",
      "         [ 0.0521,  0.0647,  0.0530],\n",
      "         [ 0.0122, -0.0338, -0.0113],\n",
      "         [-0.0248, -0.0003, -0.0690],\n",
      "         [-0.0264,  0.0088, -0.0392],\n",
      "         [-0.0258,  0.0144, -0.0573],\n",
      "         [ 0.0023, -0.0020,  0.0003],\n",
      "         [ 0.0722,  0.0040, -0.0445],\n",
      "         [-0.0385, -0.0048,  0.0603],\n",
      "         [-0.0232, -0.0020,  0.0023],\n",
      "         [ 0.0039, -0.0309, -0.0558],\n",
      "         [-0.0469, -0.0450, -0.0483],\n",
      "         [ 0.0189,  0.0125,  0.0611],\n",
      "         [-0.0077, -0.0305, -0.0179],\n",
      "         [ 0.0019, -0.0541,  0.0598],\n",
      "         [-0.0541,  0.0564,  0.0230],\n",
      "         [-0.0696,  0.0418, -0.0641],\n",
      "         [ 0.0216,  0.0568, -0.0450],\n",
      "         [ 0.0326, -0.0424,  0.0162],\n",
      "         [ 0.0081, -0.0264, -0.0721],\n",
      "         [-0.0380,  0.0338,  0.0117],\n",
      "         [-0.0700, -0.0059,  0.0013],\n",
      "         [-0.0204, -0.0028, -0.0423],\n",
      "         [ 0.0078,  0.0451, -0.0415],\n",
      "         [-0.0084,  0.0175,  0.0278],\n",
      "         [ 0.0340,  0.0043,  0.0457],\n",
      "         [ 0.0431,  0.0412,  0.0270],\n",
      "         [-0.0271, -0.0260,  0.0562],\n",
      "         [ 0.0134, -0.0068, -0.0320],\n",
      "         [-0.0638,  0.0329,  0.0064],\n",
      "         [-0.0153,  0.0047,  0.0716],\n",
      "         [ 0.0405,  0.0691,  0.0278],\n",
      "         [-0.0314,  0.0341,  0.0663],\n",
      "         [ 0.0010, -0.0585,  0.0698],\n",
      "         [ 0.0449,  0.0032, -0.0221],\n",
      "         [-0.0418, -0.0328,  0.0138],\n",
      "         [-0.0060,  0.0058,  0.0377],\n",
      "         [-0.0348,  0.0036,  0.0366],\n",
      "         [ 0.0194, -0.0643,  0.0652],\n",
      "         [ 0.0486, -0.0646, -0.0589],\n",
      "         [ 0.0036,  0.0702, -0.0653],\n",
      "         [ 0.0418, -0.0508,  0.0098]],\n",
      "\n",
      "        [[-0.0516,  0.0538, -0.0640],\n",
      "         [ 0.0196, -0.0293,  0.0597],\n",
      "         [-0.0026,  0.0233, -0.0282],\n",
      "         [-0.0200,  0.0641, -0.0139],\n",
      "         [-0.0485,  0.0166,  0.0090],\n",
      "         [ 0.0443, -0.0313, -0.0255],\n",
      "         [-0.0051, -0.0194, -0.0566],\n",
      "         [ 0.0159,  0.0112, -0.0241],\n",
      "         [-0.0233, -0.0351, -0.0123],\n",
      "         [-0.0590,  0.0664, -0.0707],\n",
      "         [ 0.0592,  0.0040, -0.0049],\n",
      "         [ 0.0621,  0.0249,  0.0302],\n",
      "         [-0.0499, -0.0197, -0.0487],\n",
      "         [-0.0423, -0.0217, -0.0445],\n",
      "         [ 0.0609, -0.0414,  0.0277],\n",
      "         [ 0.0579,  0.0015, -0.0187],\n",
      "         [-0.0631, -0.0547,  0.0632],\n",
      "         [-0.0016,  0.0320, -0.0463],\n",
      "         [-0.0632, -0.0597, -0.0496],\n",
      "         [ 0.0293,  0.0668,  0.0528],\n",
      "         [-0.0333,  0.0619,  0.0619],\n",
      "         [-0.0172,  0.0632, -0.0583],\n",
      "         [ 0.0374, -0.0620,  0.0205],\n",
      "         [ 0.0205, -0.0339,  0.0006],\n",
      "         [ 0.0599,  0.0583,  0.0520],\n",
      "         [-0.0415, -0.0364,  0.0081],\n",
      "         [-0.0412,  0.0449,  0.0058],\n",
      "         [-0.0099,  0.0198,  0.0413],\n",
      "         [-0.0522, -0.0040, -0.0122],\n",
      "         [ 0.0108, -0.0072,  0.0182],\n",
      "         [ 0.0586, -0.0425, -0.0135],\n",
      "         [ 0.0081, -0.0128, -0.0322],\n",
      "         [ 0.0053,  0.0185,  0.0651],\n",
      "         [ 0.0502, -0.0245,  0.0138],\n",
      "         [-0.0418,  0.0517, -0.0059],\n",
      "         [-0.0123,  0.0388,  0.0451],\n",
      "         [ 0.0021, -0.0482, -0.0544],\n",
      "         [-0.0189,  0.0056,  0.0248],\n",
      "         [ 0.0169,  0.0529, -0.0274],\n",
      "         [ 0.0573,  0.0167,  0.0156],\n",
      "         [ 0.0066, -0.0282,  0.0368],\n",
      "         [ 0.0009, -0.0303, -0.0308],\n",
      "         [-0.0386, -0.0554,  0.0190],\n",
      "         [ 0.0412, -0.0347, -0.0011],\n",
      "         [-0.0216, -0.0693,  0.0351],\n",
      "         [-0.0016,  0.0345, -0.0411],\n",
      "         [ 0.0137,  0.0337,  0.0595],\n",
      "         [ 0.0589, -0.0241, -0.0556],\n",
      "         [-0.0373,  0.0692, -0.0433],\n",
      "         [ 0.0620, -0.0279,  0.0476],\n",
      "         [ 0.0440,  0.0092, -0.0585],\n",
      "         [ 0.0269,  0.0114, -0.0714],\n",
      "         [ 0.0715, -0.0624, -0.0230],\n",
      "         [ 0.0348, -0.0124, -0.0687],\n",
      "         [ 0.0488,  0.0098,  0.0309],\n",
      "         [ 0.0014, -0.0352,  0.0363],\n",
      "         [-0.0398,  0.0635,  0.0321],\n",
      "         [ 0.0520,  0.0269,  0.0188],\n",
      "         [ 0.0702, -0.0105, -0.0656],\n",
      "         [ 0.0692, -0.0330, -0.0235],\n",
      "         [-0.0715, -0.0488, -0.0457],\n",
      "         [-0.0059,  0.0625,  0.0468],\n",
      "         [ 0.0140, -0.0355,  0.0459],\n",
      "         [-0.0126,  0.0360,  0.0294]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.0.bias | Size: torch.Size([64]) | Values : tensor([ 0.0270, -0.0588], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-4.8494e-02, -6.6978e-02,  3.9518e-02],\n",
      "         [-2.0103e-02, -5.7232e-02,  1.2448e-02],\n",
      "         [ 2.2867e-02, -3.8526e-02, -2.4201e-02],\n",
      "         [-3.2169e-02,  2.8403e-02,  5.1016e-02],\n",
      "         [ 5.1882e-02,  2.5546e-02,  3.2892e-02],\n",
      "         [-5.0877e-02, -3.0636e-02, -3.3167e-02],\n",
      "         [ 4.7774e-02, -6.7687e-02, -2.5368e-03],\n",
      "         [ 5.8180e-02, -6.2703e-02, -2.0656e-03],\n",
      "         [ 6.8425e-03, -2.9120e-02,  3.1569e-02],\n",
      "         [-1.9485e-02, -5.3039e-02,  6.6326e-02],\n",
      "         [-6.6080e-02,  1.5992e-03,  3.7500e-02],\n",
      "         [ 5.3933e-02, -1.8213e-02, -2.1956e-02],\n",
      "         [-4.8645e-02,  5.9651e-02,  1.5819e-02],\n",
      "         [-4.8484e-02,  4.9223e-02,  5.7307e-02],\n",
      "         [ 6.8493e-02,  2.8489e-02, -4.9047e-03],\n",
      "         [ 2.5175e-03,  6.7069e-02,  4.4303e-02],\n",
      "         [-6.4929e-02, -7.0031e-03,  6.5471e-02],\n",
      "         [-2.6614e-02, -1.0852e-02, -3.0898e-02],\n",
      "         [-4.4406e-02, -5.7423e-02, -5.3396e-02],\n",
      "         [ 3.9109e-02, -7.8594e-03, -8.2095e-03],\n",
      "         [-5.6764e-02, -3.9170e-02, -3.0088e-02],\n",
      "         [ 5.8716e-03,  5.3095e-02, -5.5901e-02],\n",
      "         [-7.0698e-02,  6.4923e-02,  3.3553e-02],\n",
      "         [-3.2280e-02, -5.8733e-02,  5.4075e-02],\n",
      "         [-1.2714e-02, -3.8138e-03, -6.6118e-03],\n",
      "         [-2.0617e-02,  6.0582e-02,  1.2307e-02],\n",
      "         [ 5.1986e-02, -3.1262e-02, -2.9025e-02],\n",
      "         [-6.4440e-02,  6.6302e-02, -6.1211e-02],\n",
      "         [ 1.3232e-02,  5.9296e-02,  6.8769e-02],\n",
      "         [ 9.9515e-03,  4.2343e-02,  6.0664e-02],\n",
      "         [ 5.7887e-02, -4.7507e-02,  4.7388e-02],\n",
      "         [-2.3808e-02,  2.9622e-02, -2.3666e-02],\n",
      "         [-4.4877e-02, -3.2024e-02,  3.1003e-02],\n",
      "         [ 2.4124e-03,  6.2225e-02, -3.4864e-02],\n",
      "         [ 2.2201e-02,  1.5898e-02, -2.0986e-02],\n",
      "         [ 5.2541e-02,  6.0869e-02,  6.1197e-02],\n",
      "         [-8.6053e-03,  2.3971e-03, -4.7695e-02],\n",
      "         [ 1.0368e-02, -5.6247e-02,  1.8722e-02],\n",
      "         [-1.6615e-02, -5.1295e-02,  5.1005e-02],\n",
      "         [ 5.0847e-02, -6.4999e-02, -4.6214e-02],\n",
      "         [ 3.6805e-02,  3.1241e-02, -6.6279e-02],\n",
      "         [ 4.6003e-03,  4.9086e-02,  2.3097e-02],\n",
      "         [ 1.0734e-02,  5.7927e-02, -3.3400e-03],\n",
      "         [-6.1992e-03,  7.1483e-02, -4.0606e-06],\n",
      "         [ 1.1476e-02,  6.0302e-02,  2.9027e-02],\n",
      "         [ 1.5267e-02, -3.4595e-02,  9.7139e-03],\n",
      "         [-6.5090e-02, -5.3792e-02,  2.2547e-03],\n",
      "         [-5.1093e-02, -6.4404e-02,  6.1182e-02],\n",
      "         [ 4.7000e-03,  4.2948e-03, -1.6597e-02],\n",
      "         [ 1.1108e-02, -7.0192e-02, -7.1133e-02],\n",
      "         [-6.1973e-02,  2.0243e-02,  2.1867e-03],\n",
      "         [ 2.5374e-02,  4.8598e-02,  3.6104e-02],\n",
      "         [-3.1111e-02, -1.8412e-02,  3.5084e-02],\n",
      "         [ 6.0610e-02, -2.8975e-02,  1.0483e-02],\n",
      "         [-4.9486e-02, -4.0679e-02,  1.1619e-02],\n",
      "         [-3.6235e-02, -2.7980e-03, -4.7421e-02],\n",
      "         [-9.9361e-03,  6.6147e-02, -3.7656e-02],\n",
      "         [-3.4466e-02,  5.5936e-02, -5.4439e-02],\n",
      "         [ 6.7073e-03, -5.7324e-02, -4.9730e-02],\n",
      "         [-5.6946e-02, -8.9554e-03,  2.4043e-02],\n",
      "         [ 6.7741e-02, -5.6234e-02,  2.9759e-02],\n",
      "         [ 1.2252e-02, -1.5725e-02, -7.0994e-02],\n",
      "         [-5.7724e-02,  2.3798e-03, -2.0230e-02],\n",
      "         [-1.2690e-02,  4.9272e-02, -1.8483e-02]],\n",
      "\n",
      "        [[-5.2913e-02, -4.0153e-02,  5.4494e-02],\n",
      "         [ 4.3676e-02,  5.2021e-03, -5.0368e-02],\n",
      "         [ 3.4139e-02, -2.0834e-02,  2.8277e-02],\n",
      "         [-4.6461e-02,  6.9605e-02,  7.1634e-02],\n",
      "         [-1.0010e-02,  5.8869e-02,  6.4235e-02],\n",
      "         [ 6.3270e-02, -6.7833e-02, -3.8160e-03],\n",
      "         [-7.1454e-02,  3.4541e-02, -5.5114e-02],\n",
      "         [-2.8123e-02,  6.1942e-02,  2.0958e-02],\n",
      "         [ 3.2584e-02,  5.9865e-02,  1.9563e-02],\n",
      "         [ 2.2300e-02, -2.3059e-02, -5.2009e-02],\n",
      "         [ 1.7392e-02, -4.5518e-02, -2.9577e-02],\n",
      "         [ 4.1845e-02,  3.9485e-02, -5.6914e-02],\n",
      "         [-3.8780e-03, -3.2031e-02,  6.8329e-02],\n",
      "         [ 2.7223e-03, -6.9706e-02,  1.6838e-03],\n",
      "         [ 5.2257e-02, -3.6073e-02, -6.8779e-02],\n",
      "         [-1.9985e-02, -3.2565e-02, -3.9059e-02],\n",
      "         [ 3.0287e-02,  4.5423e-02, -4.2313e-02],\n",
      "         [-1.5650e-02, -5.9882e-02,  1.6109e-02],\n",
      "         [ 1.6489e-02, -2.3094e-02, -5.6447e-02],\n",
      "         [ 3.6485e-02,  1.8454e-02,  9.4849e-03],\n",
      "         [ 1.2845e-02,  6.2656e-02, -6.9815e-02],\n",
      "         [ 4.6890e-02,  3.7293e-02,  5.2015e-02],\n",
      "         [-1.3924e-03, -2.7195e-03,  3.2029e-02],\n",
      "         [-3.1747e-02, -4.4743e-02, -2.6906e-02],\n",
      "         [ 3.1815e-02,  3.3911e-02, -3.6782e-02],\n",
      "         [-5.4980e-02, -6.8037e-02, -5.0644e-02],\n",
      "         [ 5.4825e-02, -5.7410e-02, -5.3606e-02],\n",
      "         [ 2.6526e-02, -6.4361e-02,  3.0143e-02],\n",
      "         [ 5.1456e-02, -6.9878e-02, -4.3323e-02],\n",
      "         [-2.4949e-02, -5.7814e-02,  6.6746e-02],\n",
      "         [ 4.5636e-02,  1.9625e-02, -4.0326e-02],\n",
      "         [ 2.3226e-02, -3.7638e-02, -3.9620e-02],\n",
      "         [ 5.1025e-02,  4.9659e-02, -3.6738e-02],\n",
      "         [-7.8272e-03, -1.8118e-02, -3.0924e-02],\n",
      "         [ 1.0589e-02, -5.3034e-02, -2.2174e-02],\n",
      "         [ 4.0426e-02, -1.0759e-02, -2.0647e-02],\n",
      "         [-2.6027e-02,  6.5011e-02,  1.9550e-02],\n",
      "         [-6.6296e-02,  5.1913e-02,  4.3676e-02],\n",
      "         [-3.3768e-02, -4.2358e-02,  2.9187e-02],\n",
      "         [ 2.8773e-02, -3.8943e-02, -7.3772e-03],\n",
      "         [-2.8165e-02, -1.7968e-02,  1.3026e-03],\n",
      "         [ 6.5948e-02, -5.5708e-02,  2.5771e-02],\n",
      "         [ 4.5503e-02,  3.6837e-02,  6.9777e-04],\n",
      "         [-7.0942e-02, -2.6605e-02, -5.4601e-02],\n",
      "         [-5.3153e-03,  6.8735e-03, -2.0487e-02],\n",
      "         [-5.7048e-02, -5.0272e-02, -5.2232e-02],\n",
      "         [ 9.1717e-03, -5.9167e-02,  6.4968e-02],\n",
      "         [ 6.9433e-02, -4.0323e-02, -6.9057e-02],\n",
      "         [-6.6400e-02,  6.5227e-02, -3.9510e-02],\n",
      "         [ 6.0740e-02,  4.7018e-02,  4.6461e-02],\n",
      "         [ 5.6383e-02, -5.3478e-03, -3.0910e-03],\n",
      "         [ 5.3870e-02,  7.0821e-02,  2.7206e-02],\n",
      "         [ 6.1115e-02,  6.1671e-02, -2.8876e-02],\n",
      "         [ 1.8432e-02,  2.5774e-02,  3.1028e-02],\n",
      "         [ 1.5949e-02, -3.3165e-02, -5.5966e-02],\n",
      "         [-1.3720e-02, -4.1681e-02, -2.8924e-02],\n",
      "         [ 6.2113e-02, -1.5230e-02, -4.6192e-02],\n",
      "         [-6.6925e-02,  6.0517e-02, -5.6901e-02],\n",
      "         [ 7.0845e-02, -2.5784e-02, -1.7656e-02],\n",
      "         [-2.5043e-02, -7.4838e-03,  5.6201e-02],\n",
      "         [ 6.3528e-02, -2.8895e-02,  2.1838e-03],\n",
      "         [-5.0578e-02, -3.5397e-02,  7.1588e-02],\n",
      "         [-6.2431e-02, -1.2052e-02,  4.6644e-02],\n",
      "         [-4.0485e-02,  6.0855e-02,  1.0687e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.0.bias | Size: torch.Size([64]) | Values : tensor([ 0.0216, -0.0599], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv3.1.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.weight | Size: torch.Size([2, 384]) | Values : tensor([[ 0.0322,  0.0167, -0.0145,  0.0236,  0.0459,  0.0133, -0.0415, -0.0099,\n",
      "          0.0173,  0.0113, -0.0110, -0.0376, -0.0194,  0.0363,  0.0504,  0.0211,\n",
      "          0.0031, -0.0002,  0.0045, -0.0080, -0.0095, -0.0039, -0.0117, -0.0315,\n",
      "         -0.0176, -0.0414, -0.0206,  0.0263,  0.0456,  0.0326,  0.0096,  0.0471,\n",
      "          0.0022, -0.0140, -0.0315,  0.0302,  0.0099,  0.0011, -0.0505, -0.0438,\n",
      "          0.0166,  0.0259,  0.0244,  0.0475,  0.0153, -0.0387, -0.0180, -0.0079,\n",
      "          0.0289, -0.0039,  0.0435,  0.0077, -0.0240,  0.0178, -0.0226, -0.0466,\n",
      "          0.0062,  0.0135, -0.0336,  0.0055,  0.0058,  0.0271, -0.0443,  0.0439,\n",
      "          0.0437, -0.0274,  0.0334,  0.0425,  0.0265, -0.0078,  0.0045,  0.0377,\n",
      "         -0.0375, -0.0356, -0.0408,  0.0290, -0.0419,  0.0250, -0.0368,  0.0178,\n",
      "          0.0360, -0.0354,  0.0113,  0.0013,  0.0483,  0.0427,  0.0078, -0.0301,\n",
      "          0.0166,  0.0469, -0.0212,  0.0282,  0.0407,  0.0090,  0.0383,  0.0242,\n",
      "         -0.0323,  0.0358, -0.0074,  0.0026,  0.0029, -0.0397,  0.0426,  0.0069,\n",
      "          0.0237, -0.0069, -0.0365, -0.0438, -0.0109,  0.0356,  0.0054,  0.0030,\n",
      "         -0.0327,  0.0043, -0.0391,  0.0311,  0.0505, -0.0134,  0.0480,  0.0275,\n",
      "         -0.0316,  0.0331,  0.0489,  0.0003,  0.0258, -0.0338, -0.0342,  0.0378,\n",
      "         -0.0160,  0.0085, -0.0328,  0.0299,  0.0136, -0.0342, -0.0019,  0.0018,\n",
      "          0.0315,  0.0365,  0.0120, -0.0032, -0.0304,  0.0376, -0.0405,  0.0459,\n",
      "          0.0250,  0.0062, -0.0418,  0.0093, -0.0040,  0.0005, -0.0004, -0.0198,\n",
      "         -0.0025, -0.0205, -0.0142,  0.0450,  0.0085,  0.0038, -0.0214,  0.0080,\n",
      "         -0.0160,  0.0422, -0.0261, -0.0218, -0.0398,  0.0493, -0.0425,  0.0359,\n",
      "         -0.0165, -0.0292,  0.0492, -0.0102, -0.0145, -0.0319, -0.0171,  0.0433,\n",
      "          0.0095, -0.0341,  0.0227,  0.0217, -0.0435,  0.0076,  0.0030,  0.0198,\n",
      "         -0.0464,  0.0211,  0.0082, -0.0113,  0.0217,  0.0315,  0.0150, -0.0120,\n",
      "          0.0379,  0.0169, -0.0165,  0.0076,  0.0095, -0.0266,  0.0328, -0.0106,\n",
      "          0.0493, -0.0341,  0.0049,  0.0076,  0.0366,  0.0388, -0.0370, -0.0279,\n",
      "          0.0282,  0.0053,  0.0442, -0.0429,  0.0029,  0.0066, -0.0170, -0.0352,\n",
      "         -0.0322, -0.0131, -0.0037,  0.0105,  0.0447,  0.0186, -0.0431,  0.0022,\n",
      "          0.0395,  0.0143, -0.0084, -0.0507,  0.0278, -0.0353,  0.0262, -0.0019,\n",
      "          0.0071,  0.0265, -0.0350, -0.0316,  0.0330, -0.0421,  0.0008,  0.0280,\n",
      "          0.0049, -0.0153, -0.0029, -0.0237, -0.0319,  0.0176,  0.0470, -0.0181,\n",
      "          0.0237,  0.0058,  0.0505,  0.0417, -0.0194,  0.0101, -0.0311, -0.0172,\n",
      "          0.0100,  0.0157, -0.0145,  0.0190,  0.0386, -0.0264,  0.0149,  0.0054,\n",
      "          0.0264, -0.0180, -0.0259,  0.0072, -0.0468,  0.0423, -0.0102, -0.0204,\n",
      "          0.0196,  0.0193, -0.0240,  0.0433,  0.0403, -0.0488,  0.0353,  0.0321,\n",
      "          0.0191,  0.0028,  0.0499,  0.0038, -0.0311,  0.0297,  0.0354,  0.0218,\n",
      "         -0.0291,  0.0301, -0.0192, -0.0254,  0.0507, -0.0288, -0.0337, -0.0101,\n",
      "          0.0390,  0.0369,  0.0436, -0.0092,  0.0508,  0.0267,  0.0372,  0.0360,\n",
      "         -0.0191, -0.0170, -0.0155, -0.0306,  0.0357,  0.0157, -0.0315, -0.0340,\n",
      "         -0.0125,  0.0031,  0.0497,  0.0313, -0.0011,  0.0451, -0.0261, -0.0327,\n",
      "          0.0172, -0.0323, -0.0013,  0.0157,  0.0222,  0.0005,  0.0027, -0.0052,\n",
      "          0.0390, -0.0435, -0.0053,  0.0406, -0.0397,  0.0377, -0.0034, -0.0410,\n",
      "          0.0172, -0.0055,  0.0398, -0.0386,  0.0003, -0.0207,  0.0298, -0.0165,\n",
      "          0.0369,  0.0388, -0.0075,  0.0428, -0.0278,  0.0423, -0.0435,  0.0176,\n",
      "         -0.0440, -0.0329, -0.0066, -0.0409,  0.0409, -0.0227, -0.0347,  0.0416,\n",
      "          0.0387,  0.0481,  0.0435, -0.0138,  0.0207,  0.0278,  0.0334, -0.0112,\n",
      "         -0.0075, -0.0190, -0.0296, -0.0215, -0.0208,  0.0170,  0.0382,  0.0103,\n",
      "          0.0315, -0.0488,  0.0421, -0.0189,  0.0068,  0.0099,  0.0247, -0.0188],\n",
      "        [-0.0468,  0.0073, -0.0492,  0.0144,  0.0033, -0.0065,  0.0018,  0.0424,\n",
      "         -0.0196, -0.0132,  0.0332, -0.0107,  0.0153, -0.0287,  0.0507,  0.0185,\n",
      "          0.0455,  0.0479, -0.0323,  0.0425, -0.0159,  0.0466,  0.0335, -0.0142,\n",
      "         -0.0112, -0.0103,  0.0143, -0.0215,  0.0323, -0.0133,  0.0284, -0.0281,\n",
      "         -0.0150, -0.0300,  0.0155, -0.0147,  0.0018, -0.0199,  0.0032, -0.0235,\n",
      "          0.0510,  0.0106, -0.0172, -0.0268,  0.0478, -0.0261, -0.0221,  0.0168,\n",
      "         -0.0402, -0.0037,  0.0165,  0.0191, -0.0505, -0.0410,  0.0454, -0.0425,\n",
      "          0.0435, -0.0365,  0.0055,  0.0417, -0.0440,  0.0455, -0.0270,  0.0089,\n",
      "          0.0427, -0.0340,  0.0327, -0.0266, -0.0122,  0.0436,  0.0448,  0.0178,\n",
      "          0.0139, -0.0427, -0.0022, -0.0115,  0.0495, -0.0057,  0.0194, -0.0397,\n",
      "          0.0046,  0.0210, -0.0289,  0.0263,  0.0270,  0.0233, -0.0350,  0.0356,\n",
      "         -0.0444, -0.0081, -0.0101, -0.0046, -0.0199,  0.0148, -0.0440,  0.0465,\n",
      "          0.0423,  0.0207,  0.0265,  0.0154, -0.0140, -0.0138,  0.0110,  0.0226,\n",
      "          0.0407,  0.0362, -0.0357, -0.0498,  0.0233,  0.0460,  0.0303,  0.0243,\n",
      "         -0.0284, -0.0215, -0.0007, -0.0050, -0.0109,  0.0288, -0.0019, -0.0409,\n",
      "         -0.0182,  0.0480, -0.0210, -0.0298, -0.0413, -0.0175, -0.0451, -0.0339,\n",
      "          0.0085, -0.0492, -0.0474, -0.0238,  0.0346,  0.0415, -0.0214, -0.0260,\n",
      "          0.0204,  0.0414,  0.0348, -0.0024,  0.0303, -0.0508, -0.0183, -0.0502,\n",
      "          0.0465,  0.0116, -0.0359, -0.0248, -0.0444,  0.0012,  0.0366,  0.0153,\n",
      "          0.0272, -0.0049, -0.0434, -0.0162, -0.0318,  0.0479, -0.0195,  0.0171,\n",
      "         -0.0436,  0.0073, -0.0442, -0.0276,  0.0453,  0.0027, -0.0141,  0.0182,\n",
      "          0.0237, -0.0501, -0.0055,  0.0349,  0.0064, -0.0400,  0.0245,  0.0116,\n",
      "         -0.0306, -0.0467, -0.0353,  0.0350, -0.0495, -0.0066,  0.0300, -0.0191,\n",
      "          0.0068, -0.0356, -0.0235, -0.0402, -0.0301,  0.0269, -0.0042, -0.0342,\n",
      "          0.0448,  0.0046, -0.0424,  0.0360, -0.0059, -0.0458,  0.0160, -0.0146,\n",
      "          0.0264, -0.0161, -0.0012,  0.0061,  0.0203, -0.0143,  0.0272, -0.0278,\n",
      "         -0.0220,  0.0131, -0.0130, -0.0483, -0.0480, -0.0179,  0.0098, -0.0091,\n",
      "         -0.0462, -0.0139,  0.0242,  0.0090, -0.0433, -0.0466, -0.0261,  0.0367,\n",
      "          0.0417, -0.0084, -0.0464, -0.0348, -0.0247,  0.0411,  0.0006, -0.0232,\n",
      "          0.0049, -0.0171, -0.0054,  0.0395, -0.0106,  0.0083, -0.0418,  0.0017,\n",
      "         -0.0498,  0.0420, -0.0084,  0.0386,  0.0138, -0.0138,  0.0337,  0.0474,\n",
      "         -0.0010,  0.0244, -0.0279, -0.0429,  0.0040, -0.0296,  0.0285,  0.0298,\n",
      "         -0.0317,  0.0144,  0.0093,  0.0456, -0.0238,  0.0462, -0.0189, -0.0384,\n",
      "         -0.0230, -0.0122, -0.0042,  0.0315, -0.0347, -0.0361,  0.0039,  0.0279,\n",
      "          0.0060,  0.0096,  0.0195,  0.0235,  0.0446,  0.0503, -0.0167, -0.0293,\n",
      "         -0.0125,  0.0163, -0.0182, -0.0404, -0.0371,  0.0484,  0.0160,  0.0395,\n",
      "          0.0352, -0.0068, -0.0096, -0.0197,  0.0384,  0.0026, -0.0003, -0.0073,\n",
      "          0.0451,  0.0382,  0.0445,  0.0123, -0.0163,  0.0329,  0.0370,  0.0086,\n",
      "         -0.0280, -0.0447, -0.0133,  0.0189,  0.0270, -0.0392, -0.0360, -0.0068,\n",
      "          0.0121, -0.0031, -0.0326, -0.0166,  0.0351, -0.0367,  0.0246,  0.0082,\n",
      "         -0.0115, -0.0239, -0.0299, -0.0510,  0.0326, -0.0370, -0.0273, -0.0040,\n",
      "          0.0077,  0.0270, -0.0079, -0.0268, -0.0509, -0.0123, -0.0465, -0.0328,\n",
      "         -0.0039, -0.0204, -0.0137, -0.0420, -0.0017, -0.0291,  0.0194, -0.0468,\n",
      "          0.0391, -0.0350, -0.0358,  0.0424, -0.0408,  0.0140, -0.0465, -0.0393,\n",
      "          0.0055,  0.0137,  0.0329,  0.0291, -0.0456,  0.0131, -0.0425, -0.0452,\n",
      "         -0.0134,  0.0409,  0.0102,  0.0468, -0.0209,  0.0381, -0.0311,  0.0327,\n",
      "          0.0437, -0.0153, -0.0507, -0.0266, -0.0076,  0.0263, -0.0031, -0.0349,\n",
      "          0.0454,  0.0128,  0.0100,  0.0141, -0.0268, -0.0148,  0.0440,  0.0189]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.bias | Size: torch.Size([2]) | Values : tensor([0.0437, 0.0328], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model_ConvLSTM = ConvLSTM().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ConvLSTM.parameters(), lr=learning_rate)\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=patience, verbose=True)\n",
    "print(f\"Model structure: {model_ConvLSTM}\\n\\n\")\n",
    "for name, param in model_ConvLSTM.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss: 1.997681  [   64/23274]\n",
      "loss: 0.134848  [ 6464/23274]\n",
      "loss: 0.249372  [12864/23274]\n",
      "loss: 0.306485  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.063369 \n",
      "\n",
      "Epoch 1:\n",
      "loss: 0.089597  [   64/23274]\n",
      "loss: 0.108071  [ 6464/23274]\n",
      "loss: 0.196678  [12864/23274]\n",
      "loss: 0.127416  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.061263 \n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.037670  [   64/23274]\n",
      "loss: 0.225822  [ 6464/23274]\n",
      "loss: 0.095868  [12864/23274]\n",
      "loss: 0.044325  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.054691 \n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.027882  [   64/23274]\n",
      "loss: 0.048998  [ 6464/23274]\n",
      "loss: 0.207568  [12864/23274]\n",
      "loss: 0.137094  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.052867 \n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.244401  [   64/23274]\n",
      "loss: 0.568347  [ 6464/23274]\n",
      "loss: 0.106899  [12864/23274]\n",
      "loss: 0.029141  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.049415 \n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.354956  [   64/23274]\n",
      "loss: 0.232809  [ 6464/23274]\n",
      "loss: 0.063304  [12864/23274]\n",
      "loss: 0.029293  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047309 \n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.039989  [   64/23274]\n",
      "loss: 0.190128  [ 6464/23274]\n",
      "loss: 0.072396  [12864/23274]\n",
      "loss: 0.036908  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.036796 \n",
      "\n",
      "Epoch 7:\n",
      "loss: 0.369074  [   64/23274]\n",
      "loss: 0.310904  [ 6464/23274]\n",
      "loss: 0.066683  [12864/23274]\n",
      "loss: 0.140016  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.032762 \n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.004836  [   64/23274]\n",
      "loss: 0.175589  [ 6464/23274]\n",
      "loss: 0.025980  [12864/23274]\n",
      "loss: 0.286112  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.036734 \n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.022780  [   64/23274]\n",
      "loss: 0.039565  [ 6464/23274]\n",
      "loss: 0.030250  [12864/23274]\n",
      "loss: 0.100581  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.041602 \n",
      "\n",
      "Epoch 10:\n",
      "loss: 0.086005  [   64/23274]\n",
      "loss: 0.027169  [ 6464/23274]\n",
      "loss: 0.021122  [12864/23274]\n",
      "loss: 0.048984  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.037851 \n",
      "\n",
      "Epoch 11:\n",
      "loss: 0.064365  [   64/23274]\n",
      "loss: 0.079476  [ 6464/23274]\n",
      "loss: 0.057514  [12864/23274]\n",
      "loss: 0.161579  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.030555 \n",
      "\n",
      "Epoch 12:\n",
      "loss: 0.044601  [   64/23274]\n",
      "loss: 0.046950  [ 6464/23274]\n",
      "loss: 0.027779  [12864/23274]\n",
      "loss: 0.003876  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.028916 \n",
      "\n",
      "Epoch 13:\n",
      "loss: 0.022659  [   64/23274]\n",
      "loss: 0.022398  [ 6464/23274]\n",
      "loss: 0.108258  [12864/23274]\n",
      "loss: 0.010286  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026502 \n",
      "\n",
      "Epoch 14:\n",
      "loss: 0.053731  [   64/23274]\n",
      "loss: 0.136214  [ 6464/23274]\n",
      "loss: 0.025631  [12864/23274]\n",
      "loss: 0.179738  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028297 \n",
      "\n",
      "Epoch 15:\n",
      "loss: 0.000680  [   64/23274]\n",
      "loss: 0.032916  [ 6464/23274]\n",
      "loss: 0.015247  [12864/23274]\n",
      "loss: 0.030203  [19264/23274]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.034175 \n",
      "\n",
      "Epoch 16:\n",
      "loss: 0.034582  [   64/23274]\n",
      "loss: 0.003466  [ 6464/23274]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\MLonMCU\\Fall_Detection\\train_Kfall.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/MLonMCU/Fall_Detection/train_Kfall.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_dataloader, model_ConvLSTM, loss_fn, optimizer,val_dataloader, \n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/MLonMCU/Fall_Detection/train_Kfall.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m            patience\u001b[39m=\u001b[39;49mpatience, scheduler\u001b[39m=\u001b[39;49mscheduler, epochs\u001b[39m=\u001b[39;49mepochs, device\u001b[39m=\u001b[39;49mdevice, B_size\u001b[39m=\u001b[39;49mB_size, A_size\u001b[39m=\u001b[39;49mA_size)\n",
      "File \u001b[1;32mg:\\MLonMCU\\Fall_Detection\\utils.py:57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, val_dataloader, patience, scheduler, device, epochs, B_size, A_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m pred \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     55\u001b[0m loss \u001b[39m=\u001b[39m (loss_fn(pred, y) \u001b[39m*\u001b[39m multipliers)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 57\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     58\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     59\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mg:\\python\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mg:\\python\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model_ConvLSTM, loss_fn, optimizer,val_dataloader, \n",
    "           patience=patience, scheduler=scheduler, epochs=epochs, device=device, B_size=B_size, A_size=A_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.817247 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.516\n",
      " specificity\t0.031\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.516\n",
      " specificity\t1.000\n",
      " sensitivity\t0.031\n",
      "[[32  0]\n",
      " [31  1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final test\n",
    "test(test_dataloader, model_ConvLSTM, loss_fn, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "!node->kind().is_aten() && !node->kind().is_prim() && !node->kind().is_attr() INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\serialization\\\\export.cpp\":896, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\MLonMCU\\Fall_Detection\\train_Kfall.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MLonMCU/Fall_Detection/train_Kfall.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m res \u001b[39m=\u001b[39m model_ConvLSTM_int8(input_fp32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/MLonMCU/Fall_Detection/train_Kfall.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m50\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/MLonMCU/Fall_Detection/train_Kfall.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model_ConvLSTM_int8, dummy_input, \u001b[39m\"\u001b[39;49m\u001b[39mConvLSTM_int8.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mg:\\python\\lib\\site-packages\\torch\\onnx\\utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     _export(\n\u001b[0;32m    507\u001b[0m         model,\n\u001b[0;32m    508\u001b[0m         args,\n\u001b[0;32m    509\u001b[0m         f,\n\u001b[0;32m    510\u001b[0m         export_params,\n\u001b[0;32m    511\u001b[0m         verbose,\n\u001b[0;32m    512\u001b[0m         training,\n\u001b[0;32m    513\u001b[0m         input_names,\n\u001b[0;32m    514\u001b[0m         output_names,\n\u001b[0;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    522\u001b[0m     )\n",
      "File \u001b[1;32mg:\\python\\lib\\site-packages\\torch\\onnx\\utils.py:1587\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[0;32m   1580\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_assign_scoped_names_for_node_and_value(graph)\n\u001b[0;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m export_params:\n\u001b[0;32m   1582\u001b[0m     (\n\u001b[0;32m   1583\u001b[0m         proto,\n\u001b[0;32m   1584\u001b[0m         export_map,\n\u001b[0;32m   1585\u001b[0m         val_use_external_data_format,\n\u001b[0;32m   1586\u001b[0m         node_names,\n\u001b[1;32m-> 1587\u001b[0m     ) \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49m_export_onnx(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1588\u001b[0m         params_dict,\n\u001b[0;32m   1589\u001b[0m         opset_version,\n\u001b[0;32m   1590\u001b[0m         dynamic_axes,\n\u001b[0;32m   1591\u001b[0m         defer_weight_export,\n\u001b[0;32m   1592\u001b[0m         operator_export_type,\n\u001b[0;32m   1593\u001b[0m         \u001b[39mnot\u001b[39;49;00m verbose,\n\u001b[0;32m   1594\u001b[0m         val_keep_init_as_ip,\n\u001b[0;32m   1595\u001b[0m         custom_opsets,\n\u001b[0;32m   1596\u001b[0m         val_add_node_names,\n\u001b[0;32m   1597\u001b[0m         model_file_location,\n\u001b[0;32m   1598\u001b[0m         node_attr_to_name,\n\u001b[0;32m   1599\u001b[0m     )\n\u001b[0;32m   1600\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1601\u001b[0m     (\n\u001b[0;32m   1602\u001b[0m         proto,\n\u001b[0;32m   1603\u001b[0m         export_map,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1617\u001b[0m         node_attr_to_name,\n\u001b[0;32m   1618\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: !node->kind().is_aten() && !node->kind().is_prim() && !node->kind().is_attr() INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\serialization\\\\export.cpp\":896, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "torch.backends.quantized.engine = 'x86'\n",
    "model_ConvLSTM.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "model_ConvLSTM_fused = torch.ao.quantization.fuse_modules(model_ConvLSTM, [['conv1.0', 'conv1.1', 'conv1.2'], ['conv2.0', 'conv2.1', 'conv2.2'], ['conv3.0', 'conv3.1', 'conv3.2']])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_ConvLSTM_prepared = torch.ao.quantization.prepare(model_ConvLSTM_fused)\n",
    "# model_ConvLSTM_prepared = torch.ao.quantization.prepare(model_ConvLSTM)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(1, 9, 50, dtype=torch.float32).to(device)\n",
    "model_ConvLSTM_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_ConvLSTM_int8 = torch.ao.quantization.convert(model_ConvLSTM_prepared)\n",
    "#model_ConvLSTM_int8 = torch.jit.script(model_ConvLSTM_int8)\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_ConvLSTM_int8(input_fp32)\n",
    "\n",
    "dummy_input = torch.randn(1, 9, 50)\n",
    "torch.onnx.export(model_ConvLSTM_int8, dummy_input, \"ConvLSTM_int8.onnx\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.convr = nn.Conv1d(in_channels=in_channels, out_channels=64, kernel_size=1)\n",
    "    def forward(self, input):\n",
    "        residual = self.convr(input)\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x += residual\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "        \n",
    "        residual = input\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x += residual\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "\n",
    "class ResNet24(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet24, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.convblk1 = ConvBlock(64)\n",
    "        self.convblk2 = ConvBlock(64)\n",
    "        self.identityblk1 = IdentityBlock(64)\n",
    "        self.convblk3 = ConvBlock(64)\n",
    "        self.identityblk2 = IdentityBlock(64)\n",
    "        self.convblk4 = ConvBlock(64)\n",
    "        self.identityblk3 = IdentityBlock(64)\n",
    "        \n",
    "        self.pool2 = nn.AvgPool1d(2)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(in_features=704, out_features=2),\n",
    "        #                        nn.Softmax()\n",
    "                                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblk1(x)\n",
    "        x = self.convblk2(x)\n",
    "        x = self.identityblk1(x)\n",
    "        x = self.convblk3(x)\n",
    "        x = self.identityblk2(x)\n",
    "        x = self.convblk4(x)\n",
    "        x = self.identityblk3(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: ResNet24(\n",
      "  (conv1): Conv1d(9, 64, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convblk1): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (convblk2): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (identityblk1): IdentityBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (convblk3): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (identityblk2): IdentityBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (convblk4): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (identityblk3): IdentityBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (pool2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=704, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[-0.0962, -0.1203,  0.0088],\n",
      "         [ 0.0250, -0.0384, -0.0394],\n",
      "         [ 0.1803,  0.0902,  0.0500],\n",
      "         [ 0.0524, -0.0922,  0.0237],\n",
      "         [-0.1035, -0.0279,  0.1900],\n",
      "         [ 0.1393,  0.1748,  0.1835],\n",
      "         [ 0.1739,  0.0814,  0.0565],\n",
      "         [ 0.1726,  0.0504,  0.0716],\n",
      "         [ 0.0109, -0.1210,  0.0899]],\n",
      "\n",
      "        [[ 0.0833, -0.1683,  0.0514],\n",
      "         [ 0.1794, -0.1675,  0.1431],\n",
      "         [ 0.1213,  0.0627,  0.1193],\n",
      "         [ 0.0012, -0.1739, -0.0930],\n",
      "         [-0.0146, -0.0357,  0.1010],\n",
      "         [ 0.0891, -0.1740, -0.1002],\n",
      "         [-0.0679, -0.1381,  0.1339],\n",
      "         [-0.1193, -0.0600,  0.1370],\n",
      "         [-0.0400,  0.0177, -0.1456]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.bias | Size: torch.Size([64]) | Values : tensor([ 0.0472, -0.1233], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.weight | Size: torch.Size([64, 64, 3]) | Values : tensor([[[-0.0266, -0.0182,  0.0011],\n",
      "         [ 0.0584, -0.0158,  0.0104],\n",
      "         [-0.0366, -0.0701, -0.0173],\n",
      "         [ 0.0182, -0.0064,  0.0562],\n",
      "         [ 0.0413, -0.0518, -0.0412],\n",
      "         [-0.0543,  0.0415, -0.0039],\n",
      "         [-0.0610, -0.0413,  0.0653],\n",
      "         [ 0.0242, -0.0328,  0.0075],\n",
      "         [-0.0671,  0.0072,  0.0509],\n",
      "         [-0.0171,  0.0137,  0.0490],\n",
      "         [ 0.0146, -0.0588,  0.0097],\n",
      "         [-0.0617, -0.0394, -0.0079],\n",
      "         [ 0.0621,  0.0161,  0.0344],\n",
      "         [-0.0623, -0.0377, -0.0029],\n",
      "         [ 0.0673,  0.0021,  0.0344],\n",
      "         [-0.0186, -0.0326,  0.0331],\n",
      "         [ 0.0465,  0.0240,  0.0206],\n",
      "         [-0.0367, -0.0335, -0.0110],\n",
      "         [ 0.0425, -0.0454, -0.0271],\n",
      "         [-0.0070,  0.0493, -0.0399],\n",
      "         [ 0.0347,  0.0322,  0.0540],\n",
      "         [ 0.0089, -0.0422,  0.0325],\n",
      "         [ 0.0482,  0.0440, -0.0052],\n",
      "         [-0.0412,  0.0680, -0.0227],\n",
      "         [ 0.0091,  0.0569,  0.0367],\n",
      "         [ 0.0245,  0.0515,  0.0582],\n",
      "         [ 0.0143,  0.0630, -0.0647],\n",
      "         [ 0.0005, -0.0279,  0.0484],\n",
      "         [-0.0686,  0.0362,  0.0636],\n",
      "         [ 0.0407,  0.0558,  0.0646],\n",
      "         [-0.0335, -0.0067, -0.0150],\n",
      "         [ 0.0343, -0.0494, -0.0127],\n",
      "         [ 0.0355,  0.0383, -0.0050],\n",
      "         [-0.0114,  0.0233,  0.0251],\n",
      "         [-0.0137,  0.0604,  0.0109],\n",
      "         [ 0.0281,  0.0154,  0.0398],\n",
      "         [ 0.0524,  0.0620,  0.0424],\n",
      "         [-0.0012, -0.0264, -0.0471],\n",
      "         [-0.0354, -0.0481, -0.0457],\n",
      "         [-0.0565,  0.0652, -0.0520],\n",
      "         [-0.0178, -0.0559, -0.0550],\n",
      "         [-0.0586,  0.0153, -0.0218],\n",
      "         [-0.0503, -0.0210, -0.0375],\n",
      "         [-0.0238,  0.0447, -0.0714],\n",
      "         [ 0.0483, -0.0111,  0.0319],\n",
      "         [-0.0029, -0.0036, -0.0170],\n",
      "         [ 0.0358,  0.0458, -0.0579],\n",
      "         [-0.0573, -0.0102, -0.0542],\n",
      "         [-0.0135, -0.0320,  0.0443],\n",
      "         [ 0.0453,  0.0278, -0.0431],\n",
      "         [ 0.0126, -0.0623,  0.0100],\n",
      "         [-0.0181, -0.0374,  0.0074],\n",
      "         [-0.0233, -0.0060,  0.0281],\n",
      "         [ 0.0600, -0.0388, -0.0669],\n",
      "         [ 0.0521,  0.0234, -0.0229],\n",
      "         [ 0.0131, -0.0493, -0.0594],\n",
      "         [-0.0535, -0.0312,  0.0177],\n",
      "         [ 0.0696,  0.0480, -0.0685],\n",
      "         [ 0.0615, -0.0181,  0.0132],\n",
      "         [ 0.0717, -0.0054,  0.0558],\n",
      "         [-0.0249, -0.0061,  0.0305],\n",
      "         [ 0.0582, -0.0030, -0.0479],\n",
      "         [-0.0271, -0.0666, -0.0158],\n",
      "         [ 0.0245,  0.0650,  0.0544]],\n",
      "\n",
      "        [[ 0.0002, -0.0694,  0.0706],\n",
      "         [-0.0381, -0.0164,  0.0408],\n",
      "         [-0.0619, -0.0451, -0.0244],\n",
      "         [ 0.0259, -0.0683, -0.0275],\n",
      "         [-0.0172, -0.0421,  0.0164],\n",
      "         [-0.0712,  0.0173,  0.0208],\n",
      "         [-0.0080, -0.0467, -0.0260],\n",
      "         [ 0.0389,  0.0469,  0.0211],\n",
      "         [ 0.0289, -0.0629, -0.0496],\n",
      "         [-0.0051,  0.0292, -0.0087],\n",
      "         [-0.0479,  0.0014,  0.0070],\n",
      "         [-0.0356,  0.0247, -0.0637],\n",
      "         [-0.0233, -0.0370,  0.0159],\n",
      "         [ 0.0059,  0.0204, -0.0622],\n",
      "         [ 0.0523, -0.0232,  0.0437],\n",
      "         [-0.0303, -0.0341, -0.0658],\n",
      "         [ 0.0327,  0.0551, -0.0632],\n",
      "         [-0.0234,  0.0306,  0.0264],\n",
      "         [ 0.0303, -0.0524, -0.0663],\n",
      "         [-0.0619,  0.0030, -0.0581],\n",
      "         [-0.0271,  0.0461,  0.0351],\n",
      "         [ 0.0078,  0.0276, -0.0296],\n",
      "         [ 0.0081,  0.0357,  0.0479],\n",
      "         [-0.0527,  0.0135,  0.0592],\n",
      "         [ 0.0579, -0.0140, -0.0034],\n",
      "         [-0.0034,  0.0135,  0.0245],\n",
      "         [-0.0609,  0.0389, -0.0680],\n",
      "         [ 0.0260,  0.0242, -0.0119],\n",
      "         [ 0.0364, -0.0510,  0.0190],\n",
      "         [-0.0034, -0.0623,  0.0049],\n",
      "         [ 0.0512,  0.0487, -0.0247],\n",
      "         [-0.0490, -0.0503, -0.0089],\n",
      "         [-0.0136, -0.0459, -0.0685],\n",
      "         [ 0.0404, -0.0412, -0.0388],\n",
      "         [-0.0021,  0.0381, -0.0021],\n",
      "         [ 0.0402, -0.0114,  0.0170],\n",
      "         [ 0.0295, -0.0691, -0.0331],\n",
      "         [-0.0348, -0.0674,  0.0421],\n",
      "         [ 0.0444, -0.0694, -0.0344],\n",
      "         [-0.0638,  0.0355,  0.0066],\n",
      "         [-0.0094, -0.0478,  0.0536],\n",
      "         [ 0.0169,  0.0134,  0.0715],\n",
      "         [-0.0652,  0.0496, -0.0491],\n",
      "         [-0.0534,  0.0720,  0.0033],\n",
      "         [-0.0522,  0.0212,  0.0507],\n",
      "         [-0.0692, -0.0666, -0.0222],\n",
      "         [-0.0572,  0.0684, -0.0150],\n",
      "         [ 0.0173,  0.0298, -0.0302],\n",
      "         [ 0.0020,  0.0122, -0.0563],\n",
      "         [ 0.0143, -0.0602, -0.0269],\n",
      "         [-0.0003,  0.0259, -0.0482],\n",
      "         [-0.0122, -0.0028, -0.0615],\n",
      "         [-0.0252, -0.0521,  0.0270],\n",
      "         [-0.0551,  0.0519, -0.0530],\n",
      "         [ 0.0001,  0.0236, -0.0246],\n",
      "         [ 0.0143, -0.0549, -0.0634],\n",
      "         [-0.0172,  0.0433, -0.0565],\n",
      "         [ 0.0520,  0.0457,  0.0712],\n",
      "         [-0.0639, -0.0094, -0.0147],\n",
      "         [ 0.0575, -0.0180, -0.0342],\n",
      "         [-0.0458, -0.0538, -0.0013],\n",
      "         [-0.0421, -0.0487,  0.0073],\n",
      "         [ 0.0678, -0.0152, -0.0666],\n",
      "         [-0.0595,  0.0172, -0.0154]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.bias | Size: torch.Size([64]) | Values : tensor([-0.0201,  0.0348], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[ 0.0766],\n",
      "         [-0.0258],\n",
      "         [-0.0095],\n",
      "         [-0.0370],\n",
      "         [ 0.0159],\n",
      "         [-0.0896],\n",
      "         [ 0.1204],\n",
      "         [-0.0288],\n",
      "         [ 0.0882],\n",
      "         [-0.0833],\n",
      "         [ 0.0046],\n",
      "         [ 0.0392],\n",
      "         [ 0.1235],\n",
      "         [ 0.0083],\n",
      "         [ 0.0428],\n",
      "         [ 0.0748],\n",
      "         [-0.0436],\n",
      "         [ 0.0596],\n",
      "         [-0.0776],\n",
      "         [ 0.0363],\n",
      "         [-0.0696],\n",
      "         [-0.0113],\n",
      "         [-0.0903],\n",
      "         [-0.1015],\n",
      "         [ 0.0663],\n",
      "         [ 0.0845],\n",
      "         [-0.0847],\n",
      "         [-0.1039],\n",
      "         [ 0.1176],\n",
      "         [ 0.0013],\n",
      "         [ 0.0700],\n",
      "         [-0.1210],\n",
      "         [ 0.0384],\n",
      "         [-0.0225],\n",
      "         [ 0.0419],\n",
      "         [-0.0508],\n",
      "         [ 0.0710],\n",
      "         [ 0.0738],\n",
      "         [-0.0421],\n",
      "         [ 0.0783],\n",
      "         [ 0.0920],\n",
      "         [ 0.1217],\n",
      "         [ 0.0110],\n",
      "         [ 0.0836],\n",
      "         [ 0.0010],\n",
      "         [ 0.1198],\n",
      "         [ 0.0273],\n",
      "         [-0.0218],\n",
      "         [-0.0435],\n",
      "         [ 0.1227],\n",
      "         [-0.0755],\n",
      "         [-0.0090],\n",
      "         [ 0.0386],\n",
      "         [ 0.0219],\n",
      "         [-0.0880],\n",
      "         [ 0.0158],\n",
      "         [-0.0167],\n",
      "         [ 0.0769],\n",
      "         [-0.0280],\n",
      "         [ 0.0738],\n",
      "         [-0.0580],\n",
      "         [ 0.0738],\n",
      "         [-0.0342],\n",
      "         [ 0.0517]],\n",
      "\n",
      "        [[-0.0823],\n",
      "         [ 0.0495],\n",
      "         [-0.1187],\n",
      "         [-0.0561],\n",
      "         [-0.0797],\n",
      "         [ 0.0536],\n",
      "         [-0.0426],\n",
      "         [-0.0651],\n",
      "         [-0.0582],\n",
      "         [ 0.0335],\n",
      "         [ 0.0315],\n",
      "         [-0.0775],\n",
      "         [ 0.0484],\n",
      "         [ 0.0219],\n",
      "         [-0.0577],\n",
      "         [ 0.0693],\n",
      "         [ 0.1131],\n",
      "         [ 0.1125],\n",
      "         [-0.0746],\n",
      "         [-0.0786],\n",
      "         [-0.0702],\n",
      "         [-0.0763],\n",
      "         [-0.1080],\n",
      "         [ 0.0529],\n",
      "         [ 0.1122],\n",
      "         [ 0.0825],\n",
      "         [-0.0515],\n",
      "         [-0.0996],\n",
      "         [ 0.0960],\n",
      "         [-0.0342],\n",
      "         [-0.0405],\n",
      "         [ 0.0113],\n",
      "         [ 0.0236],\n",
      "         [-0.0053],\n",
      "         [ 0.1134],\n",
      "         [-0.0178],\n",
      "         [ 0.0576],\n",
      "         [ 0.0456],\n",
      "         [ 0.1028],\n",
      "         [-0.1033],\n",
      "         [-0.1034],\n",
      "         [ 0.0051],\n",
      "         [ 0.0421],\n",
      "         [-0.1064],\n",
      "         [ 0.0955],\n",
      "         [-0.0346],\n",
      "         [-0.0299],\n",
      "         [-0.0067],\n",
      "         [-0.0931],\n",
      "         [ 0.0332],\n",
      "         [-0.0995],\n",
      "         [ 0.0007],\n",
      "         [ 0.0158],\n",
      "         [-0.0592],\n",
      "         [ 0.1205],\n",
      "         [ 0.0820],\n",
      "         [-0.0894],\n",
      "         [-0.1069],\n",
      "         [ 0.0775],\n",
      "         [ 0.0847],\n",
      "         [-0.0984],\n",
      "         [-0.0262],\n",
      "         [ 0.1057],\n",
      "         [-0.0091]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0327, -0.0925], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 4.6193e-02, -6.0031e-02,  2.6745e-02],\n",
      "         [ 1.1378e-01, -1.2137e-01, -4.9424e-02],\n",
      "         [-2.0874e-03, -1.0791e-01, -1.2891e-01],\n",
      "         [ 5.5932e-02, -4.9197e-02, -8.1334e-02],\n",
      "         [-1.8302e-03,  4.2814e-02,  7.5213e-02],\n",
      "         [-1.6235e-02,  6.4801e-02, -4.2451e-02],\n",
      "         [ 7.2968e-03, -7.0268e-02, -8.7228e-02],\n",
      "         [-8.6728e-02, -1.2059e-01,  2.4983e-02],\n",
      "         [ 1.2893e-01,  9.0049e-02, -2.3947e-02],\n",
      "         [-7.5854e-02, -9.9093e-02, -1.5576e-02],\n",
      "         [-6.2798e-02,  9.9659e-02, -1.3090e-01],\n",
      "         [ 7.9977e-02,  8.4689e-02,  2.0211e-03],\n",
      "         [ 8.4265e-03,  4.9329e-02,  9.7879e-02],\n",
      "         [ 5.9961e-02, -9.8693e-03, -1.1333e-01],\n",
      "         [-1.3765e-01,  8.2221e-02, -5.8628e-02],\n",
      "         [-1.3995e-01, -9.7225e-02, -1.0094e-02]],\n",
      "\n",
      "        [[-1.8429e-02, -2.9048e-03, -5.9691e-02],\n",
      "         [-6.3223e-02, -3.2996e-02,  8.4703e-02],\n",
      "         [-8.9062e-02, -1.1794e-01, -2.4169e-02],\n",
      "         [-4.3030e-02,  2.5784e-02,  2.9033e-02],\n",
      "         [ 2.9097e-02,  7.5944e-02,  6.2771e-02],\n",
      "         [ 7.8534e-02, -5.7882e-03, -1.4127e-01],\n",
      "         [-8.6788e-02,  6.0976e-05,  5.6620e-02],\n",
      "         [ 5.7280e-02, -1.2665e-01, -2.0026e-02],\n",
      "         [-8.6513e-03,  1.0956e-01, -4.1849e-02],\n",
      "         [-1.3520e-01, -1.2676e-01,  1.3990e-01],\n",
      "         [-4.7713e-02, -8.3048e-02, -6.2863e-02],\n",
      "         [ 9.9121e-02, -1.1669e-01,  6.4251e-03],\n",
      "         [ 3.2499e-02,  6.0602e-02,  4.6324e-02],\n",
      "         [ 7.9480e-02,  8.6284e-02,  9.4282e-02],\n",
      "         [-1.6018e-02,  1.0932e-01, -1.3837e-01],\n",
      "         [-1.0534e-02, -8.9782e-02,  7.7419e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv2.bias | Size: torch.Size([16]) | Values : tensor([ 0.0657, -0.1067], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.1328],\n",
      "         [ 0.1964],\n",
      "         [ 0.1370],\n",
      "         [-0.2040],\n",
      "         [ 0.0548],\n",
      "         [ 0.2259],\n",
      "         [ 0.0267],\n",
      "         [-0.2097],\n",
      "         [-0.2271],\n",
      "         [-0.1468],\n",
      "         [-0.2410],\n",
      "         [ 0.1034],\n",
      "         [ 0.0808],\n",
      "         [ 0.0343],\n",
      "         [ 0.0100],\n",
      "         [-0.0123]],\n",
      "\n",
      "        [[ 0.1607],\n",
      "         [-0.0408],\n",
      "         [ 0.1913],\n",
      "         [-0.1766],\n",
      "         [-0.2404],\n",
      "         [-0.1720],\n",
      "         [ 0.1446],\n",
      "         [ 0.0398],\n",
      "         [ 0.2338],\n",
      "         [ 0.0933],\n",
      "         [-0.0944],\n",
      "         [-0.0843],\n",
      "         [ 0.1105],\n",
      "         [-0.2245],\n",
      "         [-0.0389],\n",
      "         [ 0.0986]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.1989, -0.1399], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 1.1940e-01],\n",
      "         [-3.6673e-02],\n",
      "         [ 1.5392e-02],\n",
      "         [ 8.4104e-02],\n",
      "         [ 1.0973e-01],\n",
      "         [ 5.7571e-02],\n",
      "         [ 6.1722e-03],\n",
      "         [ 3.9591e-02],\n",
      "         [ 1.0038e-01],\n",
      "         [ 1.2259e-02],\n",
      "         [ 4.8351e-02],\n",
      "         [-3.6660e-02],\n",
      "         [ 5.9406e-02],\n",
      "         [ 1.4383e-02],\n",
      "         [ 2.9797e-02],\n",
      "         [ 4.1620e-03],\n",
      "         [ 5.5130e-02],\n",
      "         [ 1.2061e-01],\n",
      "         [ 1.9121e-02],\n",
      "         [-8.8730e-02],\n",
      "         [-1.1162e-01],\n",
      "         [-1.0327e-01],\n",
      "         [-2.0320e-02],\n",
      "         [ 1.0565e-01],\n",
      "         [ 5.2698e-02],\n",
      "         [-9.3002e-02],\n",
      "         [ 1.3870e-02],\n",
      "         [-8.4223e-03],\n",
      "         [-5.0460e-02],\n",
      "         [ 1.0307e-01],\n",
      "         [-1.2431e-01],\n",
      "         [ 2.8262e-02],\n",
      "         [-1.3750e-03],\n",
      "         [ 1.7348e-02],\n",
      "         [ 4.8915e-02],\n",
      "         [-6.7175e-03],\n",
      "         [ 8.3884e-02],\n",
      "         [-4.1731e-02],\n",
      "         [-9.6643e-03],\n",
      "         [-5.8164e-02],\n",
      "         [-3.1675e-02],\n",
      "         [-7.6813e-02],\n",
      "         [-8.5320e-02],\n",
      "         [ 5.5056e-02],\n",
      "         [-7.5198e-03],\n",
      "         [-7.4881e-02],\n",
      "         [ 5.1906e-02],\n",
      "         [-8.8711e-02],\n",
      "         [ 6.4485e-02],\n",
      "         [-6.8149e-02],\n",
      "         [ 7.4624e-03],\n",
      "         [-3.9307e-02],\n",
      "         [-4.0663e-02],\n",
      "         [ 6.7685e-03],\n",
      "         [-1.0947e-01],\n",
      "         [ 1.1958e-01],\n",
      "         [-1.1465e-01],\n",
      "         [-5.3638e-02],\n",
      "         [ 7.5653e-03],\n",
      "         [-6.6332e-02],\n",
      "         [ 9.2060e-03],\n",
      "         [-1.1397e-01],\n",
      "         [-9.5263e-05],\n",
      "         [ 5.8892e-03]],\n",
      "\n",
      "        [[-1.2487e-01],\n",
      "         [-9.2319e-03],\n",
      "         [-7.4239e-02],\n",
      "         [ 3.3831e-02],\n",
      "         [ 1.4443e-02],\n",
      "         [-2.5095e-02],\n",
      "         [-5.5231e-02],\n",
      "         [ 4.5126e-02],\n",
      "         [ 8.0637e-02],\n",
      "         [ 5.3508e-02],\n",
      "         [ 5.6659e-02],\n",
      "         [ 4.3220e-02],\n",
      "         [ 8.9120e-02],\n",
      "         [ 4.9666e-02],\n",
      "         [ 5.0709e-02],\n",
      "         [-5.5696e-02],\n",
      "         [ 7.3528e-02],\n",
      "         [-1.1144e-01],\n",
      "         [ 8.9680e-02],\n",
      "         [ 4.3112e-02],\n",
      "         [-1.1432e-01],\n",
      "         [ 1.8829e-02],\n",
      "         [ 1.9764e-03],\n",
      "         [ 6.7541e-02],\n",
      "         [ 9.5859e-02],\n",
      "         [ 1.2116e-01],\n",
      "         [ 1.1073e-02],\n",
      "         [ 5.9633e-02],\n",
      "         [-2.5261e-02],\n",
      "         [-9.4010e-02],\n",
      "         [ 1.1707e-01],\n",
      "         [ 1.9102e-02],\n",
      "         [-5.7474e-02],\n",
      "         [ 5.5104e-02],\n",
      "         [ 8.4533e-02],\n",
      "         [-9.1544e-02],\n",
      "         [-1.2470e-01],\n",
      "         [-5.5547e-02],\n",
      "         [ 8.0132e-02],\n",
      "         [ 1.9552e-02],\n",
      "         [ 7.2657e-02],\n",
      "         [ 1.4553e-02],\n",
      "         [ 1.0255e-01],\n",
      "         [ 1.2077e-02],\n",
      "         [-8.5392e-03],\n",
      "         [ 1.0028e-01],\n",
      "         [-7.7108e-02],\n",
      "         [ 5.6482e-02],\n",
      "         [ 5.9982e-02],\n",
      "         [-6.3415e-02],\n",
      "         [ 1.0453e-01],\n",
      "         [ 2.0693e-02],\n",
      "         [-1.0379e-01],\n",
      "         [-5.1848e-02],\n",
      "         [-1.2187e-02],\n",
      "         [-9.7792e-02],\n",
      "         [-7.0603e-02],\n",
      "         [-9.1842e-02],\n",
      "         [ 8.4602e-02],\n",
      "         [ 6.7320e-02],\n",
      "         [-1.0016e-02],\n",
      "         [ 1.4302e-02],\n",
      "         [ 5.9601e-02],\n",
      "         [-6.2260e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0874,  0.0520], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.1235],\n",
      "         [ 0.0455],\n",
      "         [ 0.0779],\n",
      "         [-0.0025],\n",
      "         [ 0.0694],\n",
      "         [-0.0359],\n",
      "         [-0.1032],\n",
      "         [ 0.0315],\n",
      "         [-0.1163],\n",
      "         [-0.0673],\n",
      "         [-0.1137],\n",
      "         [-0.1151],\n",
      "         [-0.0639],\n",
      "         [ 0.0869],\n",
      "         [ 0.0658],\n",
      "         [-0.0522],\n",
      "         [ 0.0369],\n",
      "         [-0.0294],\n",
      "         [-0.0468],\n",
      "         [-0.0930],\n",
      "         [-0.0693],\n",
      "         [-0.1200],\n",
      "         [-0.0299],\n",
      "         [-0.1221],\n",
      "         [-0.0818],\n",
      "         [-0.0949],\n",
      "         [-0.1118],\n",
      "         [-0.0448],\n",
      "         [ 0.0866],\n",
      "         [-0.0111],\n",
      "         [ 0.0775],\n",
      "         [ 0.0767],\n",
      "         [ 0.0413],\n",
      "         [ 0.1065],\n",
      "         [-0.0412],\n",
      "         [-0.0072],\n",
      "         [-0.0934],\n",
      "         [-0.0321],\n",
      "         [ 0.0937],\n",
      "         [-0.0898],\n",
      "         [-0.1211],\n",
      "         [ 0.0249],\n",
      "         [-0.0378],\n",
      "         [ 0.0530],\n",
      "         [ 0.0151],\n",
      "         [-0.0590],\n",
      "         [-0.0644],\n",
      "         [-0.0192],\n",
      "         [-0.0619],\n",
      "         [-0.1048],\n",
      "         [ 0.0287],\n",
      "         [ 0.0427],\n",
      "         [-0.0220],\n",
      "         [-0.1077],\n",
      "         [ 0.0899],\n",
      "         [ 0.0258],\n",
      "         [-0.0706],\n",
      "         [ 0.0325],\n",
      "         [-0.1112],\n",
      "         [-0.0636],\n",
      "         [-0.0990],\n",
      "         [ 0.0156],\n",
      "         [ 0.0299],\n",
      "         [ 0.0250]],\n",
      "\n",
      "        [[ 0.1222],\n",
      "         [ 0.0783],\n",
      "         [-0.0148],\n",
      "         [-0.0973],\n",
      "         [-0.0596],\n",
      "         [ 0.0180],\n",
      "         [ 0.0483],\n",
      "         [ 0.0917],\n",
      "         [ 0.0248],\n",
      "         [-0.0073],\n",
      "         [-0.1072],\n",
      "         [ 0.0232],\n",
      "         [-0.0061],\n",
      "         [-0.1041],\n",
      "         [ 0.0648],\n",
      "         [-0.0714],\n",
      "         [ 0.1195],\n",
      "         [-0.0998],\n",
      "         [-0.1131],\n",
      "         [ 0.1110],\n",
      "         [-0.0251],\n",
      "         [ 0.1013],\n",
      "         [-0.0543],\n",
      "         [-0.0866],\n",
      "         [ 0.0169],\n",
      "         [-0.1105],\n",
      "         [-0.0127],\n",
      "         [ 0.0268],\n",
      "         [-0.0503],\n",
      "         [ 0.0292],\n",
      "         [-0.0023],\n",
      "         [-0.0033],\n",
      "         [-0.0820],\n",
      "         [ 0.0526],\n",
      "         [-0.0506],\n",
      "         [ 0.0368],\n",
      "         [-0.0556],\n",
      "         [-0.0312],\n",
      "         [ 0.0409],\n",
      "         [-0.0255],\n",
      "         [ 0.0232],\n",
      "         [ 0.0133],\n",
      "         [ 0.0481],\n",
      "         [-0.1222],\n",
      "         [-0.1154],\n",
      "         [-0.0604],\n",
      "         [-0.0552],\n",
      "         [-0.0811],\n",
      "         [-0.0859],\n",
      "         [ 0.0129],\n",
      "         [-0.0008],\n",
      "         [ 0.0237],\n",
      "         [ 0.0765],\n",
      "         [ 0.0157],\n",
      "         [-0.0279],\n",
      "         [ 0.0313],\n",
      "         [ 0.0865],\n",
      "         [-0.1221],\n",
      "         [ 0.1142],\n",
      "         [ 0.0340],\n",
      "         [-0.0630],\n",
      "         [-0.1084],\n",
      "         [-0.0633],\n",
      "         [ 0.0297]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0089, -0.0767], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.0511,  0.0015, -0.1255],\n",
      "         [ 0.0635,  0.1024,  0.0486],\n",
      "         [ 0.0238, -0.1394, -0.0766],\n",
      "         [-0.0887, -0.0312, -0.0474],\n",
      "         [ 0.1347, -0.1257, -0.0540],\n",
      "         [-0.0933,  0.1369,  0.1387],\n",
      "         [-0.0441, -0.0842,  0.1414],\n",
      "         [-0.0813,  0.1022, -0.0903],\n",
      "         [ 0.0653, -0.0591,  0.0865],\n",
      "         [ 0.0647, -0.1050, -0.1128],\n",
      "         [-0.0998, -0.0515,  0.1347],\n",
      "         [ 0.1411, -0.1410, -0.0998],\n",
      "         [ 0.0486,  0.1030,  0.0503],\n",
      "         [-0.1025, -0.1175, -0.0958],\n",
      "         [-0.1013, -0.0067, -0.0034],\n",
      "         [-0.0214, -0.0604, -0.1167]],\n",
      "\n",
      "        [[-0.1045,  0.0063,  0.0510],\n",
      "         [ 0.1323, -0.1067,  0.1434],\n",
      "         [-0.0728,  0.1170, -0.0517],\n",
      "         [ 0.0017,  0.1420,  0.1298],\n",
      "         [ 0.0147, -0.1178,  0.1029],\n",
      "         [-0.1295,  0.0899,  0.0349],\n",
      "         [-0.0610, -0.0853, -0.0431],\n",
      "         [-0.1347,  0.0409,  0.0252],\n",
      "         [-0.0579, -0.1356,  0.0387],\n",
      "         [ 0.0755, -0.1236,  0.0878],\n",
      "         [-0.0790,  0.1410, -0.0734],\n",
      "         [ 0.0591,  0.0109,  0.0824],\n",
      "         [ 0.0687, -0.0189, -0.0097],\n",
      "         [ 0.0895,  0.0539, -0.1075],\n",
      "         [ 0.0072, -0.1361, -0.0780],\n",
      "         [ 0.0183, -0.0874, -0.1386]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv2.bias | Size: torch.Size([16]) | Values : tensor([0.1039, 0.0763], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.0139],\n",
      "         [-0.1052],\n",
      "         [ 0.1082],\n",
      "         [-0.2229],\n",
      "         [ 0.2379],\n",
      "         [ 0.0158],\n",
      "         [-0.0456],\n",
      "         [ 0.1959],\n",
      "         [ 0.2499],\n",
      "         [-0.1868],\n",
      "         [-0.1432],\n",
      "         [-0.0525],\n",
      "         [ 0.0775],\n",
      "         [-0.0826],\n",
      "         [ 0.1457],\n",
      "         [-0.0414]],\n",
      "\n",
      "        [[ 0.1166],\n",
      "         [-0.2326],\n",
      "         [-0.1986],\n",
      "         [-0.1150],\n",
      "         [-0.0077],\n",
      "         [-0.1025],\n",
      "         [-0.1124],\n",
      "         [-0.2075],\n",
      "         [ 0.1756],\n",
      "         [ 0.1871],\n",
      "         [-0.1566],\n",
      "         [-0.1607],\n",
      "         [-0.0052],\n",
      "         [-0.0181],\n",
      "         [-0.1627],\n",
      "         [ 0.0989]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.1788, -0.0845], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 0.0572],\n",
      "         [ 0.0396],\n",
      "         [-0.0978],\n",
      "         [ 0.1019],\n",
      "         [ 0.1066],\n",
      "         [ 0.0722],\n",
      "         [ 0.1100],\n",
      "         [ 0.1210],\n",
      "         [-0.0590],\n",
      "         [-0.0214],\n",
      "         [-0.0570],\n",
      "         [ 0.0698],\n",
      "         [-0.0197],\n",
      "         [ 0.0561],\n",
      "         [-0.1218],\n",
      "         [-0.0904],\n",
      "         [-0.0755],\n",
      "         [-0.0929],\n",
      "         [-0.0257],\n",
      "         [-0.0194],\n",
      "         [-0.0921],\n",
      "         [ 0.1066],\n",
      "         [-0.0750],\n",
      "         [-0.0813],\n",
      "         [ 0.0607],\n",
      "         [ 0.1203],\n",
      "         [ 0.0822],\n",
      "         [ 0.0156],\n",
      "         [-0.0860],\n",
      "         [-0.0351],\n",
      "         [-0.0957],\n",
      "         [-0.0212],\n",
      "         [ 0.0659],\n",
      "         [-0.0255],\n",
      "         [ 0.0292],\n",
      "         [-0.0324],\n",
      "         [-0.0091],\n",
      "         [-0.0587],\n",
      "         [ 0.0609],\n",
      "         [ 0.0714],\n",
      "         [-0.0369],\n",
      "         [ 0.0971],\n",
      "         [-0.0012],\n",
      "         [-0.0447],\n",
      "         [ 0.0316],\n",
      "         [-0.0026],\n",
      "         [-0.0477],\n",
      "         [ 0.0783],\n",
      "         [-0.0562],\n",
      "         [ 0.1211],\n",
      "         [-0.0082],\n",
      "         [-0.0472],\n",
      "         [ 0.0806],\n",
      "         [-0.0994],\n",
      "         [-0.0263],\n",
      "         [-0.0336],\n",
      "         [ 0.1035],\n",
      "         [ 0.1007],\n",
      "         [-0.0855],\n",
      "         [ 0.0896],\n",
      "         [ 0.0190],\n",
      "         [ 0.0418],\n",
      "         [ 0.0403],\n",
      "         [ 0.0798]],\n",
      "\n",
      "        [[ 0.1194],\n",
      "         [-0.0073],\n",
      "         [ 0.0973],\n",
      "         [-0.0104],\n",
      "         [ 0.0486],\n",
      "         [-0.0321],\n",
      "         [ 0.0323],\n",
      "         [-0.1161],\n",
      "         [ 0.0404],\n",
      "         [ 0.0566],\n",
      "         [ 0.1087],\n",
      "         [ 0.1087],\n",
      "         [-0.0888],\n",
      "         [ 0.0505],\n",
      "         [ 0.0315],\n",
      "         [-0.0686],\n",
      "         [-0.0164],\n",
      "         [ 0.1239],\n",
      "         [-0.0362],\n",
      "         [ 0.0162],\n",
      "         [ 0.0112],\n",
      "         [ 0.1018],\n",
      "         [ 0.0381],\n",
      "         [-0.0382],\n",
      "         [ 0.1083],\n",
      "         [ 0.0870],\n",
      "         [-0.1022],\n",
      "         [-0.0798],\n",
      "         [ 0.0491],\n",
      "         [ 0.0570],\n",
      "         [-0.1021],\n",
      "         [-0.0442],\n",
      "         [ 0.0309],\n",
      "         [-0.0931],\n",
      "         [-0.1188],\n",
      "         [-0.0788],\n",
      "         [-0.0137],\n",
      "         [-0.1150],\n",
      "         [-0.0651],\n",
      "         [ 0.0402],\n",
      "         [-0.0800],\n",
      "         [ 0.1080],\n",
      "         [-0.0386],\n",
      "         [ 0.0364],\n",
      "         [ 0.0434],\n",
      "         [-0.1094],\n",
      "         [ 0.0115],\n",
      "         [ 0.0766],\n",
      "         [ 0.0834],\n",
      "         [ 0.1224],\n",
      "         [-0.0050],\n",
      "         [ 0.0479],\n",
      "         [-0.0167],\n",
      "         [-0.0120],\n",
      "         [-0.0984],\n",
      "         [ 0.0773],\n",
      "         [ 0.1123],\n",
      "         [-0.1166],\n",
      "         [ 0.0459],\n",
      "         [-0.1242],\n",
      "         [ 0.0246],\n",
      "         [-0.1250],\n",
      "         [ 0.1064],\n",
      "         [-0.0707]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0430,  0.1009], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.0638],\n",
      "         [-0.0324],\n",
      "         [ 0.0150],\n",
      "         [ 0.0746],\n",
      "         [ 0.0290],\n",
      "         [ 0.0607],\n",
      "         [ 0.0713],\n",
      "         [-0.0633],\n",
      "         [-0.1161],\n",
      "         [-0.0993],\n",
      "         [-0.1200],\n",
      "         [-0.0008],\n",
      "         [ 0.1019],\n",
      "         [ 0.0061],\n",
      "         [ 0.0577],\n",
      "         [ 0.0106],\n",
      "         [-0.0902],\n",
      "         [ 0.0958],\n",
      "         [ 0.0459],\n",
      "         [ 0.0105],\n",
      "         [-0.1100],\n",
      "         [ 0.1001],\n",
      "         [ 0.0202],\n",
      "         [-0.1237],\n",
      "         [ 0.0488],\n",
      "         [-0.1218],\n",
      "         [-0.0457],\n",
      "         [-0.1129],\n",
      "         [ 0.0703],\n",
      "         [ 0.1063],\n",
      "         [-0.0643],\n",
      "         [ 0.0739],\n",
      "         [-0.0020],\n",
      "         [-0.0794],\n",
      "         [-0.1189],\n",
      "         [ 0.0235],\n",
      "         [-0.1211],\n",
      "         [-0.0190],\n",
      "         [ 0.0217],\n",
      "         [-0.0394],\n",
      "         [ 0.0242],\n",
      "         [ 0.0241],\n",
      "         [ 0.0546],\n",
      "         [-0.0658],\n",
      "         [-0.0277],\n",
      "         [ 0.1031],\n",
      "         [-0.1249],\n",
      "         [-0.0735],\n",
      "         [ 0.1107],\n",
      "         [-0.1188],\n",
      "         [-0.1191],\n",
      "         [ 0.0022],\n",
      "         [ 0.0604],\n",
      "         [-0.0044],\n",
      "         [-0.0255],\n",
      "         [-0.0382],\n",
      "         [ 0.0891],\n",
      "         [ 0.0872],\n",
      "         [-0.1143],\n",
      "         [ 0.1061],\n",
      "         [-0.0101],\n",
      "         [-0.0052],\n",
      "         [-0.0003],\n",
      "         [ 0.1158]],\n",
      "\n",
      "        [[-0.0727],\n",
      "         [ 0.1161],\n",
      "         [-0.1106],\n",
      "         [-0.1012],\n",
      "         [ 0.0229],\n",
      "         [-0.1058],\n",
      "         [ 0.0082],\n",
      "         [ 0.0157],\n",
      "         [-0.0428],\n",
      "         [-0.0892],\n",
      "         [-0.0069],\n",
      "         [ 0.0861],\n",
      "         [ 0.1240],\n",
      "         [ 0.1176],\n",
      "         [-0.0562],\n",
      "         [-0.0428],\n",
      "         [ 0.1094],\n",
      "         [ 0.1227],\n",
      "         [-0.0586],\n",
      "         [ 0.0406],\n",
      "         [ 0.0972],\n",
      "         [ 0.0588],\n",
      "         [-0.1223],\n",
      "         [-0.0438],\n",
      "         [ 0.1240],\n",
      "         [ 0.0061],\n",
      "         [ 0.0402],\n",
      "         [ 0.0415],\n",
      "         [-0.0234],\n",
      "         [ 0.0517],\n",
      "         [-0.0462],\n",
      "         [-0.0232],\n",
      "         [ 0.0445],\n",
      "         [-0.0775],\n",
      "         [-0.0950],\n",
      "         [ 0.0208],\n",
      "         [-0.0975],\n",
      "         [ 0.0807],\n",
      "         [ 0.0275],\n",
      "         [-0.0204],\n",
      "         [-0.0222],\n",
      "         [-0.0791],\n",
      "         [-0.0391],\n",
      "         [ 0.0277],\n",
      "         [ 0.1222],\n",
      "         [-0.0653],\n",
      "         [ 0.0779],\n",
      "         [-0.0628],\n",
      "         [ 0.0491],\n",
      "         [ 0.0256],\n",
      "         [-0.0518],\n",
      "         [ 0.0821],\n",
      "         [ 0.1226],\n",
      "         [ 0.0378],\n",
      "         [ 0.0065],\n",
      "         [-0.0486],\n",
      "         [ 0.0718],\n",
      "         [-0.0627],\n",
      "         [ 0.0799],\n",
      "         [-0.0221],\n",
      "         [ 0.0186],\n",
      "         [-0.0349],\n",
      "         [ 0.0275],\n",
      "         [ 0.0425]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv1.bias | Size: torch.Size([16]) | Values : tensor([ 0.1212, -0.0433], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.0621, -0.1097, -0.0352],\n",
      "         [ 0.0448, -0.1416, -0.1243],\n",
      "         [ 0.0016,  0.0500,  0.0396],\n",
      "         [-0.1353,  0.0344,  0.1431],\n",
      "         [-0.1397,  0.0937,  0.1079],\n",
      "         [ 0.0295,  0.0209, -0.1129],\n",
      "         [-0.0635, -0.0205, -0.0258],\n",
      "         [ 0.0442, -0.0981,  0.0957],\n",
      "         [-0.0553,  0.0038,  0.1152],\n",
      "         [ 0.0729, -0.0549,  0.0041],\n",
      "         [ 0.1379, -0.0562,  0.0681],\n",
      "         [ 0.0201,  0.0601, -0.0053],\n",
      "         [-0.0472, -0.0357,  0.1351],\n",
      "         [-0.1050,  0.1185,  0.0425],\n",
      "         [-0.0225, -0.1038,  0.1426],\n",
      "         [ 0.0488,  0.1000,  0.0328]],\n",
      "\n",
      "        [[ 0.0959,  0.0030,  0.0585],\n",
      "         [ 0.1093,  0.1240, -0.1337],\n",
      "         [-0.0245,  0.0024,  0.1179],\n",
      "         [-0.0463, -0.0174, -0.0182],\n",
      "         [-0.0174, -0.0822, -0.0356],\n",
      "         [ 0.0417, -0.0679, -0.1326],\n",
      "         [-0.0576,  0.0986,  0.0938],\n",
      "         [ 0.0834,  0.0982, -0.1162],\n",
      "         [ 0.1200,  0.0719,  0.0683],\n",
      "         [ 0.1431,  0.1061,  0.0790],\n",
      "         [ 0.0800, -0.0648, -0.0192],\n",
      "         [ 0.1205,  0.0281, -0.1115],\n",
      "         [ 0.0873, -0.0905, -0.0283],\n",
      "         [ 0.1011, -0.0028,  0.0311],\n",
      "         [ 0.0755,  0.0797, -0.0565],\n",
      "         [ 0.0219,  0.0812,  0.0267]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.1226, -0.0918], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.2096],\n",
      "         [ 0.1897],\n",
      "         [-0.1056],\n",
      "         [-0.2387],\n",
      "         [-0.1144],\n",
      "         [-0.0094],\n",
      "         [-0.2366],\n",
      "         [ 0.0183],\n",
      "         [ 0.0185],\n",
      "         [-0.2130],\n",
      "         [ 0.1896],\n",
      "         [-0.1117],\n",
      "         [ 0.0092],\n",
      "         [-0.2322],\n",
      "         [-0.0704],\n",
      "         [-0.0557]],\n",
      "\n",
      "        [[ 0.2026],\n",
      "         [ 0.1786],\n",
      "         [ 0.1410],\n",
      "         [ 0.1502],\n",
      "         [-0.2202],\n",
      "         [ 0.1874],\n",
      "         [-0.0634],\n",
      "         [-0.0814],\n",
      "         [ 0.1671],\n",
      "         [-0.1136],\n",
      "         [-0.0621],\n",
      "         [-0.0864],\n",
      "         [-0.2188],\n",
      "         [ 0.0941],\n",
      "         [-0.0182],\n",
      "         [-0.1914]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.0614, -0.1696], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk1.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.0414],\n",
      "         [ 0.0008],\n",
      "         [-0.1093],\n",
      "         [ 0.0217],\n",
      "         [ 0.0447],\n",
      "         [-0.0266],\n",
      "         [-0.1006],\n",
      "         [ 0.0037],\n",
      "         [ 0.0150],\n",
      "         [ 0.0683],\n",
      "         [-0.1118],\n",
      "         [-0.0930],\n",
      "         [-0.1067],\n",
      "         [-0.0901],\n",
      "         [ 0.1020],\n",
      "         [-0.0910],\n",
      "         [-0.1137],\n",
      "         [ 0.0875],\n",
      "         [-0.0716],\n",
      "         [ 0.0866],\n",
      "         [ 0.0249],\n",
      "         [ 0.0159],\n",
      "         [ 0.0371],\n",
      "         [-0.0112],\n",
      "         [ 0.0524],\n",
      "         [-0.0589],\n",
      "         [-0.1104],\n",
      "         [-0.0728],\n",
      "         [-0.0362],\n",
      "         [-0.0669],\n",
      "         [ 0.1007],\n",
      "         [ 0.0490],\n",
      "         [ 0.0530],\n",
      "         [ 0.1099],\n",
      "         [-0.1172],\n",
      "         [-0.0104],\n",
      "         [-0.0343],\n",
      "         [-0.0239],\n",
      "         [-0.0109],\n",
      "         [-0.0968],\n",
      "         [-0.1131],\n",
      "         [ 0.0236],\n",
      "         [-0.0364],\n",
      "         [-0.0212],\n",
      "         [ 0.0180],\n",
      "         [-0.0146],\n",
      "         [ 0.0922],\n",
      "         [ 0.1007],\n",
      "         [-0.1176],\n",
      "         [-0.0379],\n",
      "         [ 0.0854],\n",
      "         [ 0.0621],\n",
      "         [ 0.1044],\n",
      "         [ 0.0146],\n",
      "         [-0.0895],\n",
      "         [ 0.0232],\n",
      "         [-0.0086],\n",
      "         [ 0.0428],\n",
      "         [ 0.1061],\n",
      "         [ 0.1101],\n",
      "         [-0.0150],\n",
      "         [ 0.0093],\n",
      "         [-0.0851],\n",
      "         [ 0.0467]],\n",
      "\n",
      "        [[-0.0019],\n",
      "         [ 0.0394],\n",
      "         [ 0.0033],\n",
      "         [-0.0811],\n",
      "         [ 0.1031],\n",
      "         [ 0.1216],\n",
      "         [ 0.0889],\n",
      "         [ 0.0033],\n",
      "         [ 0.0950],\n",
      "         [ 0.0495],\n",
      "         [-0.0964],\n",
      "         [-0.0214],\n",
      "         [-0.0192],\n",
      "         [ 0.0856],\n",
      "         [-0.0346],\n",
      "         [-0.0326],\n",
      "         [-0.0667],\n",
      "         [-0.0705],\n",
      "         [ 0.0463],\n",
      "         [-0.0337],\n",
      "         [ 0.0766],\n",
      "         [-0.0891],\n",
      "         [-0.0739],\n",
      "         [ 0.0766],\n",
      "         [ 0.0806],\n",
      "         [ 0.0269],\n",
      "         [-0.0786],\n",
      "         [-0.0001],\n",
      "         [-0.0038],\n",
      "         [ 0.1036],\n",
      "         [-0.0709],\n",
      "         [-0.0555],\n",
      "         [-0.0789],\n",
      "         [-0.1188],\n",
      "         [-0.0580],\n",
      "         [ 0.0727],\n",
      "         [ 0.0710],\n",
      "         [ 0.0238],\n",
      "         [-0.0928],\n",
      "         [-0.1211],\n",
      "         [ 0.0339],\n",
      "         [-0.0788],\n",
      "         [-0.1219],\n",
      "         [ 0.0560],\n",
      "         [-0.0306],\n",
      "         [-0.1054],\n",
      "         [-0.0770],\n",
      "         [ 0.0244],\n",
      "         [ 0.1130],\n",
      "         [-0.0174],\n",
      "         [-0.0189],\n",
      "         [ 0.0353],\n",
      "         [-0.1090],\n",
      "         [ 0.0519],\n",
      "         [ 0.0323],\n",
      "         [ 0.0574],\n",
      "         [-0.0584],\n",
      "         [ 0.0645],\n",
      "         [-0.0337],\n",
      "         [-0.0238],\n",
      "         [ 0.0995],\n",
      "         [-0.0670],\n",
      "         [-0.1231],\n",
      "         [-0.0952]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv1.bias | Size: torch.Size([16]) | Values : tensor([ 0.0541, -0.0721], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[-0.0511,  0.1266,  0.0141],\n",
      "         [-0.0878, -0.0620, -0.1046],\n",
      "         [ 0.0449, -0.0743, -0.0425],\n",
      "         [ 0.1392,  0.0388, -0.0523],\n",
      "         [-0.1221,  0.0909, -0.0747],\n",
      "         [-0.0567,  0.0051,  0.1431],\n",
      "         [-0.0920,  0.0345,  0.1243],\n",
      "         [-0.1392, -0.0605, -0.0616],\n",
      "         [-0.1432, -0.0884, -0.1016],\n",
      "         [ 0.1181, -0.1083,  0.0202],\n",
      "         [ 0.1065,  0.0525,  0.0414],\n",
      "         [ 0.0640, -0.1238,  0.1108],\n",
      "         [ 0.0090,  0.0030,  0.0679],\n",
      "         [-0.0550,  0.1195,  0.0890],\n",
      "         [-0.0697,  0.0380,  0.0384],\n",
      "         [-0.0745,  0.0985,  0.0271]],\n",
      "\n",
      "        [[ 0.0529, -0.1206,  0.0338],\n",
      "         [ 0.1066,  0.0896,  0.1379],\n",
      "         [ 0.1416, -0.1330,  0.0324],\n",
      "         [ 0.1420,  0.1192,  0.0310],\n",
      "         [ 0.0115,  0.0978, -0.0180],\n",
      "         [-0.0875, -0.1092, -0.0517],\n",
      "         [ 0.1418, -0.0891,  0.0708],\n",
      "         [ 0.0672, -0.0746,  0.1041],\n",
      "         [ 0.1026, -0.0623, -0.0166],\n",
      "         [ 0.0214,  0.0452, -0.0539],\n",
      "         [ 0.0870,  0.1367, -0.0872],\n",
      "         [ 0.1174,  0.0783,  0.0027],\n",
      "         [-0.0254, -0.0308, -0.1051],\n",
      "         [ 0.0951,  0.1220, -0.0980],\n",
      "         [ 0.1254, -0.0339,  0.1327],\n",
      "         [ 0.0893,  0.0515,  0.0061]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv2.bias | Size: torch.Size([16]) | Values : tensor([ 0.0030, -0.0523], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[ 0.1999],\n",
      "         [ 0.1373],\n",
      "         [-0.0531],\n",
      "         [ 0.1694],\n",
      "         [ 0.0386],\n",
      "         [-0.1058],\n",
      "         [ 0.0553],\n",
      "         [-0.1640],\n",
      "         [-0.1912],\n",
      "         [ 0.0605],\n",
      "         [ 0.0046],\n",
      "         [-0.0873],\n",
      "         [ 0.1538],\n",
      "         [-0.1334],\n",
      "         [-0.1659],\n",
      "         [ 0.0198]],\n",
      "\n",
      "        [[ 0.1445],\n",
      "         [ 0.0995],\n",
      "         [ 0.1667],\n",
      "         [-0.0242],\n",
      "         [-0.1078],\n",
      "         [-0.0275],\n",
      "         [-0.1944],\n",
      "         [-0.0453],\n",
      "         [ 0.0258],\n",
      "         [-0.1796],\n",
      "         [ 0.1308],\n",
      "         [ 0.1122],\n",
      "         [ 0.1060],\n",
      "         [-0.2458],\n",
      "         [ 0.1561],\n",
      "         [-0.1209]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.1244, -0.0634], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 9.9084e-03],\n",
      "         [-7.1604e-02],\n",
      "         [ 7.8674e-02],\n",
      "         [ 4.1764e-02],\n",
      "         [-6.9734e-02],\n",
      "         [-3.2201e-02],\n",
      "         [ 9.3548e-02],\n",
      "         [-6.8916e-02],\n",
      "         [ 1.0003e-02],\n",
      "         [-9.5565e-02],\n",
      "         [ 7.8984e-02],\n",
      "         [ 9.8191e-02],\n",
      "         [ 7.5404e-02],\n",
      "         [ 3.8532e-02],\n",
      "         [-9.4146e-02],\n",
      "         [-9.1625e-02],\n",
      "         [ 5.6398e-03],\n",
      "         [-1.1864e-01],\n",
      "         [ 4.7860e-02],\n",
      "         [ 7.9109e-02],\n",
      "         [ 9.7196e-02],\n",
      "         [-4.9671e-02],\n",
      "         [ 5.7519e-02],\n",
      "         [ 1.0102e-01],\n",
      "         [ 2.0941e-02],\n",
      "         [ 5.3383e-02],\n",
      "         [-1.0093e-01],\n",
      "         [-4.9528e-02],\n",
      "         [ 6.2299e-02],\n",
      "         [ 3.2354e-02],\n",
      "         [-6.2817e-02],\n",
      "         [-1.0263e-01],\n",
      "         [-9.0897e-02],\n",
      "         [ 9.9882e-02],\n",
      "         [ 5.7858e-03],\n",
      "         [-2.8230e-02],\n",
      "         [-2.5271e-02],\n",
      "         [ 5.5123e-02],\n",
      "         [-6.9265e-04],\n",
      "         [-9.5772e-02],\n",
      "         [ 1.3085e-02],\n",
      "         [ 1.2483e-01],\n",
      "         [ 1.0641e-01],\n",
      "         [ 1.2211e-01],\n",
      "         [-8.2063e-02],\n",
      "         [-1.0961e-01],\n",
      "         [ 4.3212e-02],\n",
      "         [ 1.1044e-01],\n",
      "         [-1.2227e-01],\n",
      "         [ 2.8247e-02],\n",
      "         [-5.6393e-03],\n",
      "         [ 1.0041e-01],\n",
      "         [-3.9718e-02],\n",
      "         [-6.6844e-02],\n",
      "         [ 9.0866e-02],\n",
      "         [-2.7210e-03],\n",
      "         [-4.5273e-02],\n",
      "         [-5.1596e-02],\n",
      "         [ 1.1097e-01],\n",
      "         [-1.0278e-01],\n",
      "         [ 3.6040e-02],\n",
      "         [ 1.1739e-01],\n",
      "         [-1.1144e-01],\n",
      "         [-1.2001e-01]],\n",
      "\n",
      "        [[-8.3811e-02],\n",
      "         [ 9.1690e-02],\n",
      "         [-3.3592e-02],\n",
      "         [ 1.3957e-02],\n",
      "         [ 3.3270e-02],\n",
      "         [-8.4466e-02],\n",
      "         [-1.1853e-01],\n",
      "         [ 3.9411e-02],\n",
      "         [-6.6854e-02],\n",
      "         [-1.1717e-01],\n",
      "         [-9.6937e-02],\n",
      "         [ 7.8032e-03],\n",
      "         [ 7.0506e-02],\n",
      "         [-2.9606e-02],\n",
      "         [-3.4171e-02],\n",
      "         [-8.6863e-02],\n",
      "         [-2.4642e-03],\n",
      "         [ 4.0236e-02],\n",
      "         [ 4.0666e-02],\n",
      "         [ 4.0288e-02],\n",
      "         [ 8.1994e-02],\n",
      "         [-3.9498e-02],\n",
      "         [ 1.0367e-01],\n",
      "         [ 7.3008e-02],\n",
      "         [ 4.4999e-02],\n",
      "         [ 6.1269e-02],\n",
      "         [-1.1950e-01],\n",
      "         [-2.7905e-02],\n",
      "         [ 3.8821e-02],\n",
      "         [ 3.5822e-02],\n",
      "         [ 5.4308e-02],\n",
      "         [ 7.3363e-02],\n",
      "         [ 9.2214e-02],\n",
      "         [-1.5814e-02],\n",
      "         [-1.1475e-01],\n",
      "         [ 1.1832e-01],\n",
      "         [ 5.4272e-02],\n",
      "         [ 5.6072e-02],\n",
      "         [-9.8900e-02],\n",
      "         [-9.8863e-02],\n",
      "         [ 9.8909e-02],\n",
      "         [-4.0702e-02],\n",
      "         [-2.9380e-02],\n",
      "         [ 8.5455e-02],\n",
      "         [ 1.0469e-01],\n",
      "         [ 7.3239e-05],\n",
      "         [-3.9674e-03],\n",
      "         [-7.0332e-02],\n",
      "         [-1.2021e-01],\n",
      "         [ 1.4863e-02],\n",
      "         [ 3.9407e-03],\n",
      "         [-1.1666e-01],\n",
      "         [ 1.4534e-02],\n",
      "         [-8.2444e-02],\n",
      "         [ 7.6394e-02],\n",
      "         [ 9.6022e-02],\n",
      "         [ 3.9361e-02],\n",
      "         [ 3.1243e-02],\n",
      "         [ 4.0657e-02],\n",
      "         [ 2.9944e-02],\n",
      "         [-7.3916e-02],\n",
      "         [ 1.1913e-01],\n",
      "         [-4.7902e-02],\n",
      "         [-2.9164e-02]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0626, -0.0125], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.0591],\n",
      "         [ 0.1060],\n",
      "         [ 0.0816],\n",
      "         [ 0.0891],\n",
      "         [ 0.0817],\n",
      "         [ 0.0955],\n",
      "         [-0.0108],\n",
      "         [-0.0127],\n",
      "         [-0.0943],\n",
      "         [-0.0432],\n",
      "         [ 0.0122],\n",
      "         [-0.0723],\n",
      "         [ 0.0728],\n",
      "         [-0.0321],\n",
      "         [-0.0259],\n",
      "         [-0.1128],\n",
      "         [-0.0824],\n",
      "         [ 0.0624],\n",
      "         [-0.0876],\n",
      "         [ 0.0563],\n",
      "         [ 0.0943],\n",
      "         [-0.0875],\n",
      "         [ 0.0425],\n",
      "         [-0.0154],\n",
      "         [ 0.1113],\n",
      "         [-0.0178],\n",
      "         [ 0.0058],\n",
      "         [ 0.1136],\n",
      "         [-0.0651],\n",
      "         [ 0.1144],\n",
      "         [-0.0500],\n",
      "         [-0.0597],\n",
      "         [-0.0700],\n",
      "         [ 0.1072],\n",
      "         [-0.1005],\n",
      "         [-0.0803],\n",
      "         [-0.0064],\n",
      "         [ 0.1230],\n",
      "         [ 0.0847],\n",
      "         [-0.1042],\n",
      "         [ 0.0024],\n",
      "         [ 0.0893],\n",
      "         [ 0.1174],\n",
      "         [ 0.0472],\n",
      "         [-0.1240],\n",
      "         [ 0.0712],\n",
      "         [-0.1237],\n",
      "         [ 0.0868],\n",
      "         [-0.0560],\n",
      "         [-0.0662],\n",
      "         [ 0.0629],\n",
      "         [ 0.0313],\n",
      "         [ 0.0457],\n",
      "         [ 0.0072],\n",
      "         [ 0.0240],\n",
      "         [-0.0102],\n",
      "         [-0.0487],\n",
      "         [-0.0628],\n",
      "         [-0.0402],\n",
      "         [ 0.1210],\n",
      "         [-0.0400],\n",
      "         [-0.0398],\n",
      "         [-0.0307],\n",
      "         [ 0.0115]],\n",
      "\n",
      "        [[ 0.0339],\n",
      "         [-0.0863],\n",
      "         [ 0.0495],\n",
      "         [ 0.0613],\n",
      "         [ 0.1014],\n",
      "         [-0.1047],\n",
      "         [ 0.0410],\n",
      "         [ 0.0394],\n",
      "         [ 0.0796],\n",
      "         [ 0.1151],\n",
      "         [-0.0241],\n",
      "         [-0.1032],\n",
      "         [ 0.1242],\n",
      "         [-0.0133],\n",
      "         [-0.0979],\n",
      "         [-0.0175],\n",
      "         [ 0.0705],\n",
      "         [-0.1098],\n",
      "         [ 0.0629],\n",
      "         [-0.1083],\n",
      "         [ 0.0724],\n",
      "         [-0.0601],\n",
      "         [-0.0434],\n",
      "         [-0.1161],\n",
      "         [ 0.0976],\n",
      "         [ 0.0049],\n",
      "         [-0.0241],\n",
      "         [-0.1071],\n",
      "         [ 0.0923],\n",
      "         [-0.0269],\n",
      "         [ 0.0311],\n",
      "         [-0.0198],\n",
      "         [-0.0773],\n",
      "         [ 0.1103],\n",
      "         [-0.0278],\n",
      "         [-0.0877],\n",
      "         [ 0.0908],\n",
      "         [-0.0921],\n",
      "         [-0.1191],\n",
      "         [-0.0410],\n",
      "         [ 0.0156],\n",
      "         [ 0.0857],\n",
      "         [-0.0734],\n",
      "         [-0.0901],\n",
      "         [ 0.0530],\n",
      "         [ 0.0909],\n",
      "         [-0.0247],\n",
      "         [-0.0130],\n",
      "         [-0.0175],\n",
      "         [-0.0671],\n",
      "         [ 0.0589],\n",
      "         [-0.0060],\n",
      "         [-0.0230],\n",
      "         [ 0.1053],\n",
      "         [ 0.0503],\n",
      "         [-0.1170],\n",
      "         [-0.0532],\n",
      "         [-0.0966],\n",
      "         [ 0.0411],\n",
      "         [-0.1115],\n",
      "         [ 0.1232],\n",
      "         [ 0.0616],\n",
      "         [ 0.0284],\n",
      "         [-0.0463]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0169, -0.0892], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.0074, -0.0875,  0.0322],\n",
      "         [-0.0931, -0.0452, -0.0460],\n",
      "         [-0.1414,  0.0890,  0.0253],\n",
      "         [-0.0344, -0.1249, -0.1326],\n",
      "         [ 0.0212, -0.0537, -0.0395],\n",
      "         [-0.1278,  0.0586, -0.0791],\n",
      "         [ 0.0198, -0.0359, -0.1394],\n",
      "         [ 0.0496,  0.0121,  0.0823],\n",
      "         [ 0.0482, -0.1334,  0.0584],\n",
      "         [-0.0550,  0.1380,  0.1051],\n",
      "         [ 0.1422,  0.0090, -0.0510],\n",
      "         [ 0.0526, -0.0810, -0.1180],\n",
      "         [ 0.0145,  0.0397,  0.0198],\n",
      "         [-0.1027,  0.0317, -0.0891],\n",
      "         [-0.1283, -0.1188, -0.0815],\n",
      "         [ 0.0200,  0.0624, -0.1078]],\n",
      "\n",
      "        [[ 0.0910,  0.0562, -0.0270],\n",
      "         [ 0.0861, -0.0100,  0.0181],\n",
      "         [ 0.0138,  0.0018, -0.1259],\n",
      "         [-0.0889, -0.1028,  0.1300],\n",
      "         [ 0.0932,  0.0481, -0.0099],\n",
      "         [-0.0674, -0.0391, -0.0614],\n",
      "         [ 0.0409, -0.0494, -0.0053],\n",
      "         [ 0.0979,  0.1314, -0.1124],\n",
      "         [ 0.0291, -0.0058,  0.1023],\n",
      "         [ 0.1294, -0.0133, -0.1151],\n",
      "         [ 0.1427, -0.1357, -0.0088],\n",
      "         [ 0.0739,  0.1313,  0.0807],\n",
      "         [-0.0642, -0.0689,  0.0296],\n",
      "         [ 0.1304, -0.1037,  0.0006],\n",
      "         [-0.0141, -0.0085, -0.1275],\n",
      "         [-0.1223, -0.0431, -0.0367]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.0474, -0.0726], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[ 0.2264],\n",
      "         [ 0.0198],\n",
      "         [ 0.0165],\n",
      "         [ 0.0766],\n",
      "         [-0.1856],\n",
      "         [-0.1292],\n",
      "         [ 0.1836],\n",
      "         [ 0.2496],\n",
      "         [ 0.0408],\n",
      "         [ 0.0954],\n",
      "         [-0.0929],\n",
      "         [-0.2005],\n",
      "         [ 0.0897],\n",
      "         [-0.1334],\n",
      "         [ 0.2481],\n",
      "         [ 0.1429]],\n",
      "\n",
      "        [[ 0.0149],\n",
      "         [-0.0420],\n",
      "         [-0.0692],\n",
      "         [-0.1454],\n",
      "         [-0.2071],\n",
      "         [-0.1394],\n",
      "         [ 0.0054],\n",
      "         [ 0.0703],\n",
      "         [ 0.1868],\n",
      "         [-0.2115],\n",
      "         [-0.0444],\n",
      "         [-0.2280],\n",
      "         [ 0.2234],\n",
      "         [ 0.0010],\n",
      "         [ 0.1650],\n",
      "         [-0.1449]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.conv3.bias | Size: torch.Size([64]) | Values : tensor([0.1282, 0.1978], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk2.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[ 0.0604],\n",
      "         [-0.0700],\n",
      "         [-0.0528],\n",
      "         [-0.0932],\n",
      "         [-0.1073],\n",
      "         [-0.0091],\n",
      "         [ 0.0158],\n",
      "         [ 0.0384],\n",
      "         [-0.1116],\n",
      "         [-0.1028],\n",
      "         [ 0.0478],\n",
      "         [-0.0674],\n",
      "         [-0.0061],\n",
      "         [-0.0263],\n",
      "         [-0.1206],\n",
      "         [-0.0688],\n",
      "         [ 0.1206],\n",
      "         [ 0.0486],\n",
      "         [-0.0157],\n",
      "         [ 0.0722],\n",
      "         [-0.0450],\n",
      "         [-0.0707],\n",
      "         [-0.0857],\n",
      "         [-0.0163],\n",
      "         [ 0.0429],\n",
      "         [-0.0766],\n",
      "         [-0.0137],\n",
      "         [-0.0088],\n",
      "         [-0.1067],\n",
      "         [-0.0841],\n",
      "         [-0.0002],\n",
      "         [ 0.0963],\n",
      "         [ 0.0175],\n",
      "         [ 0.1153],\n",
      "         [-0.1168],\n",
      "         [-0.0144],\n",
      "         [-0.0266],\n",
      "         [-0.1175],\n",
      "         [ 0.0560],\n",
      "         [-0.0090],\n",
      "         [ 0.1114],\n",
      "         [ 0.0451],\n",
      "         [ 0.0118],\n",
      "         [-0.0767],\n",
      "         [ 0.0646],\n",
      "         [ 0.1108],\n",
      "         [ 0.0210],\n",
      "         [ 0.0123],\n",
      "         [ 0.1080],\n",
      "         [ 0.0240],\n",
      "         [ 0.0461],\n",
      "         [-0.0209],\n",
      "         [ 0.0698],\n",
      "         [ 0.0297],\n",
      "         [ 0.0723],\n",
      "         [ 0.1046],\n",
      "         [ 0.0271],\n",
      "         [ 0.0389],\n",
      "         [ 0.0359],\n",
      "         [ 0.1048],\n",
      "         [-0.0286],\n",
      "         [-0.0932],\n",
      "         [ 0.0903],\n",
      "         [-0.0648]],\n",
      "\n",
      "        [[ 0.0127],\n",
      "         [ 0.1206],\n",
      "         [-0.0899],\n",
      "         [-0.1249],\n",
      "         [-0.1121],\n",
      "         [ 0.0089],\n",
      "         [-0.0059],\n",
      "         [-0.1150],\n",
      "         [ 0.0477],\n",
      "         [ 0.1180],\n",
      "         [ 0.0052],\n",
      "         [-0.0100],\n",
      "         [-0.0099],\n",
      "         [-0.0905],\n",
      "         [-0.0820],\n",
      "         [-0.0757],\n",
      "         [-0.0657],\n",
      "         [ 0.0053],\n",
      "         [-0.0369],\n",
      "         [ 0.0083],\n",
      "         [-0.0854],\n",
      "         [-0.0785],\n",
      "         [-0.0971],\n",
      "         [ 0.0933],\n",
      "         [ 0.0228],\n",
      "         [-0.0161],\n",
      "         [-0.0821],\n",
      "         [ 0.0294],\n",
      "         [ 0.0597],\n",
      "         [ 0.0762],\n",
      "         [ 0.1135],\n",
      "         [ 0.0053],\n",
      "         [-0.0541],\n",
      "         [-0.0772],\n",
      "         [ 0.0870],\n",
      "         [-0.0448],\n",
      "         [ 0.0663],\n",
      "         [ 0.0844],\n",
      "         [-0.0701],\n",
      "         [-0.0952],\n",
      "         [-0.0567],\n",
      "         [-0.0466],\n",
      "         [ 0.0984],\n",
      "         [ 0.1041],\n",
      "         [-0.0439],\n",
      "         [ 0.0260],\n",
      "         [ 0.1216],\n",
      "         [-0.0586],\n",
      "         [-0.1110],\n",
      "         [-0.0731],\n",
      "         [-0.0170],\n",
      "         [-0.0344],\n",
      "         [ 0.0858],\n",
      "         [-0.1224],\n",
      "         [-0.0709],\n",
      "         [-0.0709],\n",
      "         [ 0.0994],\n",
      "         [ 0.0457],\n",
      "         [-0.0601],\n",
      "         [-0.0279],\n",
      "         [ 0.0359],\n",
      "         [-0.1163],\n",
      "         [-0.0884],\n",
      "         [-0.0505]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0083, -0.0654], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.0218,  0.0081,  0.0950],\n",
      "         [-0.1440, -0.0331,  0.0239],\n",
      "         [ 0.0092,  0.0045, -0.1054],\n",
      "         [ 0.0318,  0.0738,  0.1263],\n",
      "         [ 0.0767,  0.0993, -0.1022],\n",
      "         [ 0.0699,  0.0225,  0.0586],\n",
      "         [-0.0453,  0.0802,  0.0938],\n",
      "         [-0.0618, -0.1216, -0.1025],\n",
      "         [-0.0683, -0.1383,  0.1262],\n",
      "         [ 0.1354,  0.1294,  0.0012],\n",
      "         [-0.0519,  0.1107,  0.1413],\n",
      "         [-0.1291, -0.0246, -0.1207],\n",
      "         [ 0.0622,  0.1058,  0.0022],\n",
      "         [ 0.1235, -0.1209, -0.0522],\n",
      "         [ 0.1191, -0.0707, -0.0887],\n",
      "         [ 0.0278,  0.1261,  0.0566]],\n",
      "\n",
      "        [[-0.0222,  0.1258,  0.0935],\n",
      "         [-0.1122,  0.0480,  0.0183],\n",
      "         [-0.0140, -0.1037, -0.1414],\n",
      "         [ 0.1323, -0.0103, -0.0286],\n",
      "         [ 0.0936,  0.0910,  0.0276],\n",
      "         [-0.1365,  0.1306, -0.1160],\n",
      "         [ 0.1046,  0.0105,  0.1401],\n",
      "         [ 0.1381, -0.1214,  0.1423],\n",
      "         [-0.0705, -0.1191,  0.1012],\n",
      "         [-0.0280,  0.0366, -0.0149],\n",
      "         [ 0.1187,  0.0096, -0.0071],\n",
      "         [ 0.1053, -0.0013,  0.0191],\n",
      "         [ 0.0880, -0.0988, -0.0590],\n",
      "         [-0.0027, -0.0571,  0.0776],\n",
      "         [ 0.0103, -0.0387, -0.0340],\n",
      "         [ 0.1331, -0.0812, -0.1206]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.0055,  0.0180], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.0378],\n",
      "         [-0.1865],\n",
      "         [ 0.1925],\n",
      "         [-0.0058],\n",
      "         [-0.0248],\n",
      "         [-0.1517],\n",
      "         [-0.1174],\n",
      "         [-0.1949],\n",
      "         [ 0.1462],\n",
      "         [ 0.2161],\n",
      "         [-0.1962],\n",
      "         [-0.0288],\n",
      "         [-0.0494],\n",
      "         [-0.0067],\n",
      "         [ 0.1485],\n",
      "         [-0.2067]],\n",
      "\n",
      "        [[-0.1668],\n",
      "         [-0.2067],\n",
      "         [ 0.1638],\n",
      "         [-0.0860],\n",
      "         [-0.1078],\n",
      "         [ 0.2134],\n",
      "         [ 0.2016],\n",
      "         [-0.2226],\n",
      "         [-0.0647],\n",
      "         [ 0.1713],\n",
      "         [ 0.0941],\n",
      "         [-0.0403],\n",
      "         [ 0.0637],\n",
      "         [-0.0220],\n",
      "         [ 0.0266],\n",
      "         [ 0.0270]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv3.bias | Size: torch.Size([64]) | Values : tensor([ 0.1271, -0.0770], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 0.0050],\n",
      "         [ 0.0757],\n",
      "         [-0.0674],\n",
      "         [ 0.0235],\n",
      "         [ 0.1155],\n",
      "         [ 0.0442],\n",
      "         [-0.0780],\n",
      "         [ 0.1027],\n",
      "         [-0.0071],\n",
      "         [ 0.0068],\n",
      "         [-0.0935],\n",
      "         [ 0.0750],\n",
      "         [ 0.1041],\n",
      "         [-0.0257],\n",
      "         [-0.0962],\n",
      "         [-0.0521],\n",
      "         [-0.0890],\n",
      "         [ 0.0877],\n",
      "         [-0.1057],\n",
      "         [-0.0094],\n",
      "         [-0.0293],\n",
      "         [ 0.0837],\n",
      "         [ 0.1091],\n",
      "         [ 0.1168],\n",
      "         [ 0.0662],\n",
      "         [-0.0184],\n",
      "         [ 0.0681],\n",
      "         [ 0.1155],\n",
      "         [-0.0293],\n",
      "         [ 0.1041],\n",
      "         [ 0.0596],\n",
      "         [ 0.1080],\n",
      "         [-0.0349],\n",
      "         [ 0.0131],\n",
      "         [ 0.0222],\n",
      "         [ 0.1232],\n",
      "         [ 0.0870],\n",
      "         [ 0.0605],\n",
      "         [-0.1114],\n",
      "         [ 0.0856],\n",
      "         [-0.0839],\n",
      "         [-0.0829],\n",
      "         [ 0.0978],\n",
      "         [ 0.0306],\n",
      "         [-0.0557],\n",
      "         [ 0.0594],\n",
      "         [-0.1098],\n",
      "         [-0.1065],\n",
      "         [-0.1126],\n",
      "         [-0.0265],\n",
      "         [-0.0056],\n",
      "         [ 0.0567],\n",
      "         [ 0.0829],\n",
      "         [ 0.0076],\n",
      "         [-0.0474],\n",
      "         [-0.0259],\n",
      "         [-0.0059],\n",
      "         [ 0.0229],\n",
      "         [ 0.0195],\n",
      "         [-0.0238],\n",
      "         [-0.0184],\n",
      "         [ 0.0944],\n",
      "         [-0.0587],\n",
      "         [ 0.0492]],\n",
      "\n",
      "        [[-0.1189],\n",
      "         [ 0.1249],\n",
      "         [-0.0648],\n",
      "         [-0.0137],\n",
      "         [-0.0709],\n",
      "         [ 0.0480],\n",
      "         [-0.0856],\n",
      "         [-0.0650],\n",
      "         [-0.0241],\n",
      "         [-0.0667],\n",
      "         [ 0.0221],\n",
      "         [-0.1158],\n",
      "         [-0.0642],\n",
      "         [ 0.0980],\n",
      "         [-0.0129],\n",
      "         [-0.0607],\n",
      "         [-0.0633],\n",
      "         [ 0.0330],\n",
      "         [-0.0422],\n",
      "         [ 0.0379],\n",
      "         [ 0.1098],\n",
      "         [-0.1073],\n",
      "         [-0.0615],\n",
      "         [-0.1204],\n",
      "         [-0.1184],\n",
      "         [ 0.0490],\n",
      "         [ 0.0801],\n",
      "         [-0.0782],\n",
      "         [ 0.0054],\n",
      "         [ 0.0052],\n",
      "         [-0.0057],\n",
      "         [-0.0961],\n",
      "         [ 0.0796],\n",
      "         [ 0.0109],\n",
      "         [-0.0937],\n",
      "         [ 0.0262],\n",
      "         [ 0.0538],\n",
      "         [ 0.0491],\n",
      "         [-0.1039],\n",
      "         [-0.0338],\n",
      "         [ 0.0317],\n",
      "         [ 0.1230],\n",
      "         [-0.0880],\n",
      "         [ 0.0721],\n",
      "         [ 0.0984],\n",
      "         [ 0.1229],\n",
      "         [-0.0713],\n",
      "         [ 0.0959],\n",
      "         [-0.1214],\n",
      "         [-0.0504],\n",
      "         [ 0.1247],\n",
      "         [-0.0328],\n",
      "         [-0.0339],\n",
      "         [-0.1106],\n",
      "         [-0.0547],\n",
      "         [-0.0652],\n",
      "         [ 0.0484],\n",
      "         [ 0.0697],\n",
      "         [-0.0379],\n",
      "         [ 0.0328],\n",
      "         [-0.0608],\n",
      "         [ 0.0211],\n",
      "         [-0.1176],\n",
      "         [ 0.0859]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0473, -0.0887], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[ 0.0029],\n",
      "         [-0.0433],\n",
      "         [ 0.0929],\n",
      "         [ 0.1220],\n",
      "         [-0.0925],\n",
      "         [ 0.0463],\n",
      "         [-0.1021],\n",
      "         [ 0.0727],\n",
      "         [-0.0511],\n",
      "         [-0.0340],\n",
      "         [ 0.1242],\n",
      "         [-0.1172],\n",
      "         [-0.0167],\n",
      "         [-0.0777],\n",
      "         [ 0.0562],\n",
      "         [-0.0212],\n",
      "         [ 0.0044],\n",
      "         [-0.0458],\n",
      "         [ 0.0678],\n",
      "         [ 0.1113],\n",
      "         [-0.0805],\n",
      "         [-0.0251],\n",
      "         [-0.0524],\n",
      "         [-0.0084],\n",
      "         [-0.0411],\n",
      "         [-0.0216],\n",
      "         [ 0.0293],\n",
      "         [-0.0658],\n",
      "         [-0.0941],\n",
      "         [-0.1213],\n",
      "         [-0.0499],\n",
      "         [ 0.0533],\n",
      "         [-0.0842],\n",
      "         [-0.0025],\n",
      "         [ 0.1244],\n",
      "         [-0.0581],\n",
      "         [ 0.1053],\n",
      "         [ 0.0614],\n",
      "         [ 0.0723],\n",
      "         [ 0.0482],\n",
      "         [ 0.1230],\n",
      "         [-0.0727],\n",
      "         [-0.0881],\n",
      "         [ 0.0881],\n",
      "         [ 0.0790],\n",
      "         [-0.0930],\n",
      "         [ 0.0422],\n",
      "         [ 0.1152],\n",
      "         [ 0.0222],\n",
      "         [-0.0114],\n",
      "         [-0.0106],\n",
      "         [-0.0604],\n",
      "         [-0.0788],\n",
      "         [-0.0833],\n",
      "         [-0.0476],\n",
      "         [ 0.0500],\n",
      "         [-0.0261],\n",
      "         [ 0.0499],\n",
      "         [ 0.1173],\n",
      "         [-0.0061],\n",
      "         [ 0.0268],\n",
      "         [ 0.0820],\n",
      "         [-0.0082],\n",
      "         [ 0.0730]],\n",
      "\n",
      "        [[-0.0210],\n",
      "         [ 0.0017],\n",
      "         [-0.0500],\n",
      "         [-0.0172],\n",
      "         [-0.1069],\n",
      "         [-0.0546],\n",
      "         [ 0.0088],\n",
      "         [-0.1137],\n",
      "         [-0.0949],\n",
      "         [ 0.0332],\n",
      "         [-0.0859],\n",
      "         [-0.1147],\n",
      "         [ 0.0057],\n",
      "         [-0.0664],\n",
      "         [-0.0325],\n",
      "         [-0.0639],\n",
      "         [ 0.0564],\n",
      "         [-0.0374],\n",
      "         [ 0.0938],\n",
      "         [-0.0948],\n",
      "         [ 0.0250],\n",
      "         [-0.1047],\n",
      "         [ 0.0198],\n",
      "         [-0.1062],\n",
      "         [ 0.1076],\n",
      "         [ 0.1237],\n",
      "         [ 0.0607],\n",
      "         [-0.0871],\n",
      "         [-0.0068],\n",
      "         [-0.0112],\n",
      "         [ 0.0255],\n",
      "         [-0.0553],\n",
      "         [ 0.0628],\n",
      "         [-0.1023],\n",
      "         [ 0.0348],\n",
      "         [ 0.0486],\n",
      "         [-0.1004],\n",
      "         [ 0.0512],\n",
      "         [-0.0325],\n",
      "         [ 0.0275],\n",
      "         [-0.1120],\n",
      "         [ 0.1043],\n",
      "         [-0.1077],\n",
      "         [-0.1222],\n",
      "         [-0.0622],\n",
      "         [-0.0650],\n",
      "         [-0.0682],\n",
      "         [ 0.0699],\n",
      "         [ 0.0372],\n",
      "         [-0.0705],\n",
      "         [-0.0094],\n",
      "         [-0.1134],\n",
      "         [-0.0912],\n",
      "         [-0.0733],\n",
      "         [ 0.0872],\n",
      "         [ 0.1175],\n",
      "         [ 0.0180],\n",
      "         [ 0.0816],\n",
      "         [ 0.0579],\n",
      "         [-0.1143],\n",
      "         [-0.0478],\n",
      "         [ 0.0589],\n",
      "         [-0.0475],\n",
      "         [-0.0839]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0357, -0.0660], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.1108, -0.1161,  0.0170],\n",
      "         [ 0.0668, -0.1061,  0.0515],\n",
      "         [ 0.0075,  0.0682,  0.0609],\n",
      "         [-0.0509, -0.0161,  0.0937],\n",
      "         [ 0.1069,  0.1215,  0.1219],\n",
      "         [-0.0231,  0.0790,  0.0552],\n",
      "         [ 0.0725,  0.0618,  0.1400],\n",
      "         [ 0.1373,  0.1129,  0.0521],\n",
      "         [-0.1132, -0.0486,  0.0839],\n",
      "         [ 0.1257, -0.1160, -0.1277],\n",
      "         [-0.0564,  0.0103,  0.0786],\n",
      "         [ 0.0690, -0.1412, -0.0983],\n",
      "         [-0.0881, -0.1167, -0.0922],\n",
      "         [ 0.0025,  0.1245, -0.0833],\n",
      "         [ 0.1221,  0.0520, -0.0262],\n",
      "         [-0.0739, -0.0997, -0.0660]],\n",
      "\n",
      "        [[-0.0020,  0.0431, -0.1277],\n",
      "         [-0.0135,  0.0368,  0.0835],\n",
      "         [ 0.1275,  0.1080,  0.0715],\n",
      "         [ 0.0769,  0.0584, -0.0651],\n",
      "         [ 0.1284, -0.0345,  0.0822],\n",
      "         [ 0.1011, -0.0329,  0.1220],\n",
      "         [ 0.0825,  0.0013, -0.0521],\n",
      "         [ 0.1277,  0.1146,  0.0926],\n",
      "         [-0.0835,  0.1123,  0.1285],\n",
      "         [-0.0602, -0.0703, -0.0310],\n",
      "         [-0.0145, -0.0579,  0.0800],\n",
      "         [ 0.1368, -0.1011, -0.1359],\n",
      "         [-0.0397, -0.1178,  0.0437],\n",
      "         [ 0.0315,  0.0159,  0.1282],\n",
      "         [ 0.0646, -0.0995, -0.1312],\n",
      "         [-0.0623,  0.1321, -0.0798]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv2.bias | Size: torch.Size([16]) | Values : tensor([0.0823, 0.0629], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[ 0.1810],\n",
      "         [ 0.0625],\n",
      "         [-0.1329],\n",
      "         [-0.1950],\n",
      "         [ 0.1303],\n",
      "         [ 0.1103],\n",
      "         [-0.0968],\n",
      "         [ 0.1618],\n",
      "         [ 0.1877],\n",
      "         [ 0.1570],\n",
      "         [ 0.1096],\n",
      "         [ 0.2341],\n",
      "         [-0.2089],\n",
      "         [ 0.1864],\n",
      "         [-0.0754],\n",
      "         [ 0.2169]],\n",
      "\n",
      "        [[-0.0981],\n",
      "         [ 0.1130],\n",
      "         [ 0.1616],\n",
      "         [ 0.1250],\n",
      "         [-0.2482],\n",
      "         [-0.0579],\n",
      "         [ 0.1082],\n",
      "         [-0.1319],\n",
      "         [-0.1787],\n",
      "         [ 0.0562],\n",
      "         [ 0.1354],\n",
      "         [-0.1771],\n",
      "         [-0.1340],\n",
      "         [-0.1395],\n",
      "         [-0.2287],\n",
      "         [-0.2245]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.1547, -0.1238], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: identityblk3.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.1.weight | Size: torch.Size([2, 704]) | Values : tensor([[-0.0038, -0.0258,  0.0032,  ...,  0.0022,  0.0093, -0.0353],\n",
      "        [ 0.0187,  0.0041,  0.0191,  ..., -0.0011,  0.0098,  0.0302]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.1.bias | Size: torch.Size([2]) | Values : tensor([-0.0225,  0.0065], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from models.ResNet24 import ResNet24\n",
    "\n",
    "# Create an instance of the model\n",
    "model_resnet = ResNet24().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_resnet.parameters(), lr=learning_rate)\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=patience, verbose=True)\n",
    "print(f\"Model structure: {model_resnet}\\n\\n\")\n",
    "for name, param in model_resnet.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss: 1.372937  [   64/23290]\n",
      "loss: 0.051326  [ 6464/23290]\n",
      "loss: 0.069794  [12864/23290]\n",
      "loss: 0.351553  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.088575 \n",
      "\n",
      "Epoch 1:\n",
      "loss: 0.121594  [   64/23290]\n",
      "loss: 0.161675  [ 6464/23290]\n",
      "loss: 0.168580  [12864/23290]\n",
      "loss: 0.386021  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.071978 \n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.083918  [   64/23290]\n",
      "loss: 0.066072  [ 6464/23290]\n",
      "loss: 0.043095  [12864/23290]\n",
      "loss: 0.160842  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.070012 \n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.048979  [   64/23290]\n",
      "loss: 0.182078  [ 6464/23290]\n",
      "loss: 0.078956  [12864/23290]\n",
      "loss: 0.081511  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.078104 \n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.070805  [   64/23290]\n",
      "loss: 0.061576  [ 6464/23290]\n",
      "loss: 0.016617  [12864/23290]\n",
      "loss: 0.231080  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.048517 \n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.210002  [   64/23290]\n",
      "loss: 0.131951  [ 6464/23290]\n",
      "loss: 0.559788  [12864/23290]\n",
      "loss: 0.241953  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.051215 \n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.038955  [   64/23290]\n",
      "loss: 0.193390  [ 6464/23290]\n",
      "loss: 0.043909  [12864/23290]\n",
      "loss: 0.247401  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.042972 \n",
      "\n",
      "Epoch 7:\n",
      "loss: 0.119031  [   64/23290]\n",
      "loss: 0.604144  [ 6464/23290]\n",
      "loss: 0.019023  [12864/23290]\n",
      "loss: 0.090766  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.051223 \n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.025295  [   64/23290]\n",
      "loss: 0.035854  [ 6464/23290]\n",
      "loss: 0.019519  [12864/23290]\n",
      "loss: 0.061390  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.062806 \n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.059701  [   64/23290]\n",
      "loss: 0.009115  [ 6464/23290]\n",
      "loss: 0.081307  [12864/23290]\n",
      "loss: 0.115580  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049947 \n",
      "\n",
      "Epoch 10:\n",
      "loss: 0.035815  [   64/23290]\n",
      "loss: 0.042042  [ 6464/23290]\n",
      "loss: 0.122105  [12864/23290]\n",
      "loss: 0.283042  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043769 \n",
      "\n",
      "Epoch 11:\n",
      "loss: 0.034339  [   64/23290]\n",
      "loss: 0.169212  [ 6464/23290]\n",
      "loss: 0.045218  [12864/23290]\n",
      "loss: 0.027759  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.059080 \n",
      "\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model_resnet, loss_fn, optimizer,val_dataloader, \n",
    "        patience=patience, scheduler=scheduler, device=device, epochs=epochs, B_size=B_size, A_size=A_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 0.674116 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.453\n",
      " specificity\t0.719\n",
      " sensitivity\t0.188\n",
      " Label 1\n",
      "    accuracy\t0.453\n",
      " specificity\t0.188\n",
      " sensitivity\t0.719\n",
      "[[ 6 26]\n",
      " [ 9 23]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final test\n",
    "test(test_dataloader, model_resnet, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyFallNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=16, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.convr = nn.Conv1d(in_channels=in_channels, out_channels=64, kernel_size=1)\n",
    "    def forward(self, input):\n",
    "        residual = self.convr(input)\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x += residual\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "\n",
    "class TinyFallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyFallNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=9, out_channels=64, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.convblk1 = ConvBlock(64)\n",
    "        self.convblk2 = ConvBlock(64)\n",
    "        self.convblk3 = ConvBlock(64)\n",
    "        self.convblk4 = ConvBlock(64)\n",
    "        \n",
    "        self.pool2 = nn.AvgPool1d(2)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(in_features=768, out_features=2),\n",
    "                                nn.Softmax())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblk1(x)\n",
    "        x = self.convblk2(x)\n",
    "        x = self.convblk3(x)\n",
    "        x = self.convblk4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: TinyFallNet(\n",
      "  (conv1): Conv1d(9, 64, kernel_size=(3,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convblk1): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (convblk2): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (convblk3): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (convblk4): ConvBlock(\n",
      "    (conv1): Conv1d(64, 16, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU()\n",
      "    (conv3): Conv1d(16, 64, kernel_size=(1,), stride=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu3): ReLU()\n",
      "    (convr): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (pool2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (2): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: conv1.weight | Size: torch.Size([64, 9, 3]) | Values : tensor([[[-0.0750,  0.1589, -0.1657],\n",
      "         [-0.0295,  0.0671,  0.1055],\n",
      "         [-0.1272,  0.1911,  0.0734],\n",
      "         [ 0.0240, -0.1226,  0.1451],\n",
      "         [ 0.0156, -0.0270, -0.1890],\n",
      "         [-0.0245,  0.0154, -0.0644],\n",
      "         [ 0.0743,  0.0181,  0.0670],\n",
      "         [-0.1418, -0.0791,  0.0301],\n",
      "         [-0.0880, -0.0973, -0.0092]],\n",
      "\n",
      "        [[ 0.0684,  0.0912, -0.1415],\n",
      "         [ 0.1526, -0.1618,  0.1475],\n",
      "         [-0.0571,  0.1131, -0.0075],\n",
      "         [-0.1629, -0.1026,  0.0392],\n",
      "         [-0.0639,  0.1318, -0.0841],\n",
      "         [-0.0191, -0.1093,  0.0695],\n",
      "         [-0.0236,  0.0931,  0.1423],\n",
      "         [ 0.0421,  0.0686,  0.1450],\n",
      "         [ 0.1166,  0.0265, -0.1520]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.bias | Size: torch.Size([64]) | Values : tensor([ 0.1026, -0.1774], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[ 0.0039],\n",
      "         [-0.1104],\n",
      "         [-0.0967],\n",
      "         [ 0.0389],\n",
      "         [-0.0151],\n",
      "         [ 0.0295],\n",
      "         [-0.1245],\n",
      "         [ 0.0426],\n",
      "         [ 0.0445],\n",
      "         [ 0.0771],\n",
      "         [ 0.0044],\n",
      "         [ 0.1013],\n",
      "         [-0.1077],\n",
      "         [-0.0331],\n",
      "         [-0.0589],\n",
      "         [-0.0843],\n",
      "         [-0.1171],\n",
      "         [ 0.1011],\n",
      "         [-0.0852],\n",
      "         [ 0.1201],\n",
      "         [ 0.0136],\n",
      "         [-0.0401],\n",
      "         [-0.0932],\n",
      "         [-0.0323],\n",
      "         [ 0.0447],\n",
      "         [ 0.0429],\n",
      "         [-0.0038],\n",
      "         [ 0.0444],\n",
      "         [-0.0315],\n",
      "         [-0.0733],\n",
      "         [ 0.1072],\n",
      "         [-0.0162],\n",
      "         [ 0.0798],\n",
      "         [ 0.0009],\n",
      "         [ 0.0681],\n",
      "         [-0.0853],\n",
      "         [-0.0248],\n",
      "         [ 0.0725],\n",
      "         [-0.1032],\n",
      "         [ 0.0189],\n",
      "         [ 0.0679],\n",
      "         [-0.0430],\n",
      "         [ 0.0310],\n",
      "         [ 0.0910],\n",
      "         [ 0.0101],\n",
      "         [ 0.1007],\n",
      "         [ 0.1225],\n",
      "         [ 0.0419],\n",
      "         [-0.0392],\n",
      "         [-0.0034],\n",
      "         [-0.0014],\n",
      "         [ 0.0574],\n",
      "         [ 0.0386],\n",
      "         [-0.0020],\n",
      "         [ 0.0818],\n",
      "         [ 0.0219],\n",
      "         [-0.0014],\n",
      "         [-0.0457],\n",
      "         [ 0.0567],\n",
      "         [-0.0748],\n",
      "         [-0.1239],\n",
      "         [ 0.0085],\n",
      "         [-0.0028],\n",
      "         [-0.0783]],\n",
      "\n",
      "        [[-0.0046],\n",
      "         [ 0.0794],\n",
      "         [-0.0083],\n",
      "         [ 0.0922],\n",
      "         [ 0.0421],\n",
      "         [ 0.0875],\n",
      "         [ 0.0639],\n",
      "         [-0.1205],\n",
      "         [-0.0013],\n",
      "         [ 0.1122],\n",
      "         [ 0.0659],\n",
      "         [ 0.0588],\n",
      "         [-0.1092],\n",
      "         [-0.0614],\n",
      "         [-0.0556],\n",
      "         [ 0.0673],\n",
      "         [ 0.0956],\n",
      "         [ 0.0980],\n",
      "         [ 0.0149],\n",
      "         [-0.0397],\n",
      "         [ 0.0236],\n",
      "         [ 0.0396],\n",
      "         [-0.0575],\n",
      "         [-0.0028],\n",
      "         [-0.0324],\n",
      "         [-0.0635],\n",
      "         [-0.0477],\n",
      "         [-0.0929],\n",
      "         [ 0.0083],\n",
      "         [ 0.1023],\n",
      "         [ 0.1146],\n",
      "         [-0.0686],\n",
      "         [ 0.0939],\n",
      "         [ 0.1203],\n",
      "         [ 0.0352],\n",
      "         [ 0.0098],\n",
      "         [ 0.0794],\n",
      "         [ 0.1234],\n",
      "         [-0.1235],\n",
      "         [-0.0045],\n",
      "         [-0.0762],\n",
      "         [-0.0753],\n",
      "         [ 0.0840],\n",
      "         [ 0.0653],\n",
      "         [-0.0064],\n",
      "         [ 0.0715],\n",
      "         [ 0.1099],\n",
      "         [ 0.1130],\n",
      "         [ 0.0610],\n",
      "         [-0.0207],\n",
      "         [ 0.0409],\n",
      "         [-0.0227],\n",
      "         [ 0.1230],\n",
      "         [ 0.0248],\n",
      "         [ 0.1056],\n",
      "         [-0.0388],\n",
      "         [ 0.0408],\n",
      "         [-0.0599],\n",
      "         [-0.0597],\n",
      "         [ 0.0089],\n",
      "         [-0.0280],\n",
      "         [ 0.0548],\n",
      "         [ 0.0765],\n",
      "         [-0.0661]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv1.bias | Size: torch.Size([16]) | Values : tensor([ 0.0676, -0.0914], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.1352,  0.0212, -0.0852],\n",
      "         [-0.0352,  0.0744, -0.1183],\n",
      "         [ 0.0293, -0.1053, -0.0072],\n",
      "         [ 0.0444, -0.1416,  0.1256],\n",
      "         [-0.0144,  0.0643,  0.0649],\n",
      "         [-0.0836, -0.1065,  0.0891],\n",
      "         [-0.1011,  0.1020,  0.1063],\n",
      "         [-0.1137, -0.0204, -0.0498],\n",
      "         [ 0.0838, -0.0306,  0.0838],\n",
      "         [ 0.0229, -0.0247,  0.0099],\n",
      "         [-0.1015,  0.1370, -0.1382],\n",
      "         [-0.1328,  0.1135,  0.0095],\n",
      "         [ 0.1143,  0.1425, -0.0839],\n",
      "         [-0.0157, -0.0060, -0.0390],\n",
      "         [ 0.0960, -0.0185,  0.0669],\n",
      "         [ 0.1259,  0.0446,  0.0156]],\n",
      "\n",
      "        [[ 0.0154, -0.0665,  0.0737],\n",
      "         [ 0.0128, -0.0251,  0.0218],\n",
      "         [ 0.0630,  0.0221,  0.0987],\n",
      "         [ 0.0675, -0.1007,  0.0567],\n",
      "         [-0.0835, -0.1164,  0.0384],\n",
      "         [-0.0711, -0.0806,  0.1405],\n",
      "         [-0.1165, -0.0075,  0.0697],\n",
      "         [ 0.0306, -0.0995,  0.0264],\n",
      "         [ 0.0021,  0.0242,  0.0010],\n",
      "         [-0.0281, -0.0460,  0.0399],\n",
      "         [-0.1379, -0.0269, -0.0934],\n",
      "         [-0.1110, -0.0919,  0.1100],\n",
      "         [-0.1441,  0.0551, -0.1367],\n",
      "         [ 0.0590, -0.0688,  0.0089],\n",
      "         [ 0.0642, -0.1210,  0.1146],\n",
      "         [ 0.0518, -0.1028,  0.0329]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv2.bias | Size: torch.Size([16]) | Values : tensor([ 0.0220, -0.0669], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.1189],\n",
      "         [ 0.2466],\n",
      "         [-0.0692],\n",
      "         [-0.2327],\n",
      "         [-0.0512],\n",
      "         [ 0.2396],\n",
      "         [ 0.2423],\n",
      "         [ 0.1255],\n",
      "         [ 0.0145],\n",
      "         [ 0.0750],\n",
      "         [-0.2201],\n",
      "         [-0.0594],\n",
      "         [ 0.0288],\n",
      "         [-0.2246],\n",
      "         [-0.1262],\n",
      "         [ 0.2163]],\n",
      "\n",
      "        [[ 0.0188],\n",
      "         [ 0.1010],\n",
      "         [-0.0121],\n",
      "         [ 0.1653],\n",
      "         [-0.2427],\n",
      "         [-0.1243],\n",
      "         [ 0.1657],\n",
      "         [-0.1953],\n",
      "         [ 0.0856],\n",
      "         [ 0.1430],\n",
      "         [-0.0319],\n",
      "         [ 0.1290],\n",
      "         [ 0.0604],\n",
      "         [-0.1089],\n",
      "         [ 0.1861],\n",
      "         [ 0.1993]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.conv3.bias | Size: torch.Size([64]) | Values : tensor([0.1329, 0.1901], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 0.0680],\n",
      "         [-0.0137],\n",
      "         [-0.0138],\n",
      "         [-0.0996],\n",
      "         [-0.0638],\n",
      "         [ 0.0889],\n",
      "         [-0.1013],\n",
      "         [ 0.0741],\n",
      "         [ 0.1194],\n",
      "         [ 0.0459],\n",
      "         [ 0.0504],\n",
      "         [-0.0449],\n",
      "         [-0.0847],\n",
      "         [ 0.0142],\n",
      "         [ 0.0288],\n",
      "         [ 0.1190],\n",
      "         [-0.0075],\n",
      "         [ 0.0053],\n",
      "         [ 0.0208],\n",
      "         [ 0.0553],\n",
      "         [ 0.1130],\n",
      "         [ 0.0295],\n",
      "         [-0.0441],\n",
      "         [ 0.0238],\n",
      "         [ 0.0370],\n",
      "         [-0.0159],\n",
      "         [-0.0011],\n",
      "         [ 0.0753],\n",
      "         [-0.0492],\n",
      "         [-0.0912],\n",
      "         [-0.0099],\n",
      "         [ 0.1091],\n",
      "         [-0.0980],\n",
      "         [ 0.0452],\n",
      "         [ 0.0193],\n",
      "         [-0.0360],\n",
      "         [-0.0800],\n",
      "         [-0.1059],\n",
      "         [ 0.0989],\n",
      "         [-0.0291],\n",
      "         [ 0.0849],\n",
      "         [ 0.0661],\n",
      "         [ 0.0247],\n",
      "         [-0.1179],\n",
      "         [ 0.0518],\n",
      "         [ 0.0645],\n",
      "         [-0.0623],\n",
      "         [-0.0562],\n",
      "         [ 0.0245],\n",
      "         [ 0.1074],\n",
      "         [ 0.1124],\n",
      "         [ 0.0175],\n",
      "         [-0.0181],\n",
      "         [ 0.0463],\n",
      "         [ 0.1054],\n",
      "         [ 0.1178],\n",
      "         [ 0.0222],\n",
      "         [-0.0757],\n",
      "         [ 0.0856],\n",
      "         [ 0.0467],\n",
      "         [ 0.0085],\n",
      "         [ 0.1175],\n",
      "         [ 0.0418],\n",
      "         [-0.0953]],\n",
      "\n",
      "        [[ 0.0776],\n",
      "         [-0.1082],\n",
      "         [ 0.0372],\n",
      "         [-0.0462],\n",
      "         [ 0.1073],\n",
      "         [-0.0792],\n",
      "         [ 0.0909],\n",
      "         [ 0.1106],\n",
      "         [ 0.0910],\n",
      "         [-0.0304],\n",
      "         [-0.1229],\n",
      "         [-0.0050],\n",
      "         [-0.0884],\n",
      "         [-0.0681],\n",
      "         [-0.1060],\n",
      "         [ 0.0376],\n",
      "         [-0.0828],\n",
      "         [-0.0211],\n",
      "         [-0.0494],\n",
      "         [ 0.0878],\n",
      "         [-0.0528],\n",
      "         [-0.1126],\n",
      "         [-0.0340],\n",
      "         [-0.0072],\n",
      "         [-0.0788],\n",
      "         [-0.0308],\n",
      "         [-0.1230],\n",
      "         [-0.0926],\n",
      "         [ 0.0898],\n",
      "         [-0.0181],\n",
      "         [ 0.0199],\n",
      "         [-0.0108],\n",
      "         [-0.0318],\n",
      "         [-0.0138],\n",
      "         [-0.0615],\n",
      "         [ 0.0750],\n",
      "         [-0.0376],\n",
      "         [ 0.0312],\n",
      "         [ 0.1032],\n",
      "         [-0.0138],\n",
      "         [-0.0978],\n",
      "         [ 0.0757],\n",
      "         [-0.1158],\n",
      "         [-0.0655],\n",
      "         [-0.1111],\n",
      "         [ 0.0953],\n",
      "         [ 0.0342],\n",
      "         [-0.1010],\n",
      "         [ 0.0579],\n",
      "         [-0.1179],\n",
      "         [-0.0844],\n",
      "         [-0.0446],\n",
      "         [ 0.0177],\n",
      "         [-0.1241],\n",
      "         [ 0.0557],\n",
      "         [ 0.0982],\n",
      "         [-0.0530],\n",
      "         [ 0.0729],\n",
      "         [ 0.0074],\n",
      "         [ 0.1058],\n",
      "         [-0.0547],\n",
      "         [-0.0393],\n",
      "         [-0.0484],\n",
      "         [ 0.1079]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk1.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0965,  0.1052], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.0664],\n",
      "         [ 0.1162],\n",
      "         [ 0.0191],\n",
      "         [-0.0752],\n",
      "         [ 0.0432],\n",
      "         [-0.0988],\n",
      "         [-0.0717],\n",
      "         [-0.0429],\n",
      "         [-0.0148],\n",
      "         [ 0.0738],\n",
      "         [ 0.0634],\n",
      "         [-0.0938],\n",
      "         [ 0.0191],\n",
      "         [-0.0586],\n",
      "         [-0.0648],\n",
      "         [ 0.1144],\n",
      "         [-0.0104],\n",
      "         [ 0.0951],\n",
      "         [-0.1009],\n",
      "         [-0.0869],\n",
      "         [ 0.0160],\n",
      "         [ 0.0898],\n",
      "         [ 0.0914],\n",
      "         [-0.0595],\n",
      "         [ 0.1121],\n",
      "         [ 0.0045],\n",
      "         [ 0.0151],\n",
      "         [-0.1206],\n",
      "         [ 0.0713],\n",
      "         [ 0.1232],\n",
      "         [ 0.0444],\n",
      "         [ 0.0708],\n",
      "         [-0.0224],\n",
      "         [ 0.0965],\n",
      "         [-0.1198],\n",
      "         [-0.0625],\n",
      "         [-0.1189],\n",
      "         [ 0.0564],\n",
      "         [ 0.0413],\n",
      "         [-0.0364],\n",
      "         [ 0.0337],\n",
      "         [-0.0959],\n",
      "         [-0.0038],\n",
      "         [ 0.0005],\n",
      "         [ 0.0716],\n",
      "         [ 0.0565],\n",
      "         [-0.0127],\n",
      "         [ 0.1141],\n",
      "         [ 0.1206],\n",
      "         [-0.0597],\n",
      "         [ 0.1223],\n",
      "         [ 0.0316],\n",
      "         [ 0.0610],\n",
      "         [ 0.0911],\n",
      "         [ 0.1229],\n",
      "         [-0.0448],\n",
      "         [-0.0398],\n",
      "         [ 0.0169],\n",
      "         [-0.0892],\n",
      "         [-0.0684],\n",
      "         [ 0.0966],\n",
      "         [-0.0348],\n",
      "         [-0.0232],\n",
      "         [-0.0277]],\n",
      "\n",
      "        [[-0.0655],\n",
      "         [-0.0136],\n",
      "         [ 0.0578],\n",
      "         [ 0.1242],\n",
      "         [ 0.0167],\n",
      "         [-0.0735],\n",
      "         [-0.1181],\n",
      "         [ 0.0874],\n",
      "         [ 0.0382],\n",
      "         [ 0.0501],\n",
      "         [ 0.0107],\n",
      "         [-0.1062],\n",
      "         [ 0.0660],\n",
      "         [-0.0495],\n",
      "         [-0.0383],\n",
      "         [-0.0258],\n",
      "         [ 0.0523],\n",
      "         [ 0.0100],\n",
      "         [ 0.0722],\n",
      "         [-0.0552],\n",
      "         [-0.0650],\n",
      "         [-0.0747],\n",
      "         [-0.0609],\n",
      "         [-0.1015],\n",
      "         [ 0.0753],\n",
      "         [ 0.0960],\n",
      "         [ 0.1066],\n",
      "         [ 0.0829],\n",
      "         [ 0.0571],\n",
      "         [-0.1063],\n",
      "         [ 0.0908],\n",
      "         [ 0.0949],\n",
      "         [-0.0538],\n",
      "         [ 0.0472],\n",
      "         [ 0.0627],\n",
      "         [ 0.0606],\n",
      "         [ 0.1226],\n",
      "         [ 0.0298],\n",
      "         [-0.1249],\n",
      "         [-0.0090],\n",
      "         [-0.0120],\n",
      "         [-0.0790],\n",
      "         [ 0.0990],\n",
      "         [ 0.0816],\n",
      "         [ 0.0544],\n",
      "         [-0.0299],\n",
      "         [ 0.0722],\n",
      "         [-0.0951],\n",
      "         [ 0.0958],\n",
      "         [ 0.0239],\n",
      "         [ 0.0418],\n",
      "         [ 0.0696],\n",
      "         [ 0.0931],\n",
      "         [-0.0442],\n",
      "         [-0.1193],\n",
      "         [ 0.1210],\n",
      "         [-0.0502],\n",
      "         [ 0.0916],\n",
      "         [-0.0273],\n",
      "         [ 0.0451],\n",
      "         [-0.0205],\n",
      "         [-0.0130],\n",
      "         [-0.0228],\n",
      "         [-0.0654]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv1.bias | Size: torch.Size([16]) | Values : tensor([0.0763, 0.0384], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[-0.0879, -0.0650, -0.0354],\n",
      "         [-0.0214, -0.0415,  0.0230],\n",
      "         [-0.1407,  0.0985, -0.0039],\n",
      "         [ 0.0356,  0.0100,  0.1314],\n",
      "         [-0.0049, -0.1215, -0.1180],\n",
      "         [ 0.1215, -0.0903, -0.0242],\n",
      "         [-0.0889,  0.0148, -0.0072],\n",
      "         [ 0.0725,  0.0657, -0.1123],\n",
      "         [-0.0663,  0.0632,  0.1017],\n",
      "         [-0.0440, -0.1078, -0.1219],\n",
      "         [ 0.0894, -0.0117,  0.0597],\n",
      "         [ 0.1258,  0.1088, -0.0291],\n",
      "         [ 0.0980,  0.0702,  0.0241],\n",
      "         [ 0.0035,  0.0429, -0.0832],\n",
      "         [-0.0293, -0.0874, -0.1176],\n",
      "         [-0.1296, -0.0120,  0.0690]],\n",
      "\n",
      "        [[-0.1018,  0.0277,  0.1179],\n",
      "         [-0.0929, -0.1365,  0.0658],\n",
      "         [ 0.0643, -0.1087, -0.0339],\n",
      "         [ 0.0963, -0.0053, -0.0905],\n",
      "         [-0.0075,  0.0658,  0.1100],\n",
      "         [ 0.0161, -0.0770, -0.0590],\n",
      "         [-0.1191, -0.1040, -0.0486],\n",
      "         [-0.0879, -0.1329,  0.1203],\n",
      "         [ 0.0069, -0.0068, -0.0537],\n",
      "         [-0.0104, -0.0965, -0.1076],\n",
      "         [ 0.0428, -0.0244,  0.0598],\n",
      "         [-0.0914,  0.0508, -0.1029],\n",
      "         [-0.0255, -0.0365, -0.1271],\n",
      "         [ 0.1434,  0.0878,  0.1122],\n",
      "         [-0.0087, -0.0927, -0.0067],\n",
      "         [-0.0556,  0.0432, -0.1405]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.1046,  0.1255], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.1589],\n",
      "         [ 0.1523],\n",
      "         [ 0.0593],\n",
      "         [ 0.0832],\n",
      "         [ 0.1178],\n",
      "         [ 0.1771],\n",
      "         [-0.1761],\n",
      "         [-0.1047],\n",
      "         [-0.0636],\n",
      "         [ 0.0297],\n",
      "         [-0.2056],\n",
      "         [ 0.1071],\n",
      "         [ 0.1245],\n",
      "         [ 0.1786],\n",
      "         [ 0.2079],\n",
      "         [-0.1335]],\n",
      "\n",
      "        [[-0.0010],\n",
      "         [ 0.0606],\n",
      "         [ 0.0538],\n",
      "         [ 0.0085],\n",
      "         [-0.1711],\n",
      "         [ 0.0481],\n",
      "         [-0.1588],\n",
      "         [-0.1155],\n",
      "         [ 0.2098],\n",
      "         [ 0.2144],\n",
      "         [ 0.0016],\n",
      "         [ 0.0690],\n",
      "         [ 0.1554],\n",
      "         [-0.2132],\n",
      "         [ 0.0670],\n",
      "         [-0.2025]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.conv3.bias | Size: torch.Size([64]) | Values : tensor([0.0537, 0.1150], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 0.0213],\n",
      "         [ 0.0937],\n",
      "         [-0.0296],\n",
      "         [ 0.1168],\n",
      "         [-0.1056],\n",
      "         [ 0.0809],\n",
      "         [-0.0555],\n",
      "         [-0.0314],\n",
      "         [-0.1104],\n",
      "         [ 0.0397],\n",
      "         [-0.0090],\n",
      "         [-0.1036],\n",
      "         [ 0.1228],\n",
      "         [-0.0851],\n",
      "         [ 0.0861],\n",
      "         [-0.0661],\n",
      "         [ 0.1248],\n",
      "         [ 0.0004],\n",
      "         [ 0.0943],\n",
      "         [ 0.1095],\n",
      "         [ 0.0910],\n",
      "         [-0.1045],\n",
      "         [ 0.0418],\n",
      "         [ 0.0581],\n",
      "         [-0.0586],\n",
      "         [-0.0652],\n",
      "         [-0.1100],\n",
      "         [ 0.0441],\n",
      "         [-0.0189],\n",
      "         [-0.0630],\n",
      "         [ 0.1230],\n",
      "         [ 0.0318],\n",
      "         [-0.0965],\n",
      "         [ 0.0993],\n",
      "         [-0.1071],\n",
      "         [ 0.0249],\n",
      "         [ 0.0229],\n",
      "         [-0.0032],\n",
      "         [-0.0375],\n",
      "         [ 0.0400],\n",
      "         [-0.0732],\n",
      "         [-0.0056],\n",
      "         [ 0.0007],\n",
      "         [-0.0363],\n",
      "         [-0.1074],\n",
      "         [-0.0978],\n",
      "         [ 0.0359],\n",
      "         [ 0.0665],\n",
      "         [ 0.0205],\n",
      "         [-0.0243],\n",
      "         [ 0.1025],\n",
      "         [-0.1232],\n",
      "         [-0.1163],\n",
      "         [-0.0452],\n",
      "         [-0.0090],\n",
      "         [-0.0649],\n",
      "         [ 0.1217],\n",
      "         [-0.0214],\n",
      "         [ 0.1039],\n",
      "         [-0.0438],\n",
      "         [-0.0598],\n",
      "         [ 0.1092],\n",
      "         [ 0.0961],\n",
      "         [ 0.0213]],\n",
      "\n",
      "        [[ 0.0184],\n",
      "         [ 0.0701],\n",
      "         [-0.0597],\n",
      "         [-0.0033],\n",
      "         [-0.1096],\n",
      "         [ 0.0196],\n",
      "         [-0.0756],\n",
      "         [-0.0759],\n",
      "         [-0.1142],\n",
      "         [-0.0039],\n",
      "         [ 0.0681],\n",
      "         [ 0.0016],\n",
      "         [-0.1087],\n",
      "         [-0.0301],\n",
      "         [-0.0921],\n",
      "         [ 0.1108],\n",
      "         [ 0.1019],\n",
      "         [-0.0075],\n",
      "         [-0.0985],\n",
      "         [ 0.0509],\n",
      "         [ 0.0863],\n",
      "         [ 0.1044],\n",
      "         [-0.0539],\n",
      "         [-0.0338],\n",
      "         [ 0.0197],\n",
      "         [ 0.0664],\n",
      "         [-0.0773],\n",
      "         [-0.1064],\n",
      "         [ 0.0575],\n",
      "         [ 0.0002],\n",
      "         [-0.0274],\n",
      "         [ 0.0479],\n",
      "         [ 0.0965],\n",
      "         [ 0.0305],\n",
      "         [ 0.0728],\n",
      "         [ 0.0611],\n",
      "         [ 0.0689],\n",
      "         [ 0.1123],\n",
      "         [-0.0108],\n",
      "         [-0.0658],\n",
      "         [-0.0489],\n",
      "         [-0.1016],\n",
      "         [ 0.1244],\n",
      "         [ 0.1023],\n",
      "         [-0.0858],\n",
      "         [-0.0259],\n",
      "         [-0.0021],\n",
      "         [-0.0935],\n",
      "         [-0.0512],\n",
      "         [ 0.0012],\n",
      "         [ 0.1165],\n",
      "         [ 0.0978],\n",
      "         [ 0.0135],\n",
      "         [ 0.1070],\n",
      "         [ 0.0983],\n",
      "         [ 0.0895],\n",
      "         [ 0.1123],\n",
      "         [ 0.0045],\n",
      "         [ 0.1230],\n",
      "         [ 0.0539],\n",
      "         [-0.0256],\n",
      "         [-0.0219],\n",
      "         [ 0.1157],\n",
      "         [-0.1008]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk2.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0849,  0.0840], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[-0.0268],\n",
      "         [ 0.0942],\n",
      "         [ 0.0738],\n",
      "         [ 0.0023],\n",
      "         [-0.1217],\n",
      "         [-0.0468],\n",
      "         [ 0.0756],\n",
      "         [ 0.0988],\n",
      "         [ 0.0189],\n",
      "         [-0.0659],\n",
      "         [ 0.1045],\n",
      "         [-0.0268],\n",
      "         [ 0.0769],\n",
      "         [-0.0226],\n",
      "         [-0.0751],\n",
      "         [-0.0181],\n",
      "         [ 0.0338],\n",
      "         [-0.0264],\n",
      "         [-0.0376],\n",
      "         [-0.0273],\n",
      "         [ 0.0929],\n",
      "         [-0.0722],\n",
      "         [ 0.0697],\n",
      "         [ 0.0761],\n",
      "         [-0.0472],\n",
      "         [-0.0329],\n",
      "         [-0.0858],\n",
      "         [ 0.0917],\n",
      "         [ 0.0114],\n",
      "         [ 0.0375],\n",
      "         [ 0.0636],\n",
      "         [ 0.1041],\n",
      "         [ 0.0830],\n",
      "         [ 0.1119],\n",
      "         [ 0.0567],\n",
      "         [-0.1248],\n",
      "         [ 0.0546],\n",
      "         [-0.1096],\n",
      "         [ 0.0592],\n",
      "         [-0.0169],\n",
      "         [ 0.0332],\n",
      "         [ 0.0666],\n",
      "         [-0.1016],\n",
      "         [ 0.1086],\n",
      "         [ 0.0678],\n",
      "         [-0.0836],\n",
      "         [-0.0488],\n",
      "         [ 0.0517],\n",
      "         [ 0.0456],\n",
      "         [ 0.1020],\n",
      "         [-0.0169],\n",
      "         [-0.0590],\n",
      "         [ 0.0283],\n",
      "         [ 0.0069],\n",
      "         [ 0.0339],\n",
      "         [-0.0728],\n",
      "         [ 0.0715],\n",
      "         [-0.1139],\n",
      "         [-0.0436],\n",
      "         [ 0.0952],\n",
      "         [-0.0802],\n",
      "         [-0.0676],\n",
      "         [-0.0274],\n",
      "         [-0.0722]],\n",
      "\n",
      "        [[-0.1081],\n",
      "         [ 0.0044],\n",
      "         [ 0.0173],\n",
      "         [-0.1033],\n",
      "         [ 0.0053],\n",
      "         [-0.1041],\n",
      "         [ 0.0674],\n",
      "         [ 0.1131],\n",
      "         [-0.1235],\n",
      "         [-0.0960],\n",
      "         [-0.0633],\n",
      "         [ 0.1082],\n",
      "         [-0.0703],\n",
      "         [-0.0545],\n",
      "         [-0.0230],\n",
      "         [-0.0052],\n",
      "         [-0.0255],\n",
      "         [-0.0117],\n",
      "         [ 0.0097],\n",
      "         [ 0.0655],\n",
      "         [ 0.0028],\n",
      "         [ 0.0782],\n",
      "         [ 0.0911],\n",
      "         [-0.0006],\n",
      "         [-0.0958],\n",
      "         [-0.0355],\n",
      "         [ 0.1140],\n",
      "         [-0.1007],\n",
      "         [-0.0316],\n",
      "         [ 0.0275],\n",
      "         [ 0.1089],\n",
      "         [-0.1188],\n",
      "         [-0.0593],\n",
      "         [-0.0599],\n",
      "         [-0.0210],\n",
      "         [ 0.0603],\n",
      "         [-0.0611],\n",
      "         [-0.0318],\n",
      "         [ 0.0291],\n",
      "         [-0.1099],\n",
      "         [-0.0550],\n",
      "         [-0.0992],\n",
      "         [ 0.0548],\n",
      "         [ 0.0771],\n",
      "         [ 0.0230],\n",
      "         [-0.1141],\n",
      "         [-0.0631],\n",
      "         [ 0.0549],\n",
      "         [ 0.0609],\n",
      "         [ 0.0754],\n",
      "         [ 0.0133],\n",
      "         [-0.0723],\n",
      "         [ 0.1029],\n",
      "         [ 0.0488],\n",
      "         [ 0.1010],\n",
      "         [-0.1048],\n",
      "         [ 0.0271],\n",
      "         [ 0.1018],\n",
      "         [ 0.1026],\n",
      "         [-0.0252],\n",
      "         [-0.1196],\n",
      "         [-0.1048],\n",
      "         [ 0.0363],\n",
      "         [ 0.0674]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0226,  0.0574], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[ 0.0354, -0.0034,  0.0429],\n",
      "         [ 0.0827, -0.0186, -0.1294],\n",
      "         [ 0.0763,  0.0318, -0.0187],\n",
      "         [-0.0440, -0.0574, -0.0347],\n",
      "         [ 0.0664,  0.1297, -0.0498],\n",
      "         [-0.0842,  0.0759, -0.0017],\n",
      "         [ 0.1362,  0.0717, -0.1427],\n",
      "         [ 0.1008,  0.0333,  0.1047],\n",
      "         [ 0.0857,  0.0578, -0.1213],\n",
      "         [-0.0064, -0.0793, -0.0761],\n",
      "         [-0.0530,  0.1371,  0.0749],\n",
      "         [-0.0759,  0.0431, -0.1101],\n",
      "         [ 0.1082,  0.0521, -0.1285],\n",
      "         [-0.1178,  0.0418,  0.0448],\n",
      "         [-0.0003,  0.0764,  0.0335],\n",
      "         [ 0.1157, -0.0673,  0.0864]],\n",
      "\n",
      "        [[ 0.0220,  0.0485, -0.0509],\n",
      "         [ 0.0991, -0.1353,  0.0755],\n",
      "         [-0.0255, -0.0822,  0.0787],\n",
      "         [ 0.1081,  0.0608,  0.1371],\n",
      "         [ 0.0483, -0.0362,  0.1131],\n",
      "         [ 0.0421, -0.1089,  0.0540],\n",
      "         [-0.1322, -0.0344, -0.1418],\n",
      "         [-0.1292,  0.0693,  0.0675],\n",
      "         [ 0.1350,  0.0446,  0.0853],\n",
      "         [ 0.1212,  0.1297, -0.0923],\n",
      "         [-0.1423, -0.0279, -0.1384],\n",
      "         [ 0.1107, -0.0263,  0.0976],\n",
      "         [-0.0983,  0.0859,  0.0997],\n",
      "         [ 0.1215,  0.1244,  0.0628],\n",
      "         [ 0.0510, -0.0618,  0.0564],\n",
      "         [-0.0133,  0.1194, -0.0835]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv2.bias | Size: torch.Size([16]) | Values : tensor([ 0.0699, -0.0112], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[-0.2391],\n",
      "         [ 0.1636],\n",
      "         [ 0.0230],\n",
      "         [ 0.1281],\n",
      "         [-0.0859],\n",
      "         [ 0.0031],\n",
      "         [ 0.0816],\n",
      "         [ 0.1796],\n",
      "         [ 0.1518],\n",
      "         [ 0.0104],\n",
      "         [ 0.0678],\n",
      "         [ 0.0930],\n",
      "         [-0.2181],\n",
      "         [ 0.1368],\n",
      "         [-0.2477],\n",
      "         [ 0.1228]],\n",
      "\n",
      "        [[ 0.1659],\n",
      "         [ 0.2241],\n",
      "         [ 0.0693],\n",
      "         [-0.1397],\n",
      "         [ 0.0158],\n",
      "         [ 0.0934],\n",
      "         [-0.0601],\n",
      "         [-0.0242],\n",
      "         [ 0.0923],\n",
      "         [ 0.2233],\n",
      "         [-0.2177],\n",
      "         [-0.0019],\n",
      "         [-0.0417],\n",
      "         [ 0.0344],\n",
      "         [-0.0887],\n",
      "         [-0.1301]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.0126,  0.1923], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[ 0.1133],\n",
      "         [-0.0775],\n",
      "         [ 0.0354],\n",
      "         [ 0.0828],\n",
      "         [-0.0018],\n",
      "         [ 0.0528],\n",
      "         [-0.1090],\n",
      "         [-0.0273],\n",
      "         [ 0.0680],\n",
      "         [-0.0451],\n",
      "         [ 0.0464],\n",
      "         [-0.0279],\n",
      "         [-0.0052],\n",
      "         [-0.1227],\n",
      "         [ 0.0612],\n",
      "         [ 0.0025],\n",
      "         [ 0.0632],\n",
      "         [-0.0694],\n",
      "         [ 0.0619],\n",
      "         [ 0.0723],\n",
      "         [ 0.0650],\n",
      "         [-0.0383],\n",
      "         [ 0.0404],\n",
      "         [ 0.0923],\n",
      "         [-0.1025],\n",
      "         [ 0.0178],\n",
      "         [ 0.0407],\n",
      "         [ 0.1165],\n",
      "         [-0.1143],\n",
      "         [-0.0722],\n",
      "         [-0.0002],\n",
      "         [-0.1154],\n",
      "         [ 0.0078],\n",
      "         [-0.0029],\n",
      "         [-0.1047],\n",
      "         [-0.0607],\n",
      "         [-0.0474],\n",
      "         [-0.0205],\n",
      "         [-0.0171],\n",
      "         [ 0.0438],\n",
      "         [-0.0645],\n",
      "         [-0.0716],\n",
      "         [ 0.0413],\n",
      "         [ 0.1075],\n",
      "         [ 0.0665],\n",
      "         [ 0.0518],\n",
      "         [-0.1174],\n",
      "         [ 0.0860],\n",
      "         [-0.0412],\n",
      "         [-0.1204],\n",
      "         [ 0.1241],\n",
      "         [-0.0123],\n",
      "         [ 0.0336],\n",
      "         [-0.0776],\n",
      "         [ 0.0901],\n",
      "         [ 0.1142],\n",
      "         [ 0.0873],\n",
      "         [-0.0109],\n",
      "         [ 0.0002],\n",
      "         [ 0.0092],\n",
      "         [ 0.0714],\n",
      "         [ 0.0203],\n",
      "         [-0.1242],\n",
      "         [-0.0072]],\n",
      "\n",
      "        [[-0.0632],\n",
      "         [-0.0497],\n",
      "         [-0.0601],\n",
      "         [-0.0765],\n",
      "         [-0.1142],\n",
      "         [ 0.0598],\n",
      "         [-0.0431],\n",
      "         [ 0.1021],\n",
      "         [-0.0688],\n",
      "         [ 0.0932],\n",
      "         [-0.0850],\n",
      "         [-0.0369],\n",
      "         [ 0.0057],\n",
      "         [ 0.0400],\n",
      "         [-0.1149],\n",
      "         [ 0.0734],\n",
      "         [ 0.0581],\n",
      "         [-0.0685],\n",
      "         [-0.0518],\n",
      "         [ 0.0848],\n",
      "         [-0.0649],\n",
      "         [-0.0474],\n",
      "         [ 0.0290],\n",
      "         [-0.0639],\n",
      "         [-0.1144],\n",
      "         [ 0.0834],\n",
      "         [ 0.0051],\n",
      "         [ 0.0064],\n",
      "         [-0.1156],\n",
      "         [-0.0282],\n",
      "         [ 0.0912],\n",
      "         [ 0.1055],\n",
      "         [ 0.0948],\n",
      "         [-0.0179],\n",
      "         [-0.0725],\n",
      "         [-0.0226],\n",
      "         [ 0.0670],\n",
      "         [ 0.0226],\n",
      "         [-0.0800],\n",
      "         [ 0.0002],\n",
      "         [-0.0058],\n",
      "         [ 0.0815],\n",
      "         [ 0.0465],\n",
      "         [-0.1053],\n",
      "         [-0.0968],\n",
      "         [ 0.0724],\n",
      "         [-0.1182],\n",
      "         [ 0.0319],\n",
      "         [ 0.0221],\n",
      "         [ 0.1195],\n",
      "         [-0.0930],\n",
      "         [ 0.0189],\n",
      "         [ 0.0101],\n",
      "         [ 0.0868],\n",
      "         [ 0.0828],\n",
      "         [-0.0353],\n",
      "         [ 0.0675],\n",
      "         [-0.0342],\n",
      "         [ 0.0481],\n",
      "         [-0.1110],\n",
      "         [-0.0808],\n",
      "         [-0.0293],\n",
      "         [-0.0951],\n",
      "         [ 0.0969]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk3.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0846, -0.0643], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv1.weight | Size: torch.Size([16, 64, 1]) | Values : tensor([[[ 0.0493],\n",
      "         [ 0.0039],\n",
      "         [ 0.0549],\n",
      "         [-0.0752],\n",
      "         [ 0.0871],\n",
      "         [-0.0892],\n",
      "         [-0.0980],\n",
      "         [ 0.0222],\n",
      "         [-0.0092],\n",
      "         [-0.0527],\n",
      "         [-0.0600],\n",
      "         [-0.0322],\n",
      "         [ 0.0760],\n",
      "         [ 0.0160],\n",
      "         [ 0.0225],\n",
      "         [-0.1043],\n",
      "         [-0.0975],\n",
      "         [-0.0574],\n",
      "         [-0.0755],\n",
      "         [-0.0311],\n",
      "         [ 0.0212],\n",
      "         [-0.0944],\n",
      "         [-0.0065],\n",
      "         [ 0.0820],\n",
      "         [-0.0862],\n",
      "         [-0.0875],\n",
      "         [-0.1018],\n",
      "         [ 0.0797],\n",
      "         [-0.0989],\n",
      "         [ 0.0185],\n",
      "         [-0.0125],\n",
      "         [-0.0832],\n",
      "         [ 0.0975],\n",
      "         [ 0.0242],\n",
      "         [ 0.0901],\n",
      "         [-0.0633],\n",
      "         [ 0.0809],\n",
      "         [ 0.0013],\n",
      "         [-0.0911],\n",
      "         [ 0.1034],\n",
      "         [-0.1000],\n",
      "         [ 0.0218],\n",
      "         [ 0.0238],\n",
      "         [ 0.0296],\n",
      "         [ 0.0059],\n",
      "         [-0.0379],\n",
      "         [-0.0888],\n",
      "         [ 0.0610],\n",
      "         [ 0.1081],\n",
      "         [ 0.0305],\n",
      "         [ 0.0892],\n",
      "         [-0.1069],\n",
      "         [ 0.1128],\n",
      "         [-0.0879],\n",
      "         [ 0.0555],\n",
      "         [ 0.0183],\n",
      "         [ 0.0447],\n",
      "         [ 0.0734],\n",
      "         [ 0.1114],\n",
      "         [-0.1045],\n",
      "         [ 0.0540],\n",
      "         [ 0.0087],\n",
      "         [ 0.1245],\n",
      "         [-0.0698]],\n",
      "\n",
      "        [[-0.0170],\n",
      "         [ 0.1134],\n",
      "         [-0.0043],\n",
      "         [ 0.0540],\n",
      "         [ 0.0797],\n",
      "         [ 0.1041],\n",
      "         [-0.0819],\n",
      "         [ 0.1140],\n",
      "         [-0.0969],\n",
      "         [ 0.1035],\n",
      "         [-0.0271],\n",
      "         [ 0.1199],\n",
      "         [-0.1140],\n",
      "         [-0.0034],\n",
      "         [-0.0130],\n",
      "         [-0.0474],\n",
      "         [-0.0251],\n",
      "         [-0.1018],\n",
      "         [ 0.1161],\n",
      "         [-0.0195],\n",
      "         [-0.0071],\n",
      "         [-0.0042],\n",
      "         [-0.0874],\n",
      "         [-0.1134],\n",
      "         [-0.0607],\n",
      "         [-0.1044],\n",
      "         [-0.0211],\n",
      "         [ 0.1148],\n",
      "         [ 0.0926],\n",
      "         [-0.0434],\n",
      "         [ 0.0341],\n",
      "         [-0.1138],\n",
      "         [ 0.0978],\n",
      "         [ 0.0472],\n",
      "         [-0.1103],\n",
      "         [ 0.0307],\n",
      "         [-0.0117],\n",
      "         [ 0.0195],\n",
      "         [-0.1044],\n",
      "         [ 0.0579],\n",
      "         [-0.0464],\n",
      "         [ 0.1047],\n",
      "         [-0.1027],\n",
      "         [ 0.0610],\n",
      "         [ 0.1063],\n",
      "         [ 0.1095],\n",
      "         [ 0.0539],\n",
      "         [ 0.1126],\n",
      "         [-0.0390],\n",
      "         [ 0.0355],\n",
      "         [-0.0427],\n",
      "         [-0.0032],\n",
      "         [-0.0785],\n",
      "         [-0.0665],\n",
      "         [-0.0945],\n",
      "         [ 0.0036],\n",
      "         [ 0.0665],\n",
      "         [-0.0022],\n",
      "         [-0.1131],\n",
      "         [-0.0409],\n",
      "         [-0.0828],\n",
      "         [-0.0357],\n",
      "         [-0.0430],\n",
      "         [ 0.0651]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv1.bias | Size: torch.Size([16]) | Values : tensor([-0.0098,  0.1073], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn1.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn1.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv2.weight | Size: torch.Size([16, 16, 3]) | Values : tensor([[[-0.0511, -0.1001, -0.0349],\n",
      "         [ 0.0702,  0.0858,  0.0606],\n",
      "         [ 0.0903,  0.1236,  0.0434],\n",
      "         [-0.0247, -0.0130,  0.0797],\n",
      "         [ 0.0347,  0.0967,  0.1423],\n",
      "         [ 0.0300,  0.1402,  0.1154],\n",
      "         [ 0.0294,  0.0168,  0.0257],\n",
      "         [ 0.0627,  0.1429,  0.0554],\n",
      "         [-0.0237,  0.0036, -0.0187],\n",
      "         [ 0.1333, -0.0834,  0.0356],\n",
      "         [-0.1167,  0.1288,  0.0500],\n",
      "         [-0.1103, -0.0080,  0.1359],\n",
      "         [ 0.0219,  0.0616,  0.1135],\n",
      "         [-0.0138, -0.0579, -0.0504],\n",
      "         [-0.1307, -0.0916, -0.0580],\n",
      "         [ 0.0271,  0.0440, -0.0658]],\n",
      "\n",
      "        [[ 0.0383, -0.0948, -0.0473],\n",
      "         [-0.0884,  0.0215,  0.1170],\n",
      "         [-0.0548, -0.1312,  0.1137],\n",
      "         [ 0.1001, -0.1384, -0.0075],\n",
      "         [ 0.1186, -0.0251,  0.0701],\n",
      "         [ 0.0892, -0.0055,  0.1075],\n",
      "         [-0.0522,  0.0225,  0.1035],\n",
      "         [-0.0712, -0.0748, -0.0605],\n",
      "         [ 0.1386,  0.0709, -0.0315],\n",
      "         [-0.1392,  0.1257, -0.1320],\n",
      "         [-0.1151, -0.0576, -0.0011],\n",
      "         [ 0.1427, -0.0295, -0.1388],\n",
      "         [-0.0058, -0.0512,  0.0987],\n",
      "         [-0.0999, -0.1150, -0.1241],\n",
      "         [-0.0427,  0.0448, -0.1138],\n",
      "         [-0.0491,  0.0196, -0.0047]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.1223, -0.0693], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn2.weight | Size: torch.Size([16]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn2.bias | Size: torch.Size([16]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv3.weight | Size: torch.Size([64, 16, 1]) | Values : tensor([[[ 0.1358],\n",
      "         [ 0.2470],\n",
      "         [ 0.0363],\n",
      "         [ 0.1227],\n",
      "         [-0.0496],\n",
      "         [ 0.2379],\n",
      "         [ 0.1994],\n",
      "         [-0.2160],\n",
      "         [ 0.0121],\n",
      "         [ 0.2416],\n",
      "         [-0.0967],\n",
      "         [ 0.1562],\n",
      "         [-0.2020],\n",
      "         [-0.2049],\n",
      "         [-0.2361],\n",
      "         [ 0.0050]],\n",
      "\n",
      "        [[ 0.0351],\n",
      "         [-0.0263],\n",
      "         [-0.2013],\n",
      "         [ 0.2245],\n",
      "         [-0.0738],\n",
      "         [ 0.0206],\n",
      "         [ 0.0629],\n",
      "         [ 0.0038],\n",
      "         [ 0.0051],\n",
      "         [-0.1878],\n",
      "         [ 0.1629],\n",
      "         [ 0.1279],\n",
      "         [-0.2449],\n",
      "         [ 0.1661],\n",
      "         [ 0.2184],\n",
      "         [-0.1073]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.conv3.bias | Size: torch.Size([64]) | Values : tensor([-0.2161, -0.1765], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn3.weight | Size: torch.Size([64]) | Values : tensor([1., 1.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.bn3.bias | Size: torch.Size([64]) | Values : tensor([0., 0.], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.convr.weight | Size: torch.Size([64, 64, 1]) | Values : tensor([[[-0.0860],\n",
      "         [-0.0288],\n",
      "         [-0.1117],\n",
      "         [ 0.0699],\n",
      "         [-0.0235],\n",
      "         [-0.0085],\n",
      "         [ 0.1073],\n",
      "         [ 0.0635],\n",
      "         [-0.0719],\n",
      "         [-0.1220],\n",
      "         [ 0.1203],\n",
      "         [-0.0598],\n",
      "         [-0.0160],\n",
      "         [ 0.0297],\n",
      "         [ 0.1061],\n",
      "         [ 0.0940],\n",
      "         [ 0.0788],\n",
      "         [-0.0646],\n",
      "         [-0.1189],\n",
      "         [ 0.0072],\n",
      "         [ 0.0384],\n",
      "         [-0.1089],\n",
      "         [-0.0013],\n",
      "         [ 0.0478],\n",
      "         [-0.0058],\n",
      "         [-0.0954],\n",
      "         [-0.0203],\n",
      "         [-0.0500],\n",
      "         [-0.0299],\n",
      "         [-0.0425],\n",
      "         [-0.0893],\n",
      "         [ 0.1080],\n",
      "         [ 0.0163],\n",
      "         [ 0.0719],\n",
      "         [ 0.0635],\n",
      "         [ 0.0669],\n",
      "         [-0.0569],\n",
      "         [-0.0833],\n",
      "         [ 0.1133],\n",
      "         [ 0.0077],\n",
      "         [ 0.1196],\n",
      "         [ 0.0520],\n",
      "         [ 0.0086],\n",
      "         [-0.0496],\n",
      "         [-0.0509],\n",
      "         [ 0.0565],\n",
      "         [ 0.1061],\n",
      "         [-0.0032],\n",
      "         [-0.0077],\n",
      "         [ 0.0149],\n",
      "         [ 0.0225],\n",
      "         [-0.1040],\n",
      "         [ 0.0499],\n",
      "         [ 0.1165],\n",
      "         [-0.1227],\n",
      "         [-0.1172],\n",
      "         [-0.0440],\n",
      "         [-0.0605],\n",
      "         [-0.0290],\n",
      "         [-0.0982],\n",
      "         [-0.0580],\n",
      "         [ 0.0589],\n",
      "         [ 0.0997],\n",
      "         [-0.0094]],\n",
      "\n",
      "        [[ 0.0410],\n",
      "         [-0.1079],\n",
      "         [-0.0794],\n",
      "         [ 0.0391],\n",
      "         [-0.1119],\n",
      "         [ 0.0024],\n",
      "         [-0.0875],\n",
      "         [ 0.0901],\n",
      "         [-0.0742],\n",
      "         [-0.1207],\n",
      "         [-0.0246],\n",
      "         [ 0.1168],\n",
      "         [ 0.0445],\n",
      "         [-0.0845],\n",
      "         [ 0.1085],\n",
      "         [-0.0416],\n",
      "         [ 0.1140],\n",
      "         [-0.0648],\n",
      "         [ 0.1072],\n",
      "         [-0.0741],\n",
      "         [-0.0421],\n",
      "         [ 0.1149],\n",
      "         [-0.0258],\n",
      "         [-0.0183],\n",
      "         [-0.0249],\n",
      "         [-0.0054],\n",
      "         [-0.0422],\n",
      "         [ 0.0997],\n",
      "         [-0.0278],\n",
      "         [-0.0542],\n",
      "         [ 0.1128],\n",
      "         [ 0.0234],\n",
      "         [ 0.0504],\n",
      "         [ 0.0472],\n",
      "         [ 0.0613],\n",
      "         [ 0.0482],\n",
      "         [ 0.0206],\n",
      "         [-0.0707],\n",
      "         [ 0.0731],\n",
      "         [ 0.0373],\n",
      "         [ 0.0373],\n",
      "         [-0.0965],\n",
      "         [ 0.1150],\n",
      "         [-0.0200],\n",
      "         [-0.0753],\n",
      "         [ 0.0500],\n",
      "         [ 0.0624],\n",
      "         [-0.0042],\n",
      "         [ 0.1120],\n",
      "         [ 0.0157],\n",
      "         [-0.0104],\n",
      "         [-0.0849],\n",
      "         [-0.1031],\n",
      "         [-0.0890],\n",
      "         [-0.0257],\n",
      "         [-0.0922],\n",
      "         [ 0.0682],\n",
      "         [-0.1112],\n",
      "         [-0.0697],\n",
      "         [-0.1154],\n",
      "         [-0.0144],\n",
      "         [ 0.0078],\n",
      "         [-0.0382],\n",
      "         [-0.0373]]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: convblk4.convr.bias | Size: torch.Size([64]) | Values : tensor([-0.0045,  0.0451], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.1.weight | Size: torch.Size([2, 768]) | Values : tensor([[-0.0352,  0.0061, -0.0174,  ...,  0.0160,  0.0232,  0.0081],\n",
      "        [-0.0326,  0.0080,  0.0165,  ...,  0.0019,  0.0271, -0.0247]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc.1.bias | Size: torch.Size([2]) | Values : tensor([0.0196, 0.0293], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from models.TinyFallNet import TinyFallNet\n",
    "\n",
    "# Create an instance of the model\n",
    "model_tinyFallNet = TinyFallNet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_tinyFallNet.parameters(), lr=learning_rate)\n",
    "# Initialize the scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=patience, verbose=True)\n",
    "\n",
    "print(f\"Model structure: {model_tinyFallNet}\\n\\n\")\n",
    "for name, param in model_tinyFallNet.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.403193  [   64/23290]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.332907  [ 6464/23290]\n",
      "loss: 0.313266  [12864/23290]\n",
      "loss: 0.676014  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338264 \n",
      "\n",
      "Epoch 1:\n",
      "loss: 1.098921  [   64/23290]\n",
      "loss: 0.676019  [ 6464/23290]\n",
      "loss: 0.487121  [12864/23290]\n",
      "loss: 0.313264  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 2:\n",
      "loss: 0.313262  [   64/23290]\n",
      "loss: 0.676017  [ 6464/23290]\n",
      "loss: 0.676019  [12864/23290]\n",
      "loss: 0.879943  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 3:\n",
      "loss: 0.487121  [   64/23290]\n",
      "loss: 0.313262  [ 6464/23290]\n",
      "loss: 1.098907  [12864/23290]\n",
      "loss: 0.676015  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 4:\n",
      "loss: 0.487121  [   64/23290]\n",
      "loss: 1.332905  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 1.098907  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 5:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 1.332905  [ 6464/23290]\n",
      "loss: 0.487121  [12864/23290]\n",
      "loss: 0.676015  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 6:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 0.487121  [ 6464/23290]\n",
      "loss: 0.879943  [12864/23290]\n",
      "loss: 0.487121  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 7:\n",
      "loss: 0.879943  [   64/23290]\n",
      "loss: 0.487121  [ 6464/23290]\n",
      "loss: 0.487121  [12864/23290]\n",
      "loss: 0.676014  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 8:\n",
      "loss: 0.879943  [   64/23290]\n",
      "loss: 0.676014  [ 6464/23290]\n",
      "loss: 0.487121  [12864/23290]\n",
      "loss: 0.879943  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 9:\n",
      "loss: 0.879943  [   64/23290]\n",
      "loss: 1.098907  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 0.676014  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 10:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 0.487120  [ 6464/23290]\n",
      "loss: 1.332905  [12864/23290]\n",
      "loss: 0.676014  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 11:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 0.676014  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 1.332905  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 12:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 1.098907  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 0.487121  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 13:\n",
      "loss: 0.487121  [   64/23290]\n",
      "loss: 0.487121  [ 6464/23290]\n",
      "loss: 0.313262  [12864/23290]\n",
      "loss: 1.098907  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 14:\n",
      "loss: 0.879943  [   64/23290]\n",
      "loss: 0.879944  [ 6464/23290]\n",
      "loss: 0.879943  [12864/23290]\n",
      "loss: 1.098907  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 15:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 0.487121  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 0.313262  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 16:\n",
      "loss: 0.879943  [   64/23290]\n",
      "loss: 0.676014  [ 6464/23290]\n",
      "loss: 1.581939  [12864/23290]\n",
      "loss: 1.098907  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 17:\n",
      "loss: 0.487121  [   64/23290]\n",
      "loss: 0.879943  [ 6464/23290]\n",
      "loss: 0.676014  [12864/23290]\n",
      "loss: 1.332905  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 18:\n",
      "loss: 0.313262  [   64/23290]\n",
      "loss: 1.581939  [ 6464/23290]\n",
      "loss: 0.879943  [12864/23290]\n",
      "loss: 0.879943  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch 19:\n",
      "loss: 0.676014  [   64/23290]\n",
      "loss: 0.879943  [ 6464/23290]\n",
      "loss: 0.487121  [12864/23290]\n",
      "loss: 1.332906  [19264/23290]\n",
      "Validation Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.338262 \n",
      "\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, model_tinyFallNet, loss_fn, optimizer,val_dataloader, \n",
    "        patience=patience, scheduler=scheduler, device=device, epochs=epochs, B_size=B_size, A_size=A_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.813262 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.500\n",
      " specificity\t0.000\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.500\n",
      " specificity\t1.000\n",
      " sensitivity\t0.000\n",
      "[[31  0]\n",
      " [31  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final test\n",
    "test(test_dataloader, model_tinyFallNet, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.322582 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.903\n",
      " specificity\t0.806\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.903\n",
      " specificity\t1.000\n",
      " sensitivity\t0.806\n",
      "[[31  0]\n",
      " [ 6 25]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ConvLSTM\n",
    "print('ConvLSTM')\n",
    "test(test_dataloader, model_ConvLSTM, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet24\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.835674 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.839\n",
      " specificity\t0.677\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.839\n",
      " specificity\t1.000\n",
      " sensitivity\t0.677\n",
      "[[31  0]\n",
      " [10 21]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ResNet24\n",
    "print('ResNet24')\n",
    "test(test_dataloader, model_resnet, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyFallNet\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.813262 \n",
      "\n",
      " Label 0\n",
      "    accuracy\t0.500\n",
      " specificity\t0.000\n",
      " sensitivity\t1.000\n",
      " Label 1\n",
      "    accuracy\t0.500\n",
      " specificity\t1.000\n",
      " sensitivity\t0.000\n",
      "[[31  0]\n",
      " [31  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TinyFallNet\n",
    "print('TinyFallNet')\n",
    "test(test_dataloader, model_tinyFallNet, loss_fn, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

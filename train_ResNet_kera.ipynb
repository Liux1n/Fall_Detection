{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 20:14:49.714391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 20:14:49.714459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 20:14:49.715534: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 20:14:49.722582: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 20:14:50.515692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "# import torch\n",
    "#from torch import nn\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from utils import train, test\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, layers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, BatchNormalization, ReLU, Softmax, MaxPooling1D, LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "#sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "# sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "# label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.4\n",
    "num_window_fall_data = 50\n",
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  9\n",
      "data.shape:  (25595, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "in_channels = data.shape[2]\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_size:  25020\n",
      "A_size:  575\n",
      "data.shape:  [ 2.46000000e-01 -1.01900000e+00 -4.40000000e-02  3.33461556e+01\n",
      "  4.35448080e+00  2.16005166e+01  8.58061901e+01  8.07870780e-01\n",
      " -1.70919101e+02]\n",
      "(250, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "label = label.astype(np.int64)\n",
    "# one-hot encoding\n",
    "#label = to_categorical(label, num_classes=2)\n",
    "# transpose the data to (batch_size, sequence_length, in_channels)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "# transpose the data to (batch_size, in_channels, sequence_length)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "print('data.shape: ', data[0][0])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# device = (\n",
    "#      \"cuda\"\n",
    "#      if torch.cuda.is_available()\n",
    "#      else \"cpu\"\n",
    "#  )\n",
    "# #device = \"cpu\"\n",
    "# print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv2D(filters=16, kernel_size=(1, 1))\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.conv2 = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "        self.conv3 = layers.Conv2D(filters=64, kernel_size=(1, 1))\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu3 = layers.ReLU()\n",
    "        self.convr = layers.Conv2D(filters=64, kernel_size=(1, 1))\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = self.convr(inputs)\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.add([x, residual])\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "\n",
    "class IdentityBlock(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv2D(filters=16, kernel_size=(1, 1))\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.conv2 = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "        self.conv3 = layers.Conv2D(filters=64, kernel_size=(1, 1))\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu3 = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.add([x, residual])\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "\n",
    "# class ResNet(keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.reshape = layers.Reshape((1, 50, 9))\n",
    "#         self.conv1 = layers.Conv2D(filters=64, kernel_size=(1, 3))\n",
    "#         self.conv2 = layers.Conv2D(filters=64, kernel_size=(1, 3))\n",
    "#         self.pool1 = layers.MaxPooling2D(pool_size=(1, 2))\n",
    "        \n",
    "#         self.convblk1 = ConvBlock()\n",
    "#         self.convblk2 = ConvBlock()\n",
    "#         self.identityblk1 = IdentityBlock()\n",
    "#         self.convblk3 = ConvBlock()\n",
    "#         self.identityblk2 = IdentityBlock()\n",
    "#         self.convblk4 = ConvBlock()\n",
    "#         self.identityblk3 = IdentityBlock()\n",
    "        \n",
    "#         self.pool2 = layers.AveragePooling2D(pool_size=(1, 2))\n",
    "#         self.flatten = layers.Flatten()      \n",
    "#         self.fc = layers.Dense(2, activation='softmax')  \n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.reshape(inputs)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.convblk1(x)\n",
    "#         x = self.convblk2(x)\n",
    "#         x = self.identityblk1(x)\n",
    "#         x = self.convblk3(x)\n",
    "#         x = self.identityblk2(x)\n",
    "#         x = self.convblk4(x)\n",
    "#         x = self.identityblk3(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "inputs = keras.Input(shape=(50, 9))\n",
    "x = layers.Reshape((1, 50, 9))(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.MaxPooling2D(pool_size=(1, 2))(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.AveragePooling2D(pool_size=(1, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "ResNet24 = keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet24\")\n",
    "\n",
    "# ResNet24 = models.Sequential([\n",
    "#     layers.Reshape((1, 50, 9), input_shape=(50, 9)),\n",
    "#     layers.Conv2D(filters=64, kernel_size=(1, 3)),\n",
    "#     layers.Conv2D(filters=64, kernel_size=(1, 3)),\n",
    "#     layers.MaxPooling2D(pool_size=(1, 2)),\n",
    "#     ConvBlock(),\n",
    "#     ConvBlock(),\n",
    "#     IdentityBlock(),\n",
    "#     ConvBlock(),\n",
    "#     IdentityBlock(),\n",
    "#     ConvBlock(),\n",
    "#     IdentityBlock(),\n",
    "#     layers.AveragePooling2D(pool_size=(1, 2)),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(2, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# ResNet24 = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "\n",
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)         (None, 1, 50, 9)             0         ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)         (None, 1, 48, 64)            1792      ['reshape_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)         (None, 1, 46, 64)            12352     ['conv2d_135[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 1, 23, 64)            0         ['conv2d_136[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)         (None, 1, 23, 16)            1040      ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_126 (B  (None, 1, 23, 16)            64        ['conv2d_138[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_126 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_126[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_126[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_127 (B  (None, 1, 23, 16)            64        ['conv2d_139[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_127 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_127[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_127[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_128 (B  (None, 1, 23, 64)            256       ['conv2d_140[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)         (None, 1, 23, 64)            4160      ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " add_42 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_128[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_137[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_128 (ReLU)            (None, 1, 23, 64)            0         ['add_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_128[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_129 (B  (None, 1, 23, 16)            64        ['conv2d_142[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_129 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_129[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_129[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_130 (B  (None, 1, 23, 16)            64        ['conv2d_143[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_130 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_130[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_130[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_131 (B  (None, 1, 23, 64)            256       ['conv2d_144[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)         (None, 1, 23, 64)            4160      ['re_lu_128[0][0]']           \n",
      "                                                                                                  \n",
      " add_43 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_131[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_141[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_131 (ReLU)            (None, 1, 23, 64)            0         ['add_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_131[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_132 (B  (None, 1, 23, 16)            64        ['conv2d_145[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_132 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_132[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_132[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_133 (B  (None, 1, 23, 16)            64        ['conv2d_146[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_133 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_133[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_133[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_134 (B  (None, 1, 23, 64)            256       ['conv2d_147[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_44 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_134[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     're_lu_131[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_134 (ReLU)            (None, 1, 23, 64)            0         ['add_44[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_134[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_135 (B  (None, 1, 23, 16)            64        ['conv2d_149[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_135 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_135[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_135[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_136 (B  (None, 1, 23, 16)            64        ['conv2d_150[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_136 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_136[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_136[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_137 (B  (None, 1, 23, 64)            256       ['conv2d_151[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)         (None, 1, 23, 64)            4160      ['re_lu_134[0][0]']           \n",
      "                                                                                                  \n",
      " add_45 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_137[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_148[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_137 (ReLU)            (None, 1, 23, 64)            0         ['add_45[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_137[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_138 (B  (None, 1, 23, 16)            64        ['conv2d_152[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_138 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_138[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_138[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_139 (B  (None, 1, 23, 16)            64        ['conv2d_153[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_139 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_139[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_139[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_140 (B  (None, 1, 23, 64)            256       ['conv2d_154[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_46 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_140[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     're_lu_137[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_140 (ReLU)            (None, 1, 23, 64)            0         ['add_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_140[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_141 (B  (None, 1, 23, 16)            64        ['conv2d_156[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_141 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_141[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_141[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_142 (B  (None, 1, 23, 16)            64        ['conv2d_157[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_142 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_142[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_142[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_143 (B  (None, 1, 23, 64)            256       ['conv2d_158[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)         (None, 1, 23, 64)            4160      ['re_lu_140[0][0]']           \n",
      "                                                                                                  \n",
      " add_47 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_143[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'conv2d_155[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_143 (ReLU)            (None, 1, 23, 64)            0         ['add_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)         (None, 1, 23, 16)            1040      ['re_lu_143[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_144 (B  (None, 1, 23, 16)            64        ['conv2d_159[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_144 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_144[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)         (None, 1, 23, 16)            784       ['re_lu_144[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_145 (B  (None, 1, 23, 16)            64        ['conv2d_160[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_145 (ReLU)            (None, 1, 23, 16)            0         ['batch_normalization_145[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)         (None, 1, 23, 64)            1088      ['re_lu_145[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_146 (B  (None, 1, 23, 64)            256       ['conv2d_161[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_48 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_146[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     're_lu_143[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_146 (ReLU)            (None, 1, 23, 64)            0         ['add_48[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (Avera  (None, 1, 11, 64)            0         ['re_lu_146[0][0]']           \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)         (None, 704)                  0         ['average_pooling2d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 2)                    1410      ['flatten_6[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55266 (215.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 1344 (5.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' for layer in ConvLSTM.layers:\\n    weights = layer.get_weights()\\n    print(f\"Layer: {layer.name} | Weights: {weights} \\n\") '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "# Create an instance of the model\n",
    "#model_ConvLSTM = ConvLSTM().to(device)\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model_ConvLSTM.parameters(), lr=learning_rate)\n",
    "# Initialize the scheduler\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=patience, verbose=True)\n",
    "print(f\"Model structure: \\n\")\n",
    "ResNet24.build(input_shape=(None, 50, 9))\n",
    "ResNet24.summary()\n",
    "\n",
    "\"\"\" for layer in ConvLSTM.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(f\"Layer: {layer.name} | Weights: {weights} \\n\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16380, 2)\n",
      "y_val.shape:  (4096, 2)\n",
      "X_train.shape:  (16380, 50, 9)\n",
      "y_train.shape:  (16380, 2)\n",
      "256/256 [==============================] - 18s 33ms/step - loss: 1.4456 - accuracy: 0.8036 - val_loss: 0.3097 - val_accuracy: 0.9043 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train(train_dataloader, model_ConvLSTM, loss_fn, optimizer,val_dataloader, \n",
    "           patience=patience, scheduler=scheduler, epochs=epochs, device=device, B_size=B_size, A_size=A_size) \"\"\"\n",
    "# Train the model\n",
    "# Train the model without using batches\n",
    "# Compile the model\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "\n",
    "ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = ResNet24.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (250, 50, 9)\n",
      "8/8 - 1s - loss: 0.3928 - accuracy: 0.8320 - 564ms/epoch - 70ms/step\n",
      "Test loss: [0.39284422993659973, 0.8320000171661377]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = ResNet24.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6a250c1390>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for the quantized model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "function: plot_confusion_matrix\n",
    "    - input: cm, classes, normalize, title, cmap\n",
    "    - output: none\n",
    "    - description: plots the confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 6ms/step\n",
      "[[111  11]\n",
      " [ 31  97]]\n",
      "Confusion matrix, without normalization\n",
      "[[111  11]\n",
      " [ 31  97]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ResNet24.predict(X_test), axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet24.save('ResNet24.keras')  # The file needs to end with the .keras extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLa  (None, 50, 9)                3         ['input_3[0][0]']             \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " quant_reshape_5 (QuantizeW  (None, 1, 50, 9)             1         ['quantize_layer[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_135 (Quantize  (None, 1, 48, 64)            1923      ['quant_reshape_5[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_conv2d_136 (Quantize  (None, 1, 46, 64)            12483     ['quant_conv2d_135[0][0]']    \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_5 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_136[0][0]']    \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_138 (Quantize  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_5[0][0]'\n",
      " WrapperV2)                                                         ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_138[0][0]']    \n",
      " 126 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_126 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " rapperV2)                                                          6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_139 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_126[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_139[0][0]']    \n",
      " 127 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_127 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " rapperV2)                                                          7[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_140 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_127[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_140[0][0]']    \n",
      " 128 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_137 (Quantize  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_5[0][0]'\n",
      " WrapperV2)                                                         ]                             \n",
      "                                                                                                  \n",
      " quant_add_42 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_12\n",
      " perV2)                                                             8[0][0]',                     \n",
      "                                                                     'quant_conv2d_137[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_128 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_42[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_142 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_128[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_142[0][0]']    \n",
      " 129 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_129 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " rapperV2)                                                          9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_143 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_129[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_143[0][0]']    \n",
      " 130 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_130 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          0[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_144 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_130[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_144[0][0]']    \n",
      " 131 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_141 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_128[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_43 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_13\n",
      " perV2)                                                             1[0][0]',                     \n",
      "                                                                     'quant_conv2d_141[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_131 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_43[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_145 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_131[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_145[0][0]']    \n",
      " 132 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_132 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_146 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_132[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_146[0][0]']    \n",
      " 133 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_133 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          3[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_147 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_133[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_147[0][0]']    \n",
      " 134 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_add_44 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_13\n",
      " perV2)                                                             4[0][0]',                     \n",
      "                                                                     'quant_re_lu_131[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_134 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_44[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_149 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_134[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_149[0][0]']    \n",
      " 135 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_135 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_150 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_135[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_150[0][0]']    \n",
      " 136 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_136 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          6[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_151 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_136[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_151[0][0]']    \n",
      " 137 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_148 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_134[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_45 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_13\n",
      " perV2)                                                             7[0][0]',                     \n",
      "                                                                     'quant_conv2d_148[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_137 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_45[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_152 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_137[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_152[0][0]']    \n",
      " 138 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_138 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          8[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_153 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_138[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_153[0][0]']    \n",
      " 139 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_139 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " rapperV2)                                                          9[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_154 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_139[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_154[0][0]']    \n",
      " 140 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_add_46 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " perV2)                                                             0[0][0]',                     \n",
      "                                                                     'quant_re_lu_137[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_140 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_46[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_156 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_140[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_156[0][0]']    \n",
      " 141 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_141 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_14\n",
      " rapperV2)                                                          1[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_157 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_141[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_157[0][0]']    \n",
      " 142 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_142 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_14\n",
      " rapperV2)                                                          2[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_158 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_142[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_158[0][0]']    \n",
      " 143 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_155 (Quantize  (None, 1, 23, 64)            4291      ['quant_re_lu_140[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_add_47 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " perV2)                                                             3[0][0]',                     \n",
      "                                                                     'quant_conv2d_155[0][0]']    \n",
      "                                                                                                  \n",
      " quant_re_lu_143 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_47[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_159 (Quantize  (None, 1, 23, 16)            1073      ['quant_re_lu_143[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_159[0][0]']    \n",
      " 144 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_144 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_14\n",
      " rapperV2)                                                          4[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_160 (Quantize  (None, 1, 23, 16)            817       ['quant_re_lu_144[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_160[0][0]']    \n",
      " 145 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_re_lu_145 (QuantizeW  (None, 1, 23, 16)            3         ['quant_batch_normalization_14\n",
      " rapperV2)                                                          5[0][0]']                     \n",
      "                                                                                                  \n",
      " quant_conv2d_161 (Quantize  (None, 1, 23, 64)            1217      ['quant_re_lu_145[0][0]']     \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_161[0][0]']    \n",
      " 146 (QuantizeWrapperV2)                                                                          \n",
      "                                                                                                  \n",
      " quant_add_48 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " perV2)                                                             6[0][0]',                     \n",
      "                                                                     'quant_re_lu_143[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_146 (QuantizeW  (None, 1, 23, 64)            3         ['quant_add_48[0][0]']        \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_5   (None, 1, 11, 64)            3         ['quant_re_lu_146[0][0]']     \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_6 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_5[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_6 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_6[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "# with quantize_scope(\n",
    "#   {'DefaultLSTMQuantizeConfig': DefaultLSTMQuantizeConfig,\n",
    "#    'CustomLSTM': CustomLSTM}):\n",
    "#   # Use `quantize_apply` to actually make the model quantization aware.\n",
    "#   q_ConvLSTM = tfmot.quantization.keras.quantize_model(ConvLSTM)\n",
    "  \n",
    "q_ResNet24 = tfmot.quantization.keras.quantize_model(ResNet24)\n",
    "q_ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_ResNet24.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "256/256 [==============================] - 47s 135ms/step - loss: 1.0210 - accuracy: 0.7920 - val_loss: 0.1912 - val_accuracy: 0.9321 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 33s 131ms/step - loss: 0.7643 - accuracy: 0.8435 - val_loss: 0.4056 - val_accuracy: 0.8528 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 34s 133ms/step - loss: 0.6812 - accuracy: 0.8577 - val_loss: 1.4528 - val_accuracy: 0.4041 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 34s 132ms/step - loss: 0.6362 - accuracy: 0.8704 - val_loss: 0.3331 - val_accuracy: 0.8674 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 34s 133ms/step - loss: 0.6195 - accuracy: 0.8697 - val_loss: 0.2913 - val_accuracy: 0.8872 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8717\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "256/256 [==============================] - 34s 134ms/step - loss: 0.5825 - accuracy: 0.8717 - val_loss: 0.8917 - val_accuracy: 0.6592 - lr: 5.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_ResNet24.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ResNet24.save('q_ResNet24.keras')  # The file needs to end with the .keras extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4845fg38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4845fg38/assets\n",
      "/home/liyinrong/miniconda3/envs/tensorflow-dev/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-07 21:25:25.443197: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-07 21:25:25.443744: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-07 21:25:25.450805: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp4845fg38\n",
      "2023-12-07 21:25:25.486415: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-07 21:25:25.486446: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp4845fg38\n",
      "2023-12-07 21:25:25.549683: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2023-12-07 21:25:25.585922: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-07 21:25:26.485600: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp4845fg38\n",
      "2023-12-07 21:25:26.717599: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1266786 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109472"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"./q_ResNet24.tflite\", \"wb\").write(quantized_tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import torch\n",
    "#from torch import nn\n",
    "#from torch.utils.data import DataLoader\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import train, test, plot_confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, Dropout, Dense\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import flatten\n",
    "from tensorflow.keras.layers import Flatten, Softmax\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mac\n",
    "sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "#sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "#sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.4\n",
    "num_window_fall_data = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n"
     ]
    }
   ],
   "source": [
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels:  9\n",
      "data.shape:  (25617, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "in_channels = 9\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_size:  25020\n",
      "A_size:  597\n",
      "data:  [-1.09000000e-01 -3.40000000e-02  1.01200000e+00  4.01070600e-01\n",
      "  0.00000000e+00  0.00000000e+00  1.77662817e+02 -6.10773228e+00\n",
      " -2.71238317e+01]\n",
      "(240, 50, 9)\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "label = label.astype(np.int64)\n",
    "# one-hot encoding\n",
    "#label = to_categorical(label, num_classes=2)\n",
    "# transpose the data to (batch_size, sequence_length, in_channels)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "# normalize the data\n",
    "# Initialize a new scaling object for normalizing input data\n",
    "# Z-score normalization\n",
    "\n",
    "\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "# transpose the data to (batch_size, in_channels, sequence_length)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "print('data: ', data[0][0])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "device = (\n",
    "     \"cuda\"\n",
    "     if torch.cuda.is_available()\n",
    "     else \"cpu\"\n",
    " )\n",
    "#device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conv1D-3 Block as a function to avoid repetition\n",
    "def conv1d_3_block(filters, kernel_size, pool_size, pool_strides):\n",
    "    block = Sequential([\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size,padding='same'),\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "        ReLU(),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=pool_size, strides=pool_strides)\n",
    "    ])\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvLSTM = Sequential([\n",
    "    # The Conv1D-3 Block is repeated three times\n",
    "    conv1d_3_block(filters=64, kernel_size=3, pool_size=2, pool_strides=2),\n",
    "    conv1d_3_block(filters=64, kernel_size=3, pool_size=2, pool_strides=2),\n",
    "    conv1d_3_block(filters=64, kernel_size=3, pool_size=2, pool_strides=2),\n",
    "    # Followed by two LSTM layers\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Flatten the output to feed into the Dense layers\n",
    "    Flatten(),\n",
    "    # Two Dense layers with ReLU activation\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ConvLSTM = Sequential([\n",
    "    # Conv1\n",
    "    Conv1D(filters=64, kernel_size=3, padding='same', input_shape=(50, 9)),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    # Conv2\n",
    "    Conv1D(filters=64, kernel_size=3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    # Conv3\n",
    "    Conv1D(filters=64, kernel_size=3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    # LSTM layers\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    # flatten\n",
    "    Flatten(),\n",
    "    # Fully connected layer\n",
    "    Dense(32),\n",
    "    # relu\n",
    "    ReLU(),\n",
    "    # softmax\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train(train_dataloader, model_ConvLSTM, loss_fn, optimizer,val_dataloader, \n",
    "           patience=patience, scheduler=scheduler, epochs=epochs, device=device, B_size=B_size, A_size=A_size) \"\"\"\n",
    "# Train the model\n",
    "# Train the model without using batches\n",
    "# Compile the model\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "\n",
    "ConvLSTM.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = ConvLSTM.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the model\n",
    "ConvLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = ConvLSTM.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for the quantized model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "function: plot_confusion_matrix\n",
    "    - input: cm, classes, normalize, title, cmap\n",
    "    - output: none\n",
    "    - description: plots the confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ConvLSTM.predict(X_test), axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvLSTM.save('ConvLSTM_baseline.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "ConvLSTM = load_model('ConvLSTM_baseline.h5')\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ConvLSTM.predict(X_test), axis=-1)\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the model to TF lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite without quantization\n",
    "# tried to larger set of Tensorflow ops\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ConvLSTM)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "ConvLSTM_tflite_model = converter.convert()\n",
    "# save the model\n",
    "# Save the model to disk\n",
    "open(\"ConvLSTM_tflite.tflite\", \"wb\").write(ConvLSTM_tflite_model)\n",
    "#ConvLSTM_tflite_model.save('ConvLSTM_tflite.tflite')\n",
    "# Show the model size for the non-quantized HDF5 model\n",
    "ConvLSTM_h5_in_kb = os.path.getsize('ConvLSTM_baseline.h5') / 1024\n",
    "print(\"HDF5 Model size without quantization: %d KB\" % ConvLSTM_h5_in_kb)\n",
    "\n",
    "# Show the model size for the non-quantized TFLite model\n",
    "ConvLSTM_tflite_in_kb = os.path.getsize('ConvLSTM_tflite.tflite') / 1024\n",
    "print(\"TFLite Model size without quantization: %d KB\" % ConvLSTM_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in file size by a factor of %f\" % (ConvLSTM_h5_in_kb / ConvLSTM_tflite_in_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF lite dynamic range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ConvLSTM = load_model('ConvLSTM_baseline.h5')\n",
    "\n",
    "# Convert the model to TFLite with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ConvLSTM)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "dynR_quant_tflite_ConvLSTM = converter.convert()\n",
    "\n",
    "# save the quantized model\n",
    "with open('dynR_quant_tflite_ConvLSTM.tflite', 'wb') as f:\n",
    "    f.write(dynR_quant_tflite_ConvLSTM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=dynR_quant_tflite_ConvLSTM)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model size for the dynamic ranged quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('dynR_quant_tflite_ConvLSTM.tflite') / 1024\n",
    "print(\"TFLite Model size with dynamic quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % ConvLSTM_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (ConvLSTM_tflite_in_kb / tflite_quant_in_kb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# int8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(ConvLSTM)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "with open('tflite_model_quant_int8_ConvLSTM.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_quant_int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('tflite_model_quant_int8_ConvLSTM.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % ConvLSTM_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (ConvLSTM_tflite_in_kb / tflite_quant_in_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the quantized model\n",
    "X_test_int8 = X_test.astype('uint8')\n",
    "y_test_int8 = y_test.astype('uint8')\n",
    "#print(y_test_int8)\n",
    "# now y_test_int8 is one-hot encoded. convert y_test_int8 back\n",
    "#y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "print(X_test_int8.shape)\n",
    "print(y_test_int8.shape)\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= tflite_model_quant_int8)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "# convert the predictions from one hot back to int\n",
    "\n",
    "print(predictions.shape)\n",
    "y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "#print(np.argmax(y_test_int8, axis=-1))\n",
    "print(y_test_int8.shape)\n",
    "accuracy = (predictions == y_test_int8).mean()\n",
    "print('accuracy: ', accuracy)\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_int8, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the quantized model\n",
    "X_test_int8 = X_test.astype('float32')\n",
    "y_test_int8 = y_test.astype('float32')\n",
    "#print(y_test_int8)\n",
    "# now y_test_int8 is one-hot encoded. convert y_test_int8 back\n",
    "#y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "print(X_test_int8.shape)\n",
    "print(y_test_int8.shape)\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= dynR_quant_tflite_ConvLSTM)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "# convert the predictions from one hot back to int\n",
    "\n",
    "print(predictions.shape)\n",
    "y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "#print(np.argmax(y_test_int8, axis=-1))\n",
    "print(y_test_int8.shape)\n",
    "accuracy = (predictions == y_test_int8).mean()\n",
    "print('accuracy: ', accuracy)\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_int8, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(50, 9))\n",
    "x = layers.Reshape((1, 50, 9))(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 3))(x)\n",
    "x = layers.MaxPooling2D(pool_size=(1, 2))(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# ConvBlock\n",
    "residual = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# IdentityBlock\n",
    "residual = x\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=(1, 3), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=(1, 1))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Add()([x, residual])\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = layers.AveragePooling2D(pool_size=(1, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "ResNet24 = keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "\n",
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 50, 9)             0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 1, 48, 64)            1792      ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 1, 46, 64)            12352     ['conv2d_27[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 23, 64)            0         ['conv2d_28[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 1, 23, 16)            1040      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 1, 23, 16)            64        ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_21[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 1, 23, 16)            64        ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_22[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 1, 23, 64)            256       ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 1, 23, 64)            4160      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_23[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_29[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)             (None, 1, 23, 64)            0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_23[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 1, 23, 16)            64        ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_24[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 1, 23, 16)            64        ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_25[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 1, 23, 64)            256       ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_23[0][0]']            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_26[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_33[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)             (None, 1, 23, 64)            0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_26[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 1, 23, 16)            64        ['conv2d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_27[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 1, 23, 16)            64        ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_28[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 1, 23, 64)            256       ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_29[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_26[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)             (None, 1, 23, 64)            0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_29[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 1, 23, 16)            64        ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_30[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 1, 23, 16)            64        ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_31[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 1, 23, 64)            256       ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_29[0][0]']            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_32[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_40[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)             (None, 1, 23, 64)            0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_32[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 1, 23, 16)            64        ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_33[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 1, 23, 16)            64        ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_34[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 1, 23, 64)            256       ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_35[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_32[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)             (None, 1, 23, 64)            0         ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_35[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 1, 23, 16)            64        ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_36[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 1, 23, 16)            64        ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_37[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 1, 23, 64)            256       ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_35[0][0]']            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_47[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)             (None, 1, 23, 64)            0         ['add_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 1, 23, 16)            64        ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_39[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 1, 23, 16)            64        ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_40[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 1, 23, 64)            256       ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 1, 23, 64)            0         ['batch_normalization_41[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)             (None, 1, 23, 64)            0         ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (Avera  (None, 1, 11, 64)            0         ['re_lu_41[0][0]']            \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 704)                  0         ['average_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2)                    1410      ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55266 (215.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 1344 (5.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: \\n\")\n",
    "ResNet24.build(input_shape=(None, 50, 9))\n",
    "ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16394, 2)\n",
      "y_val.shape:  (4099, 2)\n",
      "X_train.shape:  (16394, 50, 9)\n",
      "y_train.shape:  (16394, 2)\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 9s 25ms/step - loss: 1.4842 - accuracy: 0.8094 - val_loss: 0.4124 - val_accuracy: 0.8922 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 7s 26ms/step - loss: 0.9538 - accuracy: 0.8587 - val_loss: 0.2678 - val_accuracy: 0.9314 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 6s 23ms/step - loss: 0.8049 - accuracy: 0.8643 - val_loss: 0.3755 - val_accuracy: 0.8931 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.5980 - accuracy: 0.8915 - val_loss: 0.2925 - val_accuracy: 0.9097 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.5557 - accuracy: 0.8840 - val_loss: 0.3996 - val_accuracy: 0.8890 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 6s 24ms/step - loss: 0.5148 - accuracy: 0.9001 - val_loss: 0.7061 - val_accuracy: 0.8353 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.7976 - accuracy: 0.8794\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 6s 25ms/step - loss: 0.7976 - accuracy: 0.8794 - val_loss: 0.2895 - val_accuracy: 0.9293 - lr: 5.0000e-04\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "\n",
    "ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = ResNet24.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (240, 50, 9)\n",
      "8/8 - 0s - loss: 0.2289 - accuracy: 0.9250 - 45ms/epoch - 6ms/step\n",
      "Test loss: [0.22887654602527618, 0.925000011920929]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = ResNet24.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x32f34dd30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "[[113   3]\n",
      " [ 15 109]]\n",
      "Confusion matrix, without normalization\n",
      "[[113   3]\n",
      " [ 15 109]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(ResNet24.predict(X_test), axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet24.save('ResNet24.keras')  # The file needs to end with the .keras extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_1 (Quantize  (None, 50, 9)                3         ['input_2[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape_1 (QuantizeW  (None, 1, 50, 9)             1         ['quantize_layer_1[0][0]']    \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_27 (QuantizeW  (None, 1, 48, 64)            1923      ['quant_reshape_1[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_conv2d_28 (QuantizeW  (None, 1, 46, 64)            12483     ['quant_conv2d_27[0][0]']     \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_max_pooling2d_1 (Qua  (None, 1, 23, 64)            1         ['quant_conv2d_28[0][0]']     \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_conv2d_30 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_30[0][0]']     \n",
      " 21 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_21 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_21\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_31 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_21[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_31[0][0]']     \n",
      " 22 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_22 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_22\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_32 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_22[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_32[0][0]']     \n",
      " 23 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_29 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_max_pooling2d_1[0][0]'\n",
      " rapperV2)                                                          ]                             \n",
      "                                                                                                  \n",
      " quant_add_7 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_23\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_29[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_23 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_7[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_34 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_23[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_34[0][0]']     \n",
      " 24 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_24 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_24\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_35 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_24[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_35[0][0]']     \n",
      " 25 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_25 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_25\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_36 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_25[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_36[0][0]']     \n",
      " 26 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_33 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_23[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_8 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_26\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_33[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_26 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_8[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_37 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_26[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_37[0][0]']     \n",
      " 27 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_27 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_27\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_38 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_27[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_38[0][0]']     \n",
      " 28 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_28 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_28\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_39 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_28[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_39[0][0]']     \n",
      " 29 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_9 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_29\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_26[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_29 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_9[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_41 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_29[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_41[0][0]']     \n",
      " 30 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_30 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_30\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_42 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_30[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_42[0][0]']     \n",
      " 31 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_31 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_31\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_43 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_31[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_43[0][0]']     \n",
      " 32 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_40 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_29[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_10 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_32\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_40[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_32 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_10[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_44 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_32[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_44[0][0]']     \n",
      " 33 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_33 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_33\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_45 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_33[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_45[0][0]']     \n",
      " 34 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_34 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_34\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_46 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_34[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_46[0][0]']     \n",
      " 35 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_11 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_35\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_32[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_35 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_11[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_48 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_35[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_48[0][0]']     \n",
      " 36 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_36 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_36\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_49 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_36[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_49[0][0]']     \n",
      " 37 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_37 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_37\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_50 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_37[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_50[0][0]']     \n",
      " 38 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_47 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_35[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_12 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_38\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_conv2d_47[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_38 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_12[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_51 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_38[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_51[0][0]']     \n",
      " 39 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_39 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_39\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_52 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_39[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_52[0][0]']     \n",
      " 40 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_40 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_40\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_53 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_40[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_53[0][0]']     \n",
      " 41 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_13 (QuantizeWrap  (None, 1, 23, 64)            1         ['quant_batch_normalization_41\n",
      " perV2)                                                             [0][0]',                      \n",
      "                                                                     'quant_re_lu_38[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_41 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_13[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d_1   (None, 1, 11, 64)            3         ['quant_re_lu_41[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_flatten_1 (QuantizeW  (None, 704)                  1         ['quant_average_pooling2d_1[0]\n",
      " rapperV2)                                                          [0]']                         \n",
      "                                                                                                  \n",
      " quant_dense_1 (QuantizeWra  (None, 2)                    1415      ['quant_flatten_1[0][0]']     \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_ResNet24 = tfmot.quantization.keras.quantize_model(ResNet24)\n",
    "q_ResNet24.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_ResNet24.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 13s 36ms/step - loss: 0.9425 - accuracy: 0.7927 - val_loss: 0.2858 - val_accuracy: 0.9158 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 9s 37ms/step - loss: 0.7741 - accuracy: 0.8362 - val_loss: 0.2423 - val_accuracy: 0.9090 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 10s 37ms/step - loss: 0.6831 - accuracy: 0.8695 - val_loss: 0.1702 - val_accuracy: 0.9305 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 9s 36ms/step - loss: 0.6453 - accuracy: 0.8734 - val_loss: 0.5239 - val_accuracy: 0.8004 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 9s 36ms/step - loss: 0.5651 - accuracy: 0.8836 - val_loss: 0.1480 - val_accuracy: 0.9444 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 9s 35ms/step - loss: 0.5660 - accuracy: 0.8823 - val_loss: 0.4149 - val_accuracy: 0.8363 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 9s 35ms/step - loss: 0.5375 - accuracy: 0.8981 - val_loss: 0.4127 - val_accuracy: 0.8619 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 9s 35ms/step - loss: 0.4875 - accuracy: 0.8995 - val_loss: 0.2242 - val_accuracy: 0.9278 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 9s 36ms/step - loss: 0.5136 - accuracy: 0.9003 - val_loss: 0.1833 - val_accuracy: 0.9488 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "255/257 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.9170\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "257/257 [==============================] - 9s 36ms/step - loss: 0.4195 - accuracy: 0.9172 - val_loss: 0.1499 - val_accuracy: 0.9468 - lr: 5.0000e-04\n",
      "Epoch 10: early stopping\n"
     ]
    }
   ],
   "source": [
    "q_history = q_ResNet24.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ResNet24.save('q_ResNet24.keras')  # The file needs to end with the .keras extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef representative_data_gen():\\n  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\\n    yield [input_value]\\n\\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\nconverter.representative_dataset = representative_data_gen\\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\nconverter._experimental_lower_tensor_list_ops = False\\nconverter.inference_input_type = tf.int8\\nconverter.inference_output_type = tf.int8\\n\\ntflite_model_quant_int8 = converter.convert()\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model_quant_int8 = converter.convert()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/liuxinqing/Documents/Fall_Detection/train_Kfall_kera.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liuxinqing/Documents/Fall_Detection/train_Kfall_kera.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m converter\u001b[39m.\u001b[39minference_input_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39muint8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liuxinqing/Documents/Fall_Detection/train_Kfall_kera.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m converter\u001b[39m.\u001b[39minference_output_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39muint8\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liuxinqing/Documents/Fall_Detection/train_Kfall_kera.ipynb#Y100sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tflite_model_quant_int8_qat \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1139\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m   1137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1138\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1093\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m   1092\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m-> 1093\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1094\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1601\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[39m@_export_metrics\u001b[39m\n\u001b[1;32m   1589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1590\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[39m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m   saved_model_convert_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_as_saved_model()\n\u001b[1;32m   1602\u001b[0m   \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1603\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1579\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1576\u001b[0m temp_dir \u001b[39m=\u001b[39m tempfile\u001b[39m.\u001b[39mmkdtemp()\n\u001b[1;32m   1577\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1578\u001b[0m   graph_def, input_tensors, output_tensors \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1579\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_keras_to_saved_model(temp_dir)\n\u001b[1;32m   1580\u001b[0m   )\n\u001b[1;32m   1581\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_model_dir:\n\u001b[1;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconvert(\n\u001b[1;32m   1583\u001b[0m         graph_def, input_tensors, output_tensors\n\u001b[1;32m   1584\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1502\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_keras_to_saved_model\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save Keras model to the SavedModel format.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39m  output_tensors: List of output tensors.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m   _save\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m   1503\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_model,\n\u001b[1;32m   1504\u001b[0m       output_dir,\n\u001b[1;32m   1505\u001b[0m       options\u001b[39m=\u001b[39;49m_save_options\u001b[39m.\u001b[39;49mSaveOptions(save_debug_info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m   \u001b[39m# When storing the given keras model to a saved model is failed, let's\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m   \u001b[39m# use original keras model conversion pipeline.\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1336\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWriteApi(_SAVE_V2_LABEL)\n\u001b[0;32m-> 1336\u001b[0m save_and_return_nodes(obj, export_dir, signatures, options)\n\u001b[1;32m   1338\u001b[0m metrics\u001b[39m.\u001b[39mIncrementWrite(write_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1371\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1367\u001b[0m saved_model \u001b[39m=\u001b[39m saved_model_pb2\u001b[39m.\u001b[39mSavedModel()\n\u001b[1;32m   1368\u001b[0m meta_graph_def \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mmeta_graphs\u001b[39m.\u001b[39madd()\n\u001b[1;32m   1370\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1371\u001b[0m     _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001b[1;32m   1372\u001b[0m saved_model\u001b[39m.\u001b[39msaved_model_schema_version \u001b[39m=\u001b[39m (\n\u001b[1;32m   1373\u001b[0m     constants\u001b[39m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1375\u001b[0m \u001b[39m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[39m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1584\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \n\u001b[1;32m   1559\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[39m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \u001b[39mwith\u001b[39;00m save_context\u001b[39m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1584\u001b[0m   \u001b[39mreturn\u001b[39;00m _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1537\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1532\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type f\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(func)\u001b[39m}\u001b[39;00m\u001b[39m. Functions in `function_aliases`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m should be created by tf.function, or concrete functions, or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m collections of concrete functions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m       )\n\u001b[0;32m-> 1537\u001b[0m object_graph_proto \u001b[39m=\u001b[39m _serialize_object_graph(\n\u001b[1;32m   1538\u001b[0m     saveable_view, asset_info\u001b[39m.\u001b[39;49masset_index\n\u001b[1;32m   1539\u001b[0m )\n\u001b[1;32m   1540\u001b[0m meta_graph_def\u001b[39m.\u001b[39mobject_graph_def\u001b[39m.\u001b[39mCopyFrom(object_graph_proto)\n\u001b[1;32m   1541\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m   1542\u001b[0m     meta_graph_def,\n\u001b[1;32m   1543\u001b[0m     exported_graph,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     saveable_view\u001b[39m.\u001b[39mnode_paths,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/save.py:1059\u001b[0m, in \u001b[0;36m_serialize_object_graph\u001b[0;34m(saveable_view, asset_file_def_index)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mfor\u001b[39;00m concrete_function \u001b[39min\u001b[39;00m saveable_view\u001b[39m.\u001b[39mconcrete_and_gradient_functions:\n\u001b[1;32m   1058\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(concrete_function\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1059\u001b[0m   serialized \u001b[39m=\u001b[39m function_serialization\u001b[39m.\u001b[39;49mserialize_concrete_function(\n\u001b[1;32m   1060\u001b[0m       concrete_function, saveable_view\u001b[39m.\u001b[39;49mcaptured_tensor_node_ids\n\u001b[1;32m   1061\u001b[0m   )\n\u001b[1;32m   1062\u001b[0m   \u001b[39mif\u001b[39;00m serialized \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     proto\u001b[39m.\u001b[39mconcrete_functions[name]\u001b[39m.\u001b[39mCopyFrom(serialized)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/function_serialization.py:80\u001b[0m, in \u001b[0;36mserialize_concrete_function\u001b[0;34m(concrete_function, node_ids)\u001b[0m\n\u001b[1;32m     76\u001b[0m concrete_function_proto \u001b[39m=\u001b[39m saved_object_graph_pb2\u001b[39m.\u001b[39mSavedConcreteFunction()\n\u001b[1;32m     77\u001b[0m structured_outputs \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39mconvert_structure_to_signature(\n\u001b[1;32m     78\u001b[0m     concrete_function\u001b[39m.\u001b[39mstructured_outputs)\n\u001b[1;32m     79\u001b[0m concrete_function_proto\u001b[39m.\u001b[39mcanonicalized_input_signature\u001b[39m.\u001b[39mCopyFrom(\n\u001b[0;32m---> 80\u001b[0m     nested_structure_coder\u001b[39m.\u001b[39;49mencode_structure(\n\u001b[1;32m     81\u001b[0m         concrete_function\u001b[39m.\u001b[39;49mstructured_input_signature))\n\u001b[1;32m     82\u001b[0m concrete_function_proto\u001b[39m.\u001b[39moutput_signature\u001b[39m.\u001b[39mCopyFrom(\n\u001b[1;32m     83\u001b[0m     nested_structure_coder\u001b[39m.\u001b[39mencode_structure(structured_outputs))\n\u001b[1;32m     84\u001b[0m concrete_function_proto\u001b[39m.\u001b[39mbound_inputs\u001b[39m.\u001b[39mextend(bound_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:90\u001b[0m, in \u001b[0;36mencode_structure\u001b[0;34m(nested_structure)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__internal__.saved_model.encode_structure\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_structure\u001b[39m(nested_structure):\n\u001b[1;32m     79\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Encodes nested structures composed of encodable types into a proto.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m    NotEncodableError: For values for which there are no encoders.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m   \u001b[39mreturn\u001b[39;00m _map_structure(nested_structure, _get_encoders())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:72\u001b[0m, in \u001b[0;36m_map_structure\u001b[0;34m(pyobj, coders)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[39mif\u001b[39;00m can(pyobj):\n\u001b[1;32m     71\u001b[0m     recursion_fn \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(_map_structure, coders\u001b[39m=\u001b[39mcoders)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m do(pyobj, recursion_fn)\n\u001b[1;32m     73\u001b[0m \u001b[39mraise\u001b[39;00m NotEncodableError(\n\u001b[1;32m     74\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo encoder for object \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:175\u001b[0m, in \u001b[0;36m_TupleCodec.do_encode\u001b[0;34m(self, tuple_value, encode_fn)\u001b[0m\n\u001b[1;32m    173\u001b[0m encoded_tuple\u001b[39m.\u001b[39mtuple_value\u001b[39m.\u001b[39mCopyFrom(struct_pb2\u001b[39m.\u001b[39mTupleValue())\n\u001b[1;32m    174\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m tuple_value:\n\u001b[0;32m--> 175\u001b[0m   encoded_tuple\u001b[39m.\u001b[39mtuple_value\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39madd()\u001b[39m.\u001b[39mCopyFrom(encode_fn(element))\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_tuple\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:72\u001b[0m, in \u001b[0;36m_map_structure\u001b[0;34m(pyobj, coders)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[39mif\u001b[39;00m can(pyobj):\n\u001b[1;32m     71\u001b[0m     recursion_fn \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(_map_structure, coders\u001b[39m=\u001b[39mcoders)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m do(pyobj, recursion_fn)\n\u001b[1;32m     73\u001b[0m \u001b[39mraise\u001b[39;00m NotEncodableError(\n\u001b[1;32m     74\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo encoder for object \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:175\u001b[0m, in \u001b[0;36m_TupleCodec.do_encode\u001b[0;34m(self, tuple_value, encode_fn)\u001b[0m\n\u001b[1;32m    173\u001b[0m encoded_tuple\u001b[39m.\u001b[39mtuple_value\u001b[39m.\u001b[39mCopyFrom(struct_pb2\u001b[39m.\u001b[39mTupleValue())\n\u001b[1;32m    174\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m tuple_value:\n\u001b[0;32m--> 175\u001b[0m   encoded_tuple\u001b[39m.\u001b[39mtuple_value\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39madd()\u001b[39m.\u001b[39mCopyFrom(encode_fn(element))\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_tuple\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:72\u001b[0m, in \u001b[0;36m_map_structure\u001b[0;34m(pyobj, coders)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[39mif\u001b[39;00m can(pyobj):\n\u001b[1;32m     71\u001b[0m     recursion_fn \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(_map_structure, coders\u001b[39m=\u001b[39mcoders)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m do(pyobj, recursion_fn)\n\u001b[1;32m     73\u001b[0m \u001b[39mraise\u001b[39;00m NotEncodableError(\n\u001b[1;32m     74\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo encoder for object \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/framework/tensor.py:1238\u001b[0m, in \u001b[0;36m_TensorSpecCodec.do_encode\u001b[0;34m(self, tensor_spec_value, encode_fn)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_encode\u001b[39m(\u001b[39mself\u001b[39m, tensor_spec_value, encode_fn):\n\u001b[1;32m   1235\u001b[0m   encoded_tensor_spec \u001b[39m=\u001b[39m struct_pb2\u001b[39m.\u001b[39mStructuredValue()\n\u001b[1;32m   1236\u001b[0m   encoded_tensor_spec\u001b[39m.\u001b[39mtensor_spec_value\u001b[39m.\u001b[39mCopyFrom(\n\u001b[1;32m   1237\u001b[0m       struct_pb2\u001b[39m.\u001b[39mTensorSpecProto(\n\u001b[0;32m-> 1238\u001b[0m           shape\u001b[39m=\u001b[39mencode_fn(tensor_spec_value\u001b[39m.\u001b[39;49mshape)\u001b[39m.\u001b[39mtensor_shape_value,\n\u001b[1;32m   1239\u001b[0m           dtype\u001b[39m=\u001b[39mencode_fn(tensor_spec_value\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mtensor_dtype_value,\n\u001b[1;32m   1240\u001b[0m           name\u001b[39m=\u001b[39mtensor_spec_value\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1241\u001b[0m   \u001b[39mreturn\u001b[39;00m encoded_tensor_spec\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:72\u001b[0m, in \u001b[0;36m_map_structure\u001b[0;34m(pyobj, coders)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[39mif\u001b[39;00m can(pyobj):\n\u001b[1;32m     71\u001b[0m     recursion_fn \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(_map_structure, coders\u001b[39m=\u001b[39mcoders)\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m do(pyobj, recursion_fn)\n\u001b[1;32m     73\u001b[0m \u001b[39mraise\u001b[39;00m NotEncodableError(\n\u001b[1;32m     74\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo encoder for object \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pyobj)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py:1529\u001b[0m, in \u001b[0;36m_TensorShapeCodec.do_encode\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[39mdel\u001b[39;00m encode_fn\n\u001b[1;32m   1528\u001b[0m encoded_tensor_shape \u001b[39m=\u001b[39m struct_pb2\u001b[39m.\u001b[39mStructuredValue()\n\u001b[0;32m-> 1529\u001b[0m encoded_tensor_shape\u001b[39m.\u001b[39;49mtensor_shape_value\u001b[39m.\u001b[39;49mCopyFrom(\n\u001b[1;32m   1530\u001b[0m     tensor_shape_value\u001b[39m.\u001b[39;49mas_proto())\n\u001b[1;32m   1531\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_tensor_shape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fall_detection/lib/python3.9/site-packages/google/protobuf/message.py:130\u001b[0m, in \u001b[0;36mMessage.CopyFrom\u001b[0;34m(self, other_msg)\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mClear()\n\u001b[0;32m--> 130\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMergeFrom(other_msg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_ResNet24)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109104"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"models/fmnist_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10717244 -0.11876063 -0.9641799  -0.57687505  1.35874835  0.82601957\n",
      " -0.25227563  0.24004562 -0.11913926]\n",
      "####\n",
      "[[  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   2   1   0   0   0]\n",
      " [  0   0   0   0   2   1   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0]\n",
      " [  0   0   0 255   2   0   0   0   0]\n",
      " [  0   0   0 255   2   0   0   0   0]\n",
      " [  0   0   0 255   2   0   0   0   0]\n",
      " [  0   0   0 255   2   0   0   0   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]\n",
      " [  0   0   0 255   2   0   0   1   0]]\n",
      "(212,)\n",
      "input:  {'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 50,  9], dtype=int32), 'shape_signature': array([-1, 50,  9], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (9.541885375976562, 75), 'quantization_parameters': {'scales': array([9.541885], dtype=float32), 'zero_points': array([75], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 108, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n"
     ]
    }
   ],
   "source": [
    "# test the quantized model\n",
    "print(X_test[0][0])\n",
    "print('####')\n",
    "X_test_int8 = X_test.astype('uint8')\n",
    "y_test_int8 = y_test.astype('uint8')\n",
    "#print(y_test_int8)\n",
    "# now y_test_int8 is one-hot encoded. convert y_test_int8 back\n",
    "#y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "print(X_test_int8[0])\n",
    "print(y_test_int8.shape)\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= tflite_model_quant_int8_qat)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "# convert the predictions from one hot back to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(predictions.shape)\n",
    "y_test_int8 = np.argmax(y_test_int8, axis=-1)\n",
    "#print(np.argmax(y_test_int8, axis=-1))\n",
    "print(y_test_int8.shape)\n",
    "accuracy = (predictions == y_test_int8).mean()\n",
    "print('y_test_int8: ', y_test_int8)\n",
    "print('predictions: ', predictions)\n",
    "print('accuracy: ', accuracy)\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_int8, predictions)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

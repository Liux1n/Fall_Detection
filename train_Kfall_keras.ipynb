{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:17:42.398687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 23:17:42.398754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 23:17:42.400991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 23:17:42.417963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 23:17:43.291165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import plot_confusion_matrix, plot_confusion_matrix, get_gzipped_model_size\n",
    "from data_organizer_Kfall import DataOrganizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models, optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from models.ConvLSTM import ConvLSTM\n",
    "from models.ConvLSTM_VGG import ConvLSTM_VGG\n",
    "from models.TinyFallNet import TinyFallNet\n",
    "from models.ResNet24 import ResNet24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:17:44.193266: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:44.238487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:44.238564: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:44.242110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:44.242195: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:44.242243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:45.570105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:45.570369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:45.570383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-10 23:17:45.570488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-10 23:17:45.570509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3411 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ResNet24\" # \"ConvLSTM\" or \"ConvLSTM_VGG\" or \"TinyFallNet\" or \"ResNet24\"\n",
    "load_from_checkpoint = False\n",
    "\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = models.load_model('./saved_models/'+model_name+'.keras')\n",
    "else:\n",
    "    if model_name == \"ConvLSTM\":\n",
    "        model = ConvLSTM()\n",
    "    elif model_name == \"ConvLSTM_VGG\":\n",
    "        \n",
    "        model = ConvLSTM_VGG()\n",
    "    elif model_name == \"TinyFallNet\":\n",
    "        \n",
    "        model = TinyFallNet()\n",
    "    elif model_name == \"ResNet24\":\n",
    "        \n",
    "        model = ResNet24()\n",
    "    else:\n",
    "        print(\"Please select a valid model name\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/32 folder...\n",
      "Processing 2/32 folder...\n",
      "Processing 3/32 folder...\n",
      "Processing 4/32 folder...\n",
      "Processing 5/32 folder...\n",
      "Processing 6/32 folder...\n",
      "Processing 7/32 folder...\n",
      "Processing 8/32 folder...\n",
      "Processing 9/32 folder...\n",
      "Processing 10/32 folder...\n",
      "Processing 11/32 folder...\n",
      "Processing 12/32 folder...\n",
      "Processing 13/32 folder...\n",
      "Processing 14/32 folder...\n",
      "Processing 15/32 folder...\n",
      "Processing 16/32 folder...\n",
      "Processing 17/32 folder...\n",
      "Processing 18/32 folder...\n",
      "Processing 19/32 folder...\n",
      "Processing 20/32 folder...\n",
      "Processing 21/32 folder...\n",
      "Processing 22/32 folder...\n",
      "Processing 23/32 folder...\n",
      "Processing 24/32 folder...\n",
      "Processing 25/32 folder...\n",
      "Processing 26/32 folder...\n",
      "Processing 27/32 folder...\n",
      "Processing 28/32 folder...\n",
      "Processing 29/32 folder...\n",
      "Processing 30/32 folder...\n",
      "Processing 31/32 folder...\n",
      "Processing 32/32 folder...\n",
      "in_channels:  9\n",
      "data.shape:  (25619, 50, 9)\n",
      "B_size:  25020\n",
      "A_size:  599\n",
      "data:  [ 5.60000000e-02 -9.96000000e-01 -2.50000000e-02  2.23453620e+00\n",
      "  1.31780340e+00 -6.87549600e-01  8.77198698e+01  3.39191136e+00\n",
      "  3.51223254e+00]\n",
      "(248, 50, 9)\n"
     ]
    }
   ],
   "source": [
    "# mac\n",
    "# sensor_data_folder = '/Users/liuxinqing/Documents/Kfall/sensor_data'  # Update with the path to sensor data\n",
    "# label_data_folder = '/Users/liuxinqing/Documents/Kfall/label_data'  \n",
    "# windows \n",
    "#sensor_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\sensor_data'  # Update with the path to sensor data\n",
    "#label_data_folder = 'G:\\MLonMCU\\Kfall_dataset\\label_data' \n",
    "# linux\n",
    "sensor_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/sensor_data'  # Update with the path to sensor data\n",
    "label_data_folder = '/home/liyinrong/Projects/MLonMCU/Final/Fall_Detection/datasets/KFall/label_data'  \n",
    "\n",
    "#window_size = 256\n",
    "# Kfall: window_size = 50\n",
    "window_size = 50\n",
    "threshold = 0.4\n",
    "num_window_fall_data = 50\n",
    "num_window_not_fall_data = 5\n",
    "\n",
    "data, label = DataOrganizer(sensor_data_folder, \n",
    "                            label_data_folder, \n",
    "                            window_size, \n",
    "                            threshold, \n",
    "                            num_window_fall_data, \n",
    "                            num_window_not_fall_data)\n",
    "\n",
    "in_channels = 9\n",
    "print('in_channels: ', in_channels)\n",
    "# the input data should have the shape (batch_size, in_channels, sequence_length)\n",
    "#data = data.reshape(data.shape[0], in_channels, -1)\n",
    "print('data.shape: ', data.shape)\n",
    "\n",
    "label = label.astype(np.int64)\n",
    "# one-hot encoding\n",
    "#label = to_categorical(label, num_classes=2)\n",
    "# transpose the data to (batch_size, sequence_length, in_channels)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "data = data.reshape(data.shape[0], 50, 9)\n",
    "# normalize the data\n",
    "# Initialize a new scaling object for normalizing input data\n",
    "# Z-score normalization\n",
    "\n",
    "# (y == 0).sum()\n",
    "B_size = (label == 0).sum()\n",
    "A_size = (label == 1).sum()\n",
    "print('B_size: ', B_size)\t\n",
    "print('A_size: ', A_size)\n",
    "# transpose the data to (batch_size, in_channels, sequence_length)\n",
    "#data = np.transpose(data, (0, 2, 1))\n",
    "print('data: ', data[0][0])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(np.unique(y_train)) # [0 1]\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "# select the test data that is not zero\n",
    "X_test_true = X_test[y_test != 0]\n",
    "y_test_true = y_test[y_test != 0]\n",
    "# length of the test data\n",
    "test_len = X_test_true.shape[0]\n",
    "X_test_false = X_test[y_test == 0]\n",
    "y_test_false = y_test[y_test == 0]\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "index = np.random.choice(X_test_false.shape[0], test_len, replace=False)\n",
    "# X_test.shape:  (17, 50, 9)\n",
    "# randomly len number of test data that is zero\n",
    "#index = np.random.choice(X_test_false.shape[0], len, replace=False)\n",
    "\n",
    "X_test_false = X_test[index]\n",
    "y_test_false = y_test[index]\n",
    "\n",
    "# concatenate the true and false test data\n",
    "X_test = np.concatenate((X_test_true, X_test_false), axis=0)\n",
    "y_test = np.concatenate((y_test_true, y_test_false), axis=0)\n",
    "#X_test = X_test[y_test != 0]\n",
    "#y_test = y_test[y_test != 0]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-4\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "factor = 0.5\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 50, 9)             0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 1, 48, 64)            1792      ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 1, 46, 64)            12352     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 1, 23, 64)            0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 1, 23, 16)            1040      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1, 23, 16)            64        ['conv2d_3[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 1, 23, 16)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 1, 23, 16)            784       ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 1, 23, 16)            64        ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 1, 23, 64)            1088      ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 1, 23, 64)            256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 1, 23, 64)            4160      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 1, 23, 64)            0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'conv2d_2[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 1, 23, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 1, 23, 16)            1040      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 1, 23, 16)            64        ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 1, 23, 16)            784       ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 1, 23, 16)            64        ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 1, 23, 64)            1088      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 1, 23, 64)            256       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 1, 23, 64)            4160      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'conv2d_6[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 1, 23, 64)            0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 1, 23, 16)            64        ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 1, 23, 16)            64        ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 1, 23, 64)            256       ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_8[0][0]'\n",
      "                                                                    , 're_lu_5[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 1, 23, 64)            0         ['add_2[0][0]']               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_14 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 1, 23, 16)            64        ['conv2d_14[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 1, 23, 16)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1, 23, 16)            64        ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 1, 23, 64)            256       ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 1, 23, 64)            0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 1, 23, 16)            64        ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 1, 23, 16)            64        ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 1, 23, 64)            256       ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_14[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 1, 23, 64)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 1, 23, 16)            64        ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 1, 23, 16)            64        ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 1, 23, 64)            256       ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 1, 23, 64)            4160      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 1, 23, 64)            0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 1, 23, 16)            1040      ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 1, 23, 16)            64        ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 1, 23, 16)            784       ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 1, 23, 16)            64        ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 1, 23, 16)            0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 1, 23, 64)            1088      ['re_lu_19[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 1, 23, 64)            256       ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 1, 23, 64)            0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)             (None, 1, 23, 64)            0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 1, 11, 64)            0         ['re_lu_20[0][0]']            \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 704)                  0         ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    1410      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55266 (215.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 1344 (5.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "            loss='categorical_crossentropy',\n",
    "            #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 50, 9))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16396, 2)\n",
      "y_val.shape:  (4099, 2)\n",
      "X_train.shape:  (16396, 50, 9)\n",
      "y_train.shape:  (16396, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:18:18.483368: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-10 23:18:19.506261: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-10 23:18:19.506991: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-10 23:18:19.537113: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-10 23:18:19.537176: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:110] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-12-10 23:18:19.567149: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-10 23:18:19.567304: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-12-10 23:18:20.271767: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-10 23:18:20.499838: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f9c177374a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 23:18:20.499871: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-12-10 23:18:20.508253: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702246700.600575   22673 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 25s 40ms/step - loss: 1.5714 - accuracy: 0.7790 - val_loss: 0.5211 - val_accuracy: 0.8446 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 8s 32ms/step - loss: 0.7549 - accuracy: 0.8738 - val_loss: 0.3272 - val_accuracy: 0.9051 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.6874 - accuracy: 0.8708 - val_loss: 0.2826 - val_accuracy: 0.9168 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.6422 - accuracy: 0.8874 - val_loss: 0.2748 - val_accuracy: 0.9219 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.5944 - accuracy: 0.8967 - val_loss: 0.4528 - val_accuracy: 0.8668 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 8s 32ms/step - loss: 0.5224 - accuracy: 0.9040 - val_loss: 0.5043 - val_accuracy: 0.8383 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.4594 - accuracy: 0.9114 - val_loss: 0.4290 - val_accuracy: 0.8624 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.4904 - accuracy: 0.8899 - val_loss: 0.2926 - val_accuracy: 0.8983 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.9159\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "257/257 [==============================] - 8s 31ms/step - loss: 0.3933 - accuracy: 0.9159 - val_loss: 0.3329 - val_accuracy: 0.8905 - lr: 5.0000e-04\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train(train_dataloader, model_ConvLSTM, loss_fn, optimizer,val_dataloader, \n",
    "           patience=patience, scheduler=scheduler, epochs=epochs, device=device, B_size=B_size, A_size=A_size) \"\"\"\n",
    "# Train the model\n",
    "# Train the model without using batches\n",
    "# Compile the model\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Calculate class weights\n",
    "B_multiplier = 1\n",
    "A_multiplier = B_size / A_size\n",
    "class_weight = {0: B_multiplier, 1: A_multiplier}\n",
    "\n",
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, verbose=1)\n",
    "print('X_train.shape: ', X_train.shape) # (23291, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (23291,)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          callbacks=[es, lrs],\n",
    "          class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9e001c75b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (248, 50, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 1s - loss: 0.1883 - accuracy: 0.9435 - 783ms/epoch - 98ms/step\n",
      "Test loss: [0.18829847872257233, 0.9435483813285828]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "if y_test.ndim == 1:\n",
    "    y_test = to_categorical(y_test)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 6ms/step\n",
      "[[111  10]\n",
      " [  4 123]]\n",
      "Confusion matrix, without normalization\n",
      "[[111  10]\n",
      " [  4 123]]\n",
      "accuracy:  0.9435483870967742\n",
      "f1_score:  0.9461538461538462\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert y_test back to its original form\n",
    "y_test_original = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_original, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "print('accuracy: ', accuracy_fp)\n",
    "# f1 score\n",
    "precision_fp = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall_fp = cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "f1_score_fp = 2 * precision_fp * recall_fp / (precision_fp + recall_fp)\n",
    "print('f1_score: ', f1_score_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuhvu1npd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuhvu1npd/assets\n",
      "2023-12-10 23:19:51.699725: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-10 23:19:51.699772: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-10 23:19:51.700162: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpuhvu1npd\n",
      "2023-12-10 23:19:51.712930: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-10 23:19:51.712956: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpuhvu1npd\n",
      "2023-12-10 23:19:51.735387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2023-12-10 23:19:51.752252: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-10 23:19:52.139836: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpuhvu1npd\n",
      "2023-12-10 23:19:52.245287: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 545127 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231716"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('./saved_models/'+model_name+'.keras')\n",
    "# convert the model to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "if model_name==\"ConvLSTM\" or model_name==\"ConvLSTM_VGG\":\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "model_tflite = converter.convert()\n",
    "# save the model\n",
    "open('./saved_models/'+model_name+'.tflite', \"wb\").write(model_tflite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for ConvLSTM model\n",
    "if model_name==\"ConvLSTM\" or model_name==\"ConvLSTM_VGG\":\n",
    "    def representative_data_gen():\n",
    "        for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(ConvLSTM)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.int8\n",
    "\n",
    "    tflite_q_model = converter.convert()\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    print('input: ', input_type)\n",
    "    output_type = interpreter.get_output_details()[0]['dtype']\n",
    "    print('output: ', output_type)\n",
    "    # Save the quantized model to disk\n",
    "    open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLa  (None, 50, 9)                3         ['input_1[0][0]']             \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " quant_reshape (QuantizeWra  (None, 1, 50, 9)             1         ['quantize_layer[0][0]']      \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d (QuantizeWrap  (None, 1, 48, 64)            1923      ['quant_reshape[0][0]']       \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 46, 64)            12483     ['quant_conv2d[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 23, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 23, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 23, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 23, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_10[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_12[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_14[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_16[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_13[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_11[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_17[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_18[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_19[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_21[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_22[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_23[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_20[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_25[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_26[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 11, 64)            3         ['quant_re_lu_20[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 704)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1415      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model = tfmot.quantization.keras.quantize_model(model)\n",
    "q_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "q_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5ikx8m3s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5ikx8m3s/assets\n",
      "/home/liyinrong/miniconda3/envs/tensorflow-dev/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:20:14.852480: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-10 23:20:14.852575: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-10 23:20:14.852958: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp5ikx8m3s\n",
      "2023-12-10 23:20:14.866108: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-10 23:20:14.866137: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp5ikx8m3s\n",
      "2023-12-10 23:20:14.926252: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-10 23:20:15.353904: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp5ikx8m3s\n",
      "2023-12-10 23:20:15.507496: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 654541 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 110, % non-converted = 5.45 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108648"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_q.keras')\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "\n",
    "# Convert the model\n",
    "tflite_q_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open('./saved_models/'+model_name+'_q.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape:  (16396, 2)\n",
      "y_val.shape:  (4099, 2)\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 47s 137ms/step - loss: 0.9247 - accuracy: 0.7889 - val_loss: 0.1314 - val_accuracy: 0.9617 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 35s 135ms/step - loss: 0.7832 - accuracy: 0.8409 - val_loss: 0.3740 - val_accuracy: 0.8785 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 34s 133ms/step - loss: 0.5639 - accuracy: 0.8806 - val_loss: 0.2679 - val_accuracy: 0.9107 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.5185 - accuracy: 0.8977 - val_loss: 0.2419 - val_accuracy: 0.9173 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 0.4648 - accuracy: 0.9058 - val_loss: 0.1931 - val_accuracy: 0.9375 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.3772 - accuracy: 0.9194\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 0.3772 - accuracy: 0.9194 - val_loss: 0.1989 - val_accuracy: 0.9344 - lr: 5.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Ensure y_train and y_val are one-hot encoded only once\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = to_categorical(y_val)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n",
    "q_history = q_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpv4u2rj46/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpv4u2rj46/assets\n",
      "/home/liyinrong/miniconda3/envs/tensorflow-dev/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-10 23:24:05.872217: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-10 23:24:05.872260: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:24:05.872447: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpv4u2rj46\n",
      "2023-12-10 23:24:05.897592: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-10 23:24:05.897622: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpv4u2rj46\n",
      "2023-12-10 23:24:05.979963: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-10 23:24:06.696978: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpv4u2rj46\n",
      "2023-12-10 23:24:06.931430: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1058984 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: INT8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108928"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.save('./saved_models/'+model_name+'_qat.keras')  # The file needs to end with the .keras extension\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype('float32')).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "# Set up the converter for quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# This is required for full integer quantization (including input and output)\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # Keep input as float32\n",
    "converter.inference_output_type = tf.int8  # Keep output as float32\n",
    "\n",
    "# Convert the model\n",
    "tflite_q_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_q_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open('./saved_models/'+model_name+'_qat.tflite', \"wb\").write(tflite_q_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_reshap  (None, 1, 50, 9)             1         ['input_1[0][0]']             \n",
      " e (PruneLowMagnitude)                                                                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 48, 64)            3522      ['prune_low_magnitude_reshape[\n",
      "  (PruneLowMagnitude)                                               0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 46, 64)            24642     ['prune_low_magnitude_conv2d[0\n",
      " _1 (PruneLowMagnitude)                                             ][0]']                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_max_po  (None, 1, 23, 64)            1         ['prune_low_magnitude_conv2d_1\n",
      " oling2d (PruneLowMagnitude                                         [0][0]']                      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_max_pool\n",
      " _3 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_3\n",
      " normalization (PruneLowMag                                         [0][0]']                      \n",
      " nitude)                                                                                          \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu   (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization[0][0]']           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu[0]\n",
      " _4 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_4\n",
      " normalization_1 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 1 (PruneLowMagnitude)                                              rmalization_1[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_1[\n",
      " _5 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_5\n",
      " normalization_2 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_max_pool\n",
      " _2 (PruneLowMagnitude)                                             ing2d[0][0]']                 \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add (P  (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " runeLowMagnitude)                                                  rmalization_2[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add[0][0\n",
      " 2 (PruneLowMagnitude)                                              ]']                           \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_2[\n",
      " _7 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_7\n",
      " normalization_3 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 3 (PruneLowMagnitude)                                              rmalization_3[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_3[\n",
      " _8 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_8\n",
      " normalization_4 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 4 (PruneLowMagnitude)                                              rmalization_4[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_4[\n",
      " _9 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_9\n",
      " normalization_5 (PruneLowM                                         [0][0]']                      \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_2[\n",
      " _6 (PruneLowMagnitude)                                             0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_1   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_5[0][0]',         \n",
      "                                                                     'prune_low_magnitude_conv2d_6\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_1[0]\n",
      " 5 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_5[\n",
      " _10 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_6 (PruneLowM                                         0[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 6 (PruneLowMagnitude)                                              rmalization_6[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_6[\n",
      " _11 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_7 (PruneLowM                                         1[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 7 (PruneLowMagnitude)                                              rmalization_7[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_7[\n",
      " _12 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_8 (PruneLowM                                         2[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_2   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_8[0][0]',         \n",
      "                                                                     'prune_low_magnitude_re_lu_5[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_2[0]\n",
      " 8 (PruneLowMagnitude)                                              [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_8[\n",
      " _14 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_9 (PruneLowM                                         4[0][0]']                     \n",
      " agnitude)                                                                                        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 9 (PruneLowMagnitude)                                              rmalization_9[0][0]']         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_9[\n",
      " _15 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_10 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 10 (PruneLowMagnitude)                                             rmalization_10[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_10\n",
      " _16 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_11 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_8[\n",
      " _13 (PruneLowMagnitude)                                            0][0]']                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_3   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_11[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_1\n",
      "                                                                    3[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_3[0]\n",
      " 11 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_11\n",
      " _17 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_12 (PruneLow                                         7[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 12 (PruneLowMagnitude)                                             rmalization_12[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_12\n",
      " _18 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_1\n",
      " normalization_13 (PruneLow                                         8[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 13 (PruneLowMagnitude)                                             rmalization_13[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_13\n",
      " _19 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_1\n",
      " normalization_14 (PruneLow                                         9[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_4   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_14[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_11\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_4[0]\n",
      " 14 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_14\n",
      " _21 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_15 (PruneLow                                         1[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 15 (PruneLowMagnitude)                                             rmalization_15[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_15\n",
      " _22 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_16 (PruneLow                                         2[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 16 (PruneLowMagnitude)                                             rmalization_16[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_16\n",
      " _23 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_17 (PruneLow                                         3[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            8258      ['prune_low_magnitude_re_lu_14\n",
      " _20 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_5   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_17[0][0]',        \n",
      "                                                                     'prune_low_magnitude_conv2d_2\n",
      "                                                                    0[0][0]']                     \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_5[0]\n",
      " 17 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            2066      ['prune_low_magnitude_re_lu_17\n",
      " _24 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_18 (PruneLow                                         4[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 18 (PruneLowMagnitude)                                             rmalization_18[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 16)            1554      ['prune_low_magnitude_re_lu_18\n",
      " _25 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 16)            65        ['prune_low_magnitude_conv2d_2\n",
      " normalization_19 (PruneLow                                         5[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 16)            1         ['prune_low_magnitude_batch_no\n",
      " 19 (PruneLowMagnitude)                                             rmalization_19[0][0]']        \n",
      "                                                                                                  \n",
      " prune_low_magnitude_conv2d  (None, 1, 23, 64)            2114      ['prune_low_magnitude_re_lu_19\n",
      " _26 (PruneLowMagnitude)                                            [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_batch_  (None, 1, 23, 64)            257       ['prune_low_magnitude_conv2d_2\n",
      " normalization_20 (PruneLow                                         6[0][0]']                     \n",
      " Magnitude)                                                                                       \n",
      "                                                                                                  \n",
      " prune_low_magnitude_add_6   (None, 1, 23, 64)            1         ['prune_low_magnitude_batch_no\n",
      " (PruneLowMagnitude)                                                rmalization_20[0][0]',        \n",
      "                                                                     'prune_low_magnitude_re_lu_17\n",
      "                                                                    [0][0]']                      \n",
      "                                                                                                  \n",
      " prune_low_magnitude_re_lu_  (None, 1, 23, 64)            1         ['prune_low_magnitude_add_6[0]\n",
      " 20 (PruneLowMagnitude)                                             [0]']                         \n",
      "                                                                                                  \n",
      " prune_low_magnitude_averag  (None, 1, 11, 64)            1         ['prune_low_magnitude_re_lu_20\n",
      " e_pooling2d (PruneLowMagni                                         [0][0]']                      \n",
      " tude)                                                                                            \n",
      "                                                                                                  \n",
      " prune_low_magnitude_flatte  (None, 704)                  1         ['prune_low_magnitude_average_\n",
      " n (PruneLowMagnitude)                                              pooling2d[0][0]']             \n",
      "                                                                                                  \n",
      " prune_low_magnitude_dense   (None, 2)                    2820      ['prune_low_magnitude_flatten[\n",
      " (PruneLowMagnitude)                                                0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 106895 (417.88 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 52973 (207.24 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "ups = pruning_callbacks.UpdatePruningStep()\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 36s 68ms/step - loss: 0.5781 - accuracy: 0.8952 - val_loss: 0.5448 - val_accuracy: 0.8148 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.4927 - accuracy: 0.9028 - val_loss: 0.2778 - val_accuracy: 0.9197 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.4412 - accuracy: 0.9143 - val_loss: 0.2964 - val_accuracy: 0.8909 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 16s 64ms/step - loss: 0.4204 - accuracy: 0.9152 - val_loss: 0.2827 - val_accuracy: 0.9136 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.4729 - accuracy: 0.9024 - val_loss: 0.2702 - val_accuracy: 0.9095 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.3454 - accuracy: 0.9254 - val_loss: 0.1803 - val_accuracy: 0.9368 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 17s 66ms/step - loss: 0.3400 - accuracy: 0.9269 - val_loss: 0.1907 - val_accuracy: 0.9390 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.3858 - accuracy: 0.9289 - val_loss: 0.2142 - val_accuracy: 0.9332 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.5117 - accuracy: 0.8945 - val_loss: 0.2072 - val_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.3065 - accuracy: 0.9280 - val_loss: 0.1650 - val_accuracy: 0.9427 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.2782 - accuracy: 0.9413 - val_loss: 0.4184 - val_accuracy: 0.8434 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 17s 64ms/step - loss: 0.2362 - accuracy: 0.9443 - val_loss: 0.1553 - val_accuracy: 0.9490 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.2002 - accuracy: 0.9550 - val_loss: 0.1479 - val_accuracy: 0.9468 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 16s 64ms/step - loss: 0.1925 - accuracy: 0.9555 - val_loss: 0.1914 - val_accuracy: 0.9351 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 17s 66ms/step - loss: 0.1720 - accuracy: 0.9617 - val_loss: 0.2792 - val_accuracy: 0.9175 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 17s 66ms/step - loss: 0.2378 - accuracy: 0.9501 - val_loss: 0.1379 - val_accuracy: 0.9588 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 17s 66ms/step - loss: 0.1609 - accuracy: 0.9621 - val_loss: 0.1306 - val_accuracy: 0.9573 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.1617 - accuracy: 0.9636 - val_loss: 0.1894 - val_accuracy: 0.9336 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.1666 - accuracy: 0.9642 - val_loss: 0.2025 - val_accuracy: 0.9351 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.1883 - accuracy: 0.9585 - val_loss: 0.1107 - val_accuracy: 0.9629 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.1284 - accuracy: 0.9713 - val_loss: 0.2060 - val_accuracy: 0.9356 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 17s 64ms/step - loss: 0.1318 - accuracy: 0.9696 - val_loss: 0.1487 - val_accuracy: 0.9568 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 16s 64ms/step - loss: 0.1227 - accuracy: 0.9737 - val_loss: 0.1401 - val_accuracy: 0.9502 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.0939 - accuracy: 0.9783 - val_loss: 0.2230 - val_accuracy: 0.9217 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9650\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.1644 - accuracy: 0.9650 - val_loss: 0.1816 - val_accuracy: 0.9529 - lr: 5.0000e-04\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9ddc13a6e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_unstructured.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs, ups],\n",
    "            class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loss:  0.19394589960575104\n",
      "Pruned model accuracy:  0.9354838728904724\n",
      "Full-precision model accuracy:  0.9435483870967742\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7rz2j_ne/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7rz2j_ne/assets\n",
      "2023-12-10 23:31:35.597711: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-10 23:31:35.597755: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-10 23:31:35.598019: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp7rz2j_ne\n",
      "2023-12-10 23:31:35.609336: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-10 23:31:35.609360: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp7rz2j_ne\n",
      "2023-12-10 23:31:35.638142: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-10 23:31:35.808204: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp7rz2j_ne\n",
      "2023-12-10 23:31:35.875383: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 277375 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 62, Total Ops 108, % non-converted = 57.41 %\n",
      " * 62 ARITH ops\n",
      "\n",
      "- arith.constant:   62 occurrences  (f32: 56, i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 27)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (i32: 1)\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "pruned_model_unstructured.save('./saved_models/'+model_name+'_pruned_unstructured.keras')  # The file needs to end with the .keras extension\n",
    "#print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))\n",
    "\n",
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = './saved_models/'+model_name+'_pruned_unstructured.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the unstructured pruned model:  127757\n",
      "Size of the full-precision model:  201952\n",
      "The achieved compression ratio is 1.58x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size('./saved_models/'+model_name+'_pruned_unstructured.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('./saved_models/'+model_name+'.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('./saved_models/'+model_name+'.tflite') / get_gzipped_model_size('./saved_models/'+model_name+'_pruned_unstructured.tflite')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 9)]              0         []                            \n",
      "                                                                                                  \n",
      " quantize_layer_1 (Quantize  (None, 50, 9)                3         ['input_1[0][0]']             \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " quant_reshape (QuantizeWra  (None, 1, 50, 9)             1         ['quantize_layer_1[0][0]']    \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d (QuantizeWrap  (None, 1, 48, 64)            1923      ['quant_reshape[0][0]']       \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantizeWr  (None, 1, 46, 64)            12483     ['quant_conv2d[0][0]']        \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_max_pooling2d (Quant  (None, 1, 23, 64)            1         ['quant_conv2d_1[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization   (None, 1, 23, 16)            65        ['quant_conv2d_3[0][0]']      \n",
      " (QuantizeWrapperV2)                                                                              \n",
      "                                                                                                  \n",
      " quant_re_lu (QuantizeWrapp  (None, 1, 23, 16)            3         ['quant_batch_normalization[0]\n",
      " erV2)                                                              [0]']                         \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_4[0][0]']      \n",
      " 1 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_1 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_1[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_1[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_5[0][0]']      \n",
      " 2 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_max_pooling2d[0][0]'] \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add (QuantizeWrapper  (None, 1, 23, 64)            1         ['quant_batch_normalization_2[\n",
      " V2)                                                                0][0]',                       \n",
      "                                                                     'quant_conv2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_2 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add[0][0]']           \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantizeWr  (None, 1, 23, 16)            1073      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_7[0][0]']      \n",
      " 3 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_3 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_3[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantizeWr  (None, 1, 23, 16)            817       ['quant_re_lu_3[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_8[0][0]']      \n",
      " 4 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_4 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_4[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantizeWr  (None, 1, 23, 64)            1217      ['quant_re_lu_4[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_9[0][0]']      \n",
      " 5 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantizeWr  (None, 1, 23, 64)            4291      ['quant_re_lu_2[0][0]']       \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_add_1 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_5[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_conv2d_6[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_5 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_1[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_5[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_10[0][0]']     \n",
      " 6 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_6 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_6[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_6[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_11[0][0]']     \n",
      " 7 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_7 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_7[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_7[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_12[0][0]']     \n",
      " 8 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_add_2 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_8[\n",
      " erV2)                                                              0][0]',                       \n",
      "                                                                     'quant_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " quant_re_lu_8 (QuantizeWra  (None, 1, 23, 64)            3         ['quant_add_2[0][0]']         \n",
      " pperV2)                                                                                          \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_14[0][0]']     \n",
      " 9 (QuantizeWrapperV2)                                                                            \n",
      "                                                                                                  \n",
      " quant_re_lu_9 (QuantizeWra  (None, 1, 23, 16)            3         ['quant_batch_normalization_9[\n",
      " pperV2)                                                            0][0]']                       \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_9[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_15[0][0]']     \n",
      " 10 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_10 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_10\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_10[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_16[0][0]']     \n",
      " 11 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_8[0][0]']       \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_3 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_11\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_13[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_11 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_3[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_11[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_17[0][0]']     \n",
      " 12 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_12 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_12\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_12[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_18[0][0]']     \n",
      " 13 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_13 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_13\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_19 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_13[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_19[0][0]']     \n",
      " 14 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_4 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_14\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_14 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_4[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_21 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_21[0][0]']     \n",
      " 15 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_15 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_15\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_22 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_15[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_22[0][0]']     \n",
      " 16 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_16 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_16\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_23 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_16[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_23[0][0]']     \n",
      " 17 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_conv2d_20 (QuantizeW  (None, 1, 23, 64)            4291      ['quant_re_lu_14[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_add_5 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_17\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_conv2d_20[0][0]']     \n",
      "                                                                                                  \n",
      " quant_re_lu_17 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_5[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_conv2d_24 (QuantizeW  (None, 1, 23, 16)            1073      ['quant_re_lu_17[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_24[0][0]']     \n",
      " 18 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_18 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_18\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_25 (QuantizeW  (None, 1, 23, 16)            817       ['quant_re_lu_18[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 16)            65        ['quant_conv2d_25[0][0]']     \n",
      " 19 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_re_lu_19 (QuantizeWr  (None, 1, 23, 16)            3         ['quant_batch_normalization_19\n",
      " apperV2)                                                           [0][0]']                      \n",
      "                                                                                                  \n",
      " quant_conv2d_26 (QuantizeW  (None, 1, 23, 64)            1217      ['quant_re_lu_19[0][0]']      \n",
      " rapperV2)                                                                                        \n",
      "                                                                                                  \n",
      " quant_batch_normalization_  (None, 1, 23, 64)            259       ['quant_conv2d_26[0][0]']     \n",
      " 20 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_add_6 (QuantizeWrapp  (None, 1, 23, 64)            1         ['quant_batch_normalization_20\n",
      " erV2)                                                              [0][0]',                      \n",
      "                                                                     'quant_re_lu_17[0][0]']      \n",
      "                                                                                                  \n",
      " quant_re_lu_20 (QuantizeWr  (None, 1, 23, 64)            3         ['quant_add_6[0][0]']         \n",
      " apperV2)                                                                                         \n",
      "                                                                                                  \n",
      " quant_average_pooling2d (Q  (None, 1, 11, 64)            3         ['quant_re_lu_20[0][0]']      \n",
      " uantizeWrapperV2)                                                                                \n",
      "                                                                                                  \n",
      " quant_flatten (QuantizeWra  (None, 704)                  1         ['quant_average_pooling2d[0][0\n",
      " pperV2)                                                            ]']                           \n",
      "                                                                                                  \n",
      " quant_dense (QuantizeWrapp  (None, 2)                    1415      ['quant_flatten[0][0]']       \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57536 (224.75 KB)\n",
      "Trainable params: 53922 (210.63 KB)\n",
      "Non-trainable params: 3614 (14.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (16396, 50, 9)\n",
      "y_train.shape:  (16396, 2)\n",
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "257/257 [==============================] - 43s 128ms/step - loss: 1.2473 - accuracy: 0.7596 - val_loss: 0.2433 - val_accuracy: 0.9080 - lr: 5.0000e-04\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 0.6699 - accuracy: 0.8447 - val_loss: 0.3022 - val_accuracy: 0.8766 - lr: 5.0000e-04\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 0.4701 - accuracy: 0.8922 - val_loss: 0.3014 - val_accuracy: 0.8949 - lr: 5.0000e-04\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.3617 - accuracy: 0.9168 - val_loss: 0.2725 - val_accuracy: 0.9063 - lr: 5.0000e-04\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.2603 - accuracy: 0.9409 - val_loss: 0.2222 - val_accuracy: 0.9266 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 32s 124ms/step - loss: 0.2172 - accuracy: 0.9525 - val_loss: 0.1652 - val_accuracy: 0.9471 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.1706 - accuracy: 0.9619 - val_loss: 0.1474 - val_accuracy: 0.9529 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 32s 123ms/step - loss: 0.1381 - accuracy: 0.9681 - val_loss: 0.1393 - val_accuracy: 0.9568 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 32s 125ms/step - loss: 0.1399 - accuracy: 0.9701 - val_loss: 0.1645 - val_accuracy: 0.9546 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 33s 127ms/step - loss: 0.2078 - accuracy: 0.9549 - val_loss: 0.2640 - val_accuracy: 0.9219 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.1818 - accuracy: 0.9610 - val_loss: 0.1943 - val_accuracy: 0.9512 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 32s 125ms/step - loss: 0.1193 - accuracy: 0.9746 - val_loss: 0.1978 - val_accuracy: 0.9444 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9791\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "257/257 [==============================] - 32s 126ms/step - loss: 0.1002 - accuracy: 0.9791 - val_loss: 0.2417 - val_accuracy: 0.9488 - lr: 5.0000e-04\n",
      "Epoch 13: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9dbc23c9a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X_train.shape: ', X_train.shape) # (16362, 50, 9)\n",
    "print('y_train.shape: ', y_train.shape) # (16362, 2)\n",
    "print(batch_size)\n",
    "pruned_qat_model.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, lrs],\n",
    "            class_weight=class_weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned QAT model loss:  0.2202487736940384\n",
      "Pruned QAT model accuracy:  0.9354838728904724\n",
      "Full-precision model accuracy:  0.9435483870967742\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_loss, pruned_qat_acc = pruned_qat_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Pruned QAT model loss: ', pruned_qat_loss)\n",
    "print('Pruned QAT model accuracy: ', pruned_qat_acc)\n",
    "print('Full-precision model accuracy: ', accuracy_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxjgiwyag/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxjgiwyag/assets\n",
      "/home/liyinrong/miniconda3/envs/tensorflow-dev/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2023-12-10 23:39:06.869146: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-12-10 23:39:06.869218: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-12-10 23:39:06.869485: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxjgiwyag\n",
      "2023-12-10 23:39:06.888448: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-12-10 23:39:06.888470: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpxjgiwyag\n",
      "2023-12-10 23:39:06.958542: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-12-10 23:39:07.597569: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpxjgiwyag\n",
      "2023-12-10 23:39:07.788718: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 919235 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 6, Total Ops 111, % non-converted = 5.41 %\n",
      " * 6 ARITH ops\n",
      "\n",
      "- arith.constant:    6 occurrences  (i32: 6)\n",
      "\n",
      "\n",
      "\n",
      "  (uq_8: 7)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 27)\n",
      "  (f32: 1)\n",
      "  (uq_8: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n",
      "  (uq_8: 28, uq_32: 28)\n",
      "  (uq_8: 2)\n",
      "  (uq_8: 2)\n",
      "  (i32: 1)\n",
      "  (uq_8: 1)\n",
      "  (i32: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109104"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_qat_model.save('./saved_models/'+model_name+'_pqat.keras')  # The file needs to end with the .keras extension\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "pruned_qat_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "open('./saved_models/'+model_name+'_pqat.tflite', \"wb\").write(pruned_qat_tflite_model)\n",
    "\n",
    "# write TFLite model to a C source (or header) file\n",
    "#c_model_name = 'pruned_qat_fmnist'\n",
    "\n",
    "#with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "#    file.write(hex_to_c_array(pruned_qat_tflite_model, c_model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#39 is a dynamic-sized tensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  {'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 50,  9], dtype=int32), 'shape_signature': array([-1, 50,  9], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "output:  {'name': 'StatefulPartitionedCall:0', 'index': 108, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Evaluated on  0 .\n",
      "Evaluated on  100 .\n",
      "Evaluated on  200 .\n",
      "[[117   4]\n",
      " [ 12 115]]\n",
      "Confusion matrix, without normalization\n",
      "[[117   4]\n",
      " [ 12 115]]\n",
      "f1_score:  0.9349593495934959\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the quantized model\n",
    "X_test_int8 = X_test.astype('float32')\n",
    "y_test_int8 = y_test.astype('int8')\n",
    "# Load the model into an interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content= pruned_qat_tflite_model)\n",
    "# Allocate memory for the model's input Tensor(s)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "print(\"input: \", input_details)\n",
    "print(\"output: \", output_details)\n",
    "predictions = np.zeros(X_test.shape[0])\n",
    "for i, test_data in enumerate(X_test_int8):\n",
    "    test_data = np.expand_dims(test_data, axis=0)\n",
    "    #print(test_data.shape)\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    if i%100 == 0:\n",
    "        # print(\"Evaluated on %d images.\" % test_image_index)\n",
    "        print('Evaluated on ', i, '.')\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "gt = np.argmax(y_test_int8, axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gt, predictions)\n",
    "\n",
    "print(cm)\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cm, classes=['Not Fall', 'Fall'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# get accuracy\n",
    "#accuracy_fp = (cm[0][0] + cm[1][1]) / np.sum(cm)\n",
    "#print('accuracy: ', accuracy_fp)\n",
    "\n",
    "f1_score = 2 * cm[1][1] / (2 * cm[1][1] + cm[0][1] + cm[1][0])\n",
    "print('f1_score: ', f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the pruned QAT model:  53273\n",
      "Size of th QAT model:  69771\n",
      "Size of the full-precision model:  201952\n",
      "The achieved compression ratio is 2.30x\n"
     ]
    }
   ],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size('./saved_models/'+model_name+'_pqat.tflite'))\n",
    "print('Size of th QAT model: ', get_gzipped_model_size('./saved_models/'+model_name+'_qat.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('./saved_models/'+model_name+'.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('saved_models/TinyFallNet.tflite') / get_gzipped_model_size('./saved_models/'+model_name+'_pqat.tflite')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
